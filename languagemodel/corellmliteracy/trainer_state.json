{
  "best_global_step": 294378,
  "best_metric": 2.817159652709961,
  "best_model_checkpoint": "/content/distilgpt2_core/checkpoint-294378",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 294378,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0020381957890875,
      "grad_norm": 11.42972183227539,
      "learning_rate": 4.996619991983097e-05,
      "loss": 3.5253,
      "step": 200
    },
    {
      "epoch": 0.004076391578175,
      "grad_norm": 5.063648223876953,
      "learning_rate": 4.993222999001284e-05,
      "loss": 3.4066,
      "step": 400
    },
    {
      "epoch": 0.006114587367262499,
      "grad_norm": 5.467136859893799,
      "learning_rate": 4.9898260060194715e-05,
      "loss": 3.4111,
      "step": 600
    },
    {
      "epoch": 0.00815278315635,
      "grad_norm": 4.572113990783691,
      "learning_rate": 4.9864290130376594e-05,
      "loss": 3.389,
      "step": 800
    },
    {
      "epoch": 0.010190978945437499,
      "grad_norm": 5.350765228271484,
      "learning_rate": 4.983032020055847e-05,
      "loss": 3.3159,
      "step": 1000
    },
    {
      "epoch": 0.012229174734524999,
      "grad_norm": 5.054690361022949,
      "learning_rate": 4.9796350270740346e-05,
      "loss": 3.2796,
      "step": 1200
    },
    {
      "epoch": 0.014267370523612499,
      "grad_norm": 4.237917423248291,
      "learning_rate": 4.976238034092222e-05,
      "loss": 3.3588,
      "step": 1400
    },
    {
      "epoch": 0.0163055663127,
      "grad_norm": 2.871448040008545,
      "learning_rate": 4.972841041110409e-05,
      "loss": 3.3418,
      "step": 1600
    },
    {
      "epoch": 0.018343762101787498,
      "grad_norm": 4.065662384033203,
      "learning_rate": 4.969444048128597e-05,
      "loss": 3.2535,
      "step": 1800
    },
    {
      "epoch": 0.020381957890874998,
      "grad_norm": 5.636057376861572,
      "learning_rate": 4.966047055146784e-05,
      "loss": 3.2659,
      "step": 2000
    },
    {
      "epoch": 0.022420153679962498,
      "grad_norm": 5.317505836486816,
      "learning_rate": 4.962650062164972e-05,
      "loss": 3.1778,
      "step": 2200
    },
    {
      "epoch": 0.024458349469049998,
      "grad_norm": 4.69949197769165,
      "learning_rate": 4.9592530691831594e-05,
      "loss": 3.2455,
      "step": 2400
    },
    {
      "epoch": 0.026496545258137497,
      "grad_norm": 3.957355499267578,
      "learning_rate": 4.9558560762013466e-05,
      "loss": 3.2291,
      "step": 2600
    },
    {
      "epoch": 0.028534741047224997,
      "grad_norm": 5.445454120635986,
      "learning_rate": 4.9524590832195345e-05,
      "loss": 3.3238,
      "step": 2800
    },
    {
      "epoch": 0.030572936836312497,
      "grad_norm": 8.161276817321777,
      "learning_rate": 4.949062090237722e-05,
      "loss": 3.2721,
      "step": 3000
    },
    {
      "epoch": 0.0326111326254,
      "grad_norm": 5.618339538574219,
      "learning_rate": 4.945665097255909e-05,
      "loss": 3.1988,
      "step": 3200
    },
    {
      "epoch": 0.03464932841448749,
      "grad_norm": 3.584151268005371,
      "learning_rate": 4.942268104274097e-05,
      "loss": 3.2113,
      "step": 3400
    },
    {
      "epoch": 0.036687524203574996,
      "grad_norm": 4.730441093444824,
      "learning_rate": 4.938871111292284e-05,
      "loss": 3.2445,
      "step": 3600
    },
    {
      "epoch": 0.03872571999266249,
      "grad_norm": 3.737166166305542,
      "learning_rate": 4.935474118310472e-05,
      "loss": 3.2187,
      "step": 3800
    },
    {
      "epoch": 0.040763915781749996,
      "grad_norm": 4.450522422790527,
      "learning_rate": 4.932077125328659e-05,
      "loss": 3.2361,
      "step": 4000
    },
    {
      "epoch": 0.04280211157083749,
      "grad_norm": 3.599135160446167,
      "learning_rate": 4.9286801323468465e-05,
      "loss": 3.2002,
      "step": 4200
    },
    {
      "epoch": 0.044840307359924995,
      "grad_norm": 4.120450973510742,
      "learning_rate": 4.9252831393650345e-05,
      "loss": 3.2099,
      "step": 4400
    },
    {
      "epoch": 0.04687850314901249,
      "grad_norm": 4.360260009765625,
      "learning_rate": 4.921886146383222e-05,
      "loss": 3.2233,
      "step": 4600
    },
    {
      "epoch": 0.048916698938099995,
      "grad_norm": 3.001777410507202,
      "learning_rate": 4.9184891534014096e-05,
      "loss": 3.2623,
      "step": 4800
    },
    {
      "epoch": 0.05095489472718749,
      "grad_norm": 4.724865913391113,
      "learning_rate": 4.915092160419597e-05,
      "loss": 3.2252,
      "step": 5000
    },
    {
      "epoch": 0.052993090516274995,
      "grad_norm": 4.452153205871582,
      "learning_rate": 4.911695167437784e-05,
      "loss": 3.2274,
      "step": 5200
    },
    {
      "epoch": 0.05503128630536249,
      "grad_norm": 7.057386875152588,
      "learning_rate": 4.908298174455972e-05,
      "loss": 3.1978,
      "step": 5400
    },
    {
      "epoch": 0.057069482094449994,
      "grad_norm": 3.957796096801758,
      "learning_rate": 4.904901181474159e-05,
      "loss": 3.1742,
      "step": 5600
    },
    {
      "epoch": 0.05910767788353749,
      "grad_norm": 6.534478664398193,
      "learning_rate": 4.901504188492347e-05,
      "loss": 3.1873,
      "step": 5800
    },
    {
      "epoch": 0.061145873672624994,
      "grad_norm": 4.414292812347412,
      "learning_rate": 4.8981071955105344e-05,
      "loss": 3.1842,
      "step": 6000
    },
    {
      "epoch": 0.0631840694617125,
      "grad_norm": 4.062869548797607,
      "learning_rate": 4.8947102025287216e-05,
      "loss": 3.2061,
      "step": 6200
    },
    {
      "epoch": 0.0652222652508,
      "grad_norm": 3.044996976852417,
      "learning_rate": 4.8913132095469096e-05,
      "loss": 3.1538,
      "step": 6400
    },
    {
      "epoch": 0.06726046103988749,
      "grad_norm": 3.0355615615844727,
      "learning_rate": 4.887916216565097e-05,
      "loss": 3.2187,
      "step": 6600
    },
    {
      "epoch": 0.06929865682897499,
      "grad_norm": 5.344139575958252,
      "learning_rate": 4.884519223583284e-05,
      "loss": 3.2261,
      "step": 6800
    },
    {
      "epoch": 0.0713368526180625,
      "grad_norm": 4.489282131195068,
      "learning_rate": 4.881122230601472e-05,
      "loss": 3.2505,
      "step": 7000
    },
    {
      "epoch": 0.07337504840714999,
      "grad_norm": 3.7993974685668945,
      "learning_rate": 4.877725237619659e-05,
      "loss": 3.1717,
      "step": 7200
    },
    {
      "epoch": 0.07541324419623749,
      "grad_norm": 2.5515053272247314,
      "learning_rate": 4.874328244637847e-05,
      "loss": 3.1836,
      "step": 7400
    },
    {
      "epoch": 0.07745143998532499,
      "grad_norm": 3.7366323471069336,
      "learning_rate": 4.870931251656034e-05,
      "loss": 3.1377,
      "step": 7600
    },
    {
      "epoch": 0.0794896357744125,
      "grad_norm": 5.104907512664795,
      "learning_rate": 4.8675342586742216e-05,
      "loss": 3.1511,
      "step": 7800
    },
    {
      "epoch": 0.08152783156349999,
      "grad_norm": 3.978756904602051,
      "learning_rate": 4.8641372656924095e-05,
      "loss": 3.1561,
      "step": 8000
    },
    {
      "epoch": 0.08356602735258749,
      "grad_norm": 4.326467037200928,
      "learning_rate": 4.860740272710597e-05,
      "loss": 3.1449,
      "step": 8200
    },
    {
      "epoch": 0.08560422314167498,
      "grad_norm": 5.183226585388184,
      "learning_rate": 4.8573432797287846e-05,
      "loss": 3.1833,
      "step": 8400
    },
    {
      "epoch": 0.0876424189307625,
      "grad_norm": 4.0579705238342285,
      "learning_rate": 4.853946286746972e-05,
      "loss": 3.1657,
      "step": 8600
    },
    {
      "epoch": 0.08968061471984999,
      "grad_norm": 2.7841265201568604,
      "learning_rate": 4.850549293765159e-05,
      "loss": 3.1307,
      "step": 8800
    },
    {
      "epoch": 0.09171881050893749,
      "grad_norm": 4.094544410705566,
      "learning_rate": 4.847152300783347e-05,
      "loss": 3.1616,
      "step": 9000
    },
    {
      "epoch": 0.09375700629802498,
      "grad_norm": 2.4517111778259277,
      "learning_rate": 4.843755307801534e-05,
      "loss": 3.1394,
      "step": 9200
    },
    {
      "epoch": 0.0957952020871125,
      "grad_norm": 5.639666557312012,
      "learning_rate": 4.8403583148197215e-05,
      "loss": 3.0958,
      "step": 9400
    },
    {
      "epoch": 0.09783339787619999,
      "grad_norm": 3.6059648990631104,
      "learning_rate": 4.8369613218379094e-05,
      "loss": 3.1376,
      "step": 9600
    },
    {
      "epoch": 0.09987159366528749,
      "grad_norm": 4.411273956298828,
      "learning_rate": 4.833564328856097e-05,
      "loss": 3.117,
      "step": 9800
    },
    {
      "epoch": 0.10190978945437498,
      "grad_norm": 4.18103551864624,
      "learning_rate": 4.8301673358742846e-05,
      "loss": 3.1254,
      "step": 10000
    },
    {
      "epoch": 0.10394798524346249,
      "grad_norm": 4.731839656829834,
      "learning_rate": 4.826770342892472e-05,
      "loss": 3.1507,
      "step": 10200
    },
    {
      "epoch": 0.10598618103254999,
      "grad_norm": 6.36750602722168,
      "learning_rate": 4.823373349910659e-05,
      "loss": 3.1724,
      "step": 10400
    },
    {
      "epoch": 0.10802437682163749,
      "grad_norm": 2.65356707572937,
      "learning_rate": 4.819976356928847e-05,
      "loss": 3.1372,
      "step": 10600
    },
    {
      "epoch": 0.11006257261072498,
      "grad_norm": 3.869141101837158,
      "learning_rate": 4.816579363947034e-05,
      "loss": 3.1498,
      "step": 10800
    },
    {
      "epoch": 0.11210076839981249,
      "grad_norm": 4.655971050262451,
      "learning_rate": 4.813182370965222e-05,
      "loss": 3.1477,
      "step": 11000
    },
    {
      "epoch": 0.11413896418889999,
      "grad_norm": 4.9511027336120605,
      "learning_rate": 4.8097853779834094e-05,
      "loss": 3.1413,
      "step": 11200
    },
    {
      "epoch": 0.11617715997798748,
      "grad_norm": 2.749008893966675,
      "learning_rate": 4.8063883850015966e-05,
      "loss": 3.0844,
      "step": 11400
    },
    {
      "epoch": 0.11821535576707498,
      "grad_norm": 4.648101806640625,
      "learning_rate": 4.8029913920197845e-05,
      "loss": 3.0835,
      "step": 11600
    },
    {
      "epoch": 0.12025355155616249,
      "grad_norm": 2.478585958480835,
      "learning_rate": 4.799594399037972e-05,
      "loss": 3.0873,
      "step": 11800
    },
    {
      "epoch": 0.12229174734524999,
      "grad_norm": 2.8660686016082764,
      "learning_rate": 4.796197406056159e-05,
      "loss": 3.1216,
      "step": 12000
    },
    {
      "epoch": 0.12432994313433748,
      "grad_norm": 5.944960117340088,
      "learning_rate": 4.792800413074347e-05,
      "loss": 3.054,
      "step": 12200
    },
    {
      "epoch": 0.126368138923425,
      "grad_norm": 2.6887075901031494,
      "learning_rate": 4.789403420092534e-05,
      "loss": 3.1121,
      "step": 12400
    },
    {
      "epoch": 0.12840633471251248,
      "grad_norm": 5.6405558586120605,
      "learning_rate": 4.786006427110722e-05,
      "loss": 3.0966,
      "step": 12600
    },
    {
      "epoch": 0.1304445305016,
      "grad_norm": 5.184147357940674,
      "learning_rate": 4.782609434128909e-05,
      "loss": 3.1193,
      "step": 12800
    },
    {
      "epoch": 0.1324827262906875,
      "grad_norm": 4.39505672454834,
      "learning_rate": 4.7792124411470965e-05,
      "loss": 3.077,
      "step": 13000
    },
    {
      "epoch": 0.13452092207977498,
      "grad_norm": 5.185802459716797,
      "learning_rate": 4.7758154481652845e-05,
      "loss": 3.0872,
      "step": 13200
    },
    {
      "epoch": 0.1365591178688625,
      "grad_norm": 3.793583869934082,
      "learning_rate": 4.772418455183472e-05,
      "loss": 3.106,
      "step": 13400
    },
    {
      "epoch": 0.13859731365794997,
      "grad_norm": 2.4212565422058105,
      "learning_rate": 4.7690214622016596e-05,
      "loss": 3.1034,
      "step": 13600
    },
    {
      "epoch": 0.14063550944703748,
      "grad_norm": 5.654056072235107,
      "learning_rate": 4.765624469219847e-05,
      "loss": 3.0765,
      "step": 13800
    },
    {
      "epoch": 0.142673705236125,
      "grad_norm": 5.961930751800537,
      "learning_rate": 4.762227476238034e-05,
      "loss": 3.1212,
      "step": 14000
    },
    {
      "epoch": 0.14471190102521247,
      "grad_norm": 2.9882707595825195,
      "learning_rate": 4.758830483256222e-05,
      "loss": 3.1037,
      "step": 14200
    },
    {
      "epoch": 0.14675009681429999,
      "grad_norm": 3.390658378601074,
      "learning_rate": 4.755433490274409e-05,
      "loss": 3.1471,
      "step": 14400
    },
    {
      "epoch": 0.14878829260338747,
      "grad_norm": 4.776495456695557,
      "learning_rate": 4.7520364972925965e-05,
      "loss": 3.1218,
      "step": 14600
    },
    {
      "epoch": 0.15082648839247498,
      "grad_norm": 3.462865114212036,
      "learning_rate": 4.7486395043107844e-05,
      "loss": 3.0414,
      "step": 14800
    },
    {
      "epoch": 0.1528646841815625,
      "grad_norm": 3.227867841720581,
      "learning_rate": 4.7452425113289716e-05,
      "loss": 3.0621,
      "step": 15000
    },
    {
      "epoch": 0.15490287997064997,
      "grad_norm": 3.4450249671936035,
      "learning_rate": 4.7418455183471596e-05,
      "loss": 3.0808,
      "step": 15200
    },
    {
      "epoch": 0.15694107575973748,
      "grad_norm": 6.344278335571289,
      "learning_rate": 4.738448525365347e-05,
      "loss": 3.0588,
      "step": 15400
    },
    {
      "epoch": 0.158979271548825,
      "grad_norm": 3.6270179748535156,
      "learning_rate": 4.735051532383534e-05,
      "loss": 3.0887,
      "step": 15600
    },
    {
      "epoch": 0.16101746733791247,
      "grad_norm": 4.711390018463135,
      "learning_rate": 4.731654539401722e-05,
      "loss": 3.1241,
      "step": 15800
    },
    {
      "epoch": 0.16305566312699998,
      "grad_norm": 3.8674983978271484,
      "learning_rate": 4.728257546419909e-05,
      "loss": 3.1321,
      "step": 16000
    },
    {
      "epoch": 0.16509385891608747,
      "grad_norm": 4.067463397979736,
      "learning_rate": 4.724860553438097e-05,
      "loss": 3.0435,
      "step": 16200
    },
    {
      "epoch": 0.16713205470517498,
      "grad_norm": 3.155003786087036,
      "learning_rate": 4.7214635604562843e-05,
      "loss": 3.061,
      "step": 16400
    },
    {
      "epoch": 0.1691702504942625,
      "grad_norm": 2.8848061561584473,
      "learning_rate": 4.7180665674744716e-05,
      "loss": 3.1188,
      "step": 16600
    },
    {
      "epoch": 0.17120844628334997,
      "grad_norm": 3.1662092208862305,
      "learning_rate": 4.7146695744926595e-05,
      "loss": 3.1296,
      "step": 16800
    },
    {
      "epoch": 0.17324664207243748,
      "grad_norm": 6.7743144035339355,
      "learning_rate": 4.711272581510847e-05,
      "loss": 3.0491,
      "step": 17000
    },
    {
      "epoch": 0.175284837861525,
      "grad_norm": 3.8755173683166504,
      "learning_rate": 4.7078755885290347e-05,
      "loss": 3.1058,
      "step": 17200
    },
    {
      "epoch": 0.17732303365061247,
      "grad_norm": 4.150558948516846,
      "learning_rate": 4.704478595547222e-05,
      "loss": 3.1119,
      "step": 17400
    },
    {
      "epoch": 0.17936122943969998,
      "grad_norm": 3.0356578826904297,
      "learning_rate": 4.701081602565409e-05,
      "loss": 3.0905,
      "step": 17600
    },
    {
      "epoch": 0.18139942522878746,
      "grad_norm": 2.9186975955963135,
      "learning_rate": 4.697684609583597e-05,
      "loss": 3.0456,
      "step": 17800
    },
    {
      "epoch": 0.18343762101787497,
      "grad_norm": 4.726252555847168,
      "learning_rate": 4.694287616601784e-05,
      "loss": 3.0586,
      "step": 18000
    },
    {
      "epoch": 0.18547581680696248,
      "grad_norm": 3.605436325073242,
      "learning_rate": 4.6908906236199715e-05,
      "loss": 3.095,
      "step": 18200
    },
    {
      "epoch": 0.18751401259604997,
      "grad_norm": 4.152395248413086,
      "learning_rate": 4.6874936306381594e-05,
      "loss": 3.1145,
      "step": 18400
    },
    {
      "epoch": 0.18955220838513748,
      "grad_norm": 3.2785496711730957,
      "learning_rate": 4.684096637656347e-05,
      "loss": 3.018,
      "step": 18600
    },
    {
      "epoch": 0.191590404174225,
      "grad_norm": 2.9670679569244385,
      "learning_rate": 4.6806996446745346e-05,
      "loss": 3.0468,
      "step": 18800
    },
    {
      "epoch": 0.19362859996331247,
      "grad_norm": 2.43912410736084,
      "learning_rate": 4.677302651692722e-05,
      "loss": 3.0775,
      "step": 19000
    },
    {
      "epoch": 0.19566679575239998,
      "grad_norm": 5.4011430740356445,
      "learning_rate": 4.673905658710909e-05,
      "loss": 3.0447,
      "step": 19200
    },
    {
      "epoch": 0.19770499154148746,
      "grad_norm": 4.187340259552002,
      "learning_rate": 4.670508665729097e-05,
      "loss": 3.0633,
      "step": 19400
    },
    {
      "epoch": 0.19974318733057497,
      "grad_norm": 3.7929599285125732,
      "learning_rate": 4.667111672747284e-05,
      "loss": 3.061,
      "step": 19600
    },
    {
      "epoch": 0.20178138311966248,
      "grad_norm": 2.9957661628723145,
      "learning_rate": 4.663714679765472e-05,
      "loss": 3.1352,
      "step": 19800
    },
    {
      "epoch": 0.20381957890874997,
      "grad_norm": 2.63325834274292,
      "learning_rate": 4.6603176867836594e-05,
      "loss": 3.0467,
      "step": 20000
    },
    {
      "epoch": 0.20585777469783748,
      "grad_norm": 3.544180154800415,
      "learning_rate": 4.6569206938018466e-05,
      "loss": 3.0426,
      "step": 20200
    },
    {
      "epoch": 0.20789597048692499,
      "grad_norm": 3.4033777713775635,
      "learning_rate": 4.6535237008200345e-05,
      "loss": 3.04,
      "step": 20400
    },
    {
      "epoch": 0.20993416627601247,
      "grad_norm": 2.3343887329101562,
      "learning_rate": 4.650126707838222e-05,
      "loss": 3.0729,
      "step": 20600
    },
    {
      "epoch": 0.21197236206509998,
      "grad_norm": 4.445541858673096,
      "learning_rate": 4.646729714856409e-05,
      "loss": 3.0668,
      "step": 20800
    },
    {
      "epoch": 0.21401055785418746,
      "grad_norm": 4.2030558586120605,
      "learning_rate": 4.643332721874597e-05,
      "loss": 3.0444,
      "step": 21000
    },
    {
      "epoch": 0.21604875364327497,
      "grad_norm": 4.698331356048584,
      "learning_rate": 4.639935728892784e-05,
      "loss": 3.0504,
      "step": 21200
    },
    {
      "epoch": 0.21808694943236248,
      "grad_norm": 5.228298187255859,
      "learning_rate": 4.636538735910972e-05,
      "loss": 3.0598,
      "step": 21400
    },
    {
      "epoch": 0.22012514522144996,
      "grad_norm": 3.065608501434326,
      "learning_rate": 4.633141742929159e-05,
      "loss": 3.0663,
      "step": 21600
    },
    {
      "epoch": 0.22216334101053747,
      "grad_norm": 2.750922203063965,
      "learning_rate": 4.6297447499473466e-05,
      "loss": 3.1208,
      "step": 21800
    },
    {
      "epoch": 0.22420153679962498,
      "grad_norm": 7.067135334014893,
      "learning_rate": 4.6263477569655345e-05,
      "loss": 3.066,
      "step": 22000
    },
    {
      "epoch": 0.22623973258871247,
      "grad_norm": 3.6371591091156006,
      "learning_rate": 4.622950763983722e-05,
      "loss": 3.0387,
      "step": 22200
    },
    {
      "epoch": 0.22827792837779998,
      "grad_norm": 5.675937175750732,
      "learning_rate": 4.6195537710019096e-05,
      "loss": 3.0176,
      "step": 22400
    },
    {
      "epoch": 0.23031612416688746,
      "grad_norm": 3.882448196411133,
      "learning_rate": 4.616156778020097e-05,
      "loss": 2.997,
      "step": 22600
    },
    {
      "epoch": 0.23235431995597497,
      "grad_norm": 3.7666831016540527,
      "learning_rate": 4.612759785038284e-05,
      "loss": 2.9778,
      "step": 22800
    },
    {
      "epoch": 0.23439251574506248,
      "grad_norm": 5.166198253631592,
      "learning_rate": 4.609362792056472e-05,
      "loss": 2.9888,
      "step": 23000
    },
    {
      "epoch": 0.23643071153414996,
      "grad_norm": 4.271574020385742,
      "learning_rate": 4.605965799074659e-05,
      "loss": 3.0728,
      "step": 23200
    },
    {
      "epoch": 0.23846890732323747,
      "grad_norm": 2.5934925079345703,
      "learning_rate": 4.6025688060928465e-05,
      "loss": 3.0491,
      "step": 23400
    },
    {
      "epoch": 0.24050710311232498,
      "grad_norm": 5.924987316131592,
      "learning_rate": 4.5991718131110344e-05,
      "loss": 3.0166,
      "step": 23600
    },
    {
      "epoch": 0.24254529890141246,
      "grad_norm": 2.527514934539795,
      "learning_rate": 4.5957748201292216e-05,
      "loss": 3.1568,
      "step": 23800
    },
    {
      "epoch": 0.24458349469049998,
      "grad_norm": 4.6422858238220215,
      "learning_rate": 4.5923778271474096e-05,
      "loss": 3.0396,
      "step": 24000
    },
    {
      "epoch": 0.24662169047958746,
      "grad_norm": 10.30395221710205,
      "learning_rate": 4.588980834165597e-05,
      "loss": 3.0169,
      "step": 24200
    },
    {
      "epoch": 0.24865988626867497,
      "grad_norm": 3.389484405517578,
      "learning_rate": 4.585583841183784e-05,
      "loss": 3.0485,
      "step": 24400
    },
    {
      "epoch": 0.25069808205776245,
      "grad_norm": 3.0084431171417236,
      "learning_rate": 4.582186848201972e-05,
      "loss": 3.0345,
      "step": 24600
    },
    {
      "epoch": 0.25273627784685,
      "grad_norm": 3.426323890686035,
      "learning_rate": 4.578789855220159e-05,
      "loss": 3.0985,
      "step": 24800
    },
    {
      "epoch": 0.25477447363593747,
      "grad_norm": 4.393150806427002,
      "learning_rate": 4.575392862238347e-05,
      "loss": 3.028,
      "step": 25000
    },
    {
      "epoch": 0.25681266942502495,
      "grad_norm": 5.221738815307617,
      "learning_rate": 4.5719958692565344e-05,
      "loss": 3.0055,
      "step": 25200
    },
    {
      "epoch": 0.2588508652141125,
      "grad_norm": 4.730093955993652,
      "learning_rate": 4.5685988762747216e-05,
      "loss": 3.07,
      "step": 25400
    },
    {
      "epoch": 0.2608890610032,
      "grad_norm": 2.699852705001831,
      "learning_rate": 4.5652018832929095e-05,
      "loss": 3.0266,
      "step": 25600
    },
    {
      "epoch": 0.26292725679228746,
      "grad_norm": 6.5174970626831055,
      "learning_rate": 4.561804890311097e-05,
      "loss": 3.068,
      "step": 25800
    },
    {
      "epoch": 0.264965452581375,
      "grad_norm": 3.4059150218963623,
      "learning_rate": 4.558407897329284e-05,
      "loss": 3.0247,
      "step": 26000
    },
    {
      "epoch": 0.2670036483704625,
      "grad_norm": 3.0144259929656982,
      "learning_rate": 4.555010904347472e-05,
      "loss": 3.0507,
      "step": 26200
    },
    {
      "epoch": 0.26904184415954996,
      "grad_norm": 3.2056453227996826,
      "learning_rate": 4.551613911365659e-05,
      "loss": 3.007,
      "step": 26400
    },
    {
      "epoch": 0.27108003994863744,
      "grad_norm": 2.449439287185669,
      "learning_rate": 4.548216918383847e-05,
      "loss": 3.0196,
      "step": 26600
    },
    {
      "epoch": 0.273118235737725,
      "grad_norm": 2.7961575984954834,
      "learning_rate": 4.544819925402034e-05,
      "loss": 3.0185,
      "step": 26800
    },
    {
      "epoch": 0.27515643152681246,
      "grad_norm": 3.6217968463897705,
      "learning_rate": 4.5414229324202215e-05,
      "loss": 2.9872,
      "step": 27000
    },
    {
      "epoch": 0.27719462731589994,
      "grad_norm": 2.994443655014038,
      "learning_rate": 4.5380259394384094e-05,
      "loss": 3.0429,
      "step": 27200
    },
    {
      "epoch": 0.2792328231049875,
      "grad_norm": 2.3351943492889404,
      "learning_rate": 4.534628946456597e-05,
      "loss": 3.0149,
      "step": 27400
    },
    {
      "epoch": 0.28127101889407496,
      "grad_norm": 6.825267791748047,
      "learning_rate": 4.5312319534747846e-05,
      "loss": 2.9863,
      "step": 27600
    },
    {
      "epoch": 0.28330921468316245,
      "grad_norm": 3.0878102779388428,
      "learning_rate": 4.527834960492972e-05,
      "loss": 3.0408,
      "step": 27800
    },
    {
      "epoch": 0.28534741047225,
      "grad_norm": 2.6365859508514404,
      "learning_rate": 4.524437967511159e-05,
      "loss": 2.9937,
      "step": 28000
    },
    {
      "epoch": 0.28738560626133747,
      "grad_norm": 3.356351137161255,
      "learning_rate": 4.521040974529347e-05,
      "loss": 2.9833,
      "step": 28200
    },
    {
      "epoch": 0.28942380205042495,
      "grad_norm": 2.7274956703186035,
      "learning_rate": 4.517643981547534e-05,
      "loss": 3.0109,
      "step": 28400
    },
    {
      "epoch": 0.2914619978395125,
      "grad_norm": 4.190547943115234,
      "learning_rate": 4.514246988565722e-05,
      "loss": 3.0054,
      "step": 28600
    },
    {
      "epoch": 0.29350019362859997,
      "grad_norm": 2.903484344482422,
      "learning_rate": 4.5108499955839094e-05,
      "loss": 3.0188,
      "step": 28800
    },
    {
      "epoch": 0.29553838941768745,
      "grad_norm": 3.6146421432495117,
      "learning_rate": 4.5074530026020966e-05,
      "loss": 3.0748,
      "step": 29000
    },
    {
      "epoch": 0.29757658520677494,
      "grad_norm": 3.937335968017578,
      "learning_rate": 4.5040560096202845e-05,
      "loss": 2.9852,
      "step": 29200
    },
    {
      "epoch": 0.2996147809958625,
      "grad_norm": 4.694575309753418,
      "learning_rate": 4.500659016638472e-05,
      "loss": 2.9988,
      "step": 29400
    },
    {
      "epoch": 0.30165297678494996,
      "grad_norm": 5.2949628829956055,
      "learning_rate": 4.497262023656659e-05,
      "loss": 3.0373,
      "step": 29600
    },
    {
      "epoch": 0.30369117257403744,
      "grad_norm": 3.2556278705596924,
      "learning_rate": 4.493865030674847e-05,
      "loss": 3.0245,
      "step": 29800
    },
    {
      "epoch": 0.305729368363125,
      "grad_norm": 5.743946075439453,
      "learning_rate": 4.490468037693034e-05,
      "loss": 3.0427,
      "step": 30000
    },
    {
      "epoch": 0.30776756415221246,
      "grad_norm": 3.819344997406006,
      "learning_rate": 4.487071044711222e-05,
      "loss": 2.991,
      "step": 30200
    },
    {
      "epoch": 0.30980575994129994,
      "grad_norm": 3.654238700866699,
      "learning_rate": 4.483674051729409e-05,
      "loss": 3.0585,
      "step": 30400
    },
    {
      "epoch": 0.3118439557303875,
      "grad_norm": 3.462470531463623,
      "learning_rate": 4.4802770587475966e-05,
      "loss": 2.9999,
      "step": 30600
    },
    {
      "epoch": 0.31388215151947496,
      "grad_norm": 2.893023729324341,
      "learning_rate": 4.4768800657657845e-05,
      "loss": 3.032,
      "step": 30800
    },
    {
      "epoch": 0.31592034730856244,
      "grad_norm": 6.425617218017578,
      "learning_rate": 4.473483072783972e-05,
      "loss": 2.9557,
      "step": 31000
    },
    {
      "epoch": 0.31795854309765,
      "grad_norm": 5.558531284332275,
      "learning_rate": 4.4700860798021596e-05,
      "loss": 3.0077,
      "step": 31200
    },
    {
      "epoch": 0.31999673888673746,
      "grad_norm": 2.6155741214752197,
      "learning_rate": 4.466689086820347e-05,
      "loss": 3.0333,
      "step": 31400
    },
    {
      "epoch": 0.32203493467582495,
      "grad_norm": 4.514867305755615,
      "learning_rate": 4.463292093838534e-05,
      "loss": 3.0622,
      "step": 31600
    },
    {
      "epoch": 0.3240731304649125,
      "grad_norm": 12.623297691345215,
      "learning_rate": 4.459895100856722e-05,
      "loss": 3.0295,
      "step": 31800
    },
    {
      "epoch": 0.32611132625399997,
      "grad_norm": 2.8749053478240967,
      "learning_rate": 4.456498107874909e-05,
      "loss": 3.0049,
      "step": 32000
    },
    {
      "epoch": 0.32814952204308745,
      "grad_norm": 5.282417297363281,
      "learning_rate": 4.4531011148930965e-05,
      "loss": 3.0511,
      "step": 32200
    },
    {
      "epoch": 0.33018771783217493,
      "grad_norm": 7.3941192626953125,
      "learning_rate": 4.4497041219112844e-05,
      "loss": 3.022,
      "step": 32400
    },
    {
      "epoch": 0.33222591362126247,
      "grad_norm": 2.479275941848755,
      "learning_rate": 4.4463071289294717e-05,
      "loss": 2.968,
      "step": 32600
    },
    {
      "epoch": 0.33426410941034995,
      "grad_norm": 3.2214276790618896,
      "learning_rate": 4.4429101359476596e-05,
      "loss": 3.0,
      "step": 32800
    },
    {
      "epoch": 0.33630230519943743,
      "grad_norm": 3.86893367767334,
      "learning_rate": 4.439513142965847e-05,
      "loss": 3.0103,
      "step": 33000
    },
    {
      "epoch": 0.338340500988525,
      "grad_norm": 3.720561981201172,
      "learning_rate": 4.436116149984034e-05,
      "loss": 3.0501,
      "step": 33200
    },
    {
      "epoch": 0.34037869677761246,
      "grad_norm": 4.453960418701172,
      "learning_rate": 4.432719157002222e-05,
      "loss": 3.0187,
      "step": 33400
    },
    {
      "epoch": 0.34241689256669994,
      "grad_norm": 6.839476108551025,
      "learning_rate": 4.429322164020409e-05,
      "loss": 2.9937,
      "step": 33600
    },
    {
      "epoch": 0.3444550883557875,
      "grad_norm": 4.593486785888672,
      "learning_rate": 4.425925171038597e-05,
      "loss": 2.9384,
      "step": 33800
    },
    {
      "epoch": 0.34649328414487496,
      "grad_norm": 7.020261764526367,
      "learning_rate": 4.4225281780567844e-05,
      "loss": 2.9795,
      "step": 34000
    },
    {
      "epoch": 0.34853147993396244,
      "grad_norm": 5.589587211608887,
      "learning_rate": 4.4191311850749716e-05,
      "loss": 3.0377,
      "step": 34200
    },
    {
      "epoch": 0.35056967572305,
      "grad_norm": 3.3932313919067383,
      "learning_rate": 4.4157341920931595e-05,
      "loss": 3.0289,
      "step": 34400
    },
    {
      "epoch": 0.35260787151213746,
      "grad_norm": 3.8503668308258057,
      "learning_rate": 4.412337199111347e-05,
      "loss": 3.0507,
      "step": 34600
    },
    {
      "epoch": 0.35464606730122494,
      "grad_norm": 6.169458866119385,
      "learning_rate": 4.408940206129534e-05,
      "loss": 3.0286,
      "step": 34800
    },
    {
      "epoch": 0.3566842630903125,
      "grad_norm": 2.041290760040283,
      "learning_rate": 4.405543213147722e-05,
      "loss": 2.9653,
      "step": 35000
    },
    {
      "epoch": 0.35872245887939996,
      "grad_norm": 3.705859899520874,
      "learning_rate": 4.402146220165909e-05,
      "loss": 3.0291,
      "step": 35200
    },
    {
      "epoch": 0.36076065466848745,
      "grad_norm": 2.9485018253326416,
      "learning_rate": 4.398749227184097e-05,
      "loss": 3.0413,
      "step": 35400
    },
    {
      "epoch": 0.36279885045757493,
      "grad_norm": 4.0915751457214355,
      "learning_rate": 4.395352234202284e-05,
      "loss": 3.0155,
      "step": 35600
    },
    {
      "epoch": 0.36483704624666247,
      "grad_norm": 3.3842267990112305,
      "learning_rate": 4.3919552412204715e-05,
      "loss": 3.0357,
      "step": 35800
    },
    {
      "epoch": 0.36687524203574995,
      "grad_norm": 4.542517185211182,
      "learning_rate": 4.3885582482386595e-05,
      "loss": 2.9834,
      "step": 36000
    },
    {
      "epoch": 0.36891343782483743,
      "grad_norm": 3.1330127716064453,
      "learning_rate": 4.385161255256847e-05,
      "loss": 2.935,
      "step": 36200
    },
    {
      "epoch": 0.37095163361392497,
      "grad_norm": 3.0294644832611084,
      "learning_rate": 4.3817642622750346e-05,
      "loss": 3.0017,
      "step": 36400
    },
    {
      "epoch": 0.37298982940301245,
      "grad_norm": 3.894557476043701,
      "learning_rate": 4.378367269293222e-05,
      "loss": 2.9946,
      "step": 36600
    },
    {
      "epoch": 0.37502802519209993,
      "grad_norm": 4.064080715179443,
      "learning_rate": 4.374970276311409e-05,
      "loss": 2.9943,
      "step": 36800
    },
    {
      "epoch": 0.37706622098118747,
      "grad_norm": 4.141182899475098,
      "learning_rate": 4.371573283329597e-05,
      "loss": 2.9673,
      "step": 37000
    },
    {
      "epoch": 0.37910441677027495,
      "grad_norm": 3.538997173309326,
      "learning_rate": 4.368176290347784e-05,
      "loss": 2.9837,
      "step": 37200
    },
    {
      "epoch": 0.38114261255936244,
      "grad_norm": 3.54469895362854,
      "learning_rate": 4.3647792973659715e-05,
      "loss": 3.0096,
      "step": 37400
    },
    {
      "epoch": 0.38318080834845,
      "grad_norm": 4.254579544067383,
      "learning_rate": 4.3613823043841594e-05,
      "loss": 3.0415,
      "step": 37600
    },
    {
      "epoch": 0.38521900413753746,
      "grad_norm": 5.319736003875732,
      "learning_rate": 4.3579853114023466e-05,
      "loss": 2.9468,
      "step": 37800
    },
    {
      "epoch": 0.38725719992662494,
      "grad_norm": 4.92690372467041,
      "learning_rate": 4.3545883184205345e-05,
      "loss": 2.9997,
      "step": 38000
    },
    {
      "epoch": 0.3892953957157125,
      "grad_norm": 4.050243377685547,
      "learning_rate": 4.351191325438722e-05,
      "loss": 2.9774,
      "step": 38200
    },
    {
      "epoch": 0.39133359150479996,
      "grad_norm": 2.598417043685913,
      "learning_rate": 4.347794332456909e-05,
      "loss": 2.991,
      "step": 38400
    },
    {
      "epoch": 0.39337178729388744,
      "grad_norm": 3.871670722961426,
      "learning_rate": 4.344397339475097e-05,
      "loss": 2.9444,
      "step": 38600
    },
    {
      "epoch": 0.3954099830829749,
      "grad_norm": 4.35900354385376,
      "learning_rate": 4.341000346493284e-05,
      "loss": 2.9919,
      "step": 38800
    },
    {
      "epoch": 0.39744817887206246,
      "grad_norm": 2.429495334625244,
      "learning_rate": 4.337603353511472e-05,
      "loss": 3.0115,
      "step": 39000
    },
    {
      "epoch": 0.39948637466114995,
      "grad_norm": 4.187370777130127,
      "learning_rate": 4.334206360529659e-05,
      "loss": 2.963,
      "step": 39200
    },
    {
      "epoch": 0.40152457045023743,
      "grad_norm": 2.5322747230529785,
      "learning_rate": 4.3308093675478466e-05,
      "loss": 2.9876,
      "step": 39400
    },
    {
      "epoch": 0.40356276623932497,
      "grad_norm": 3.609203577041626,
      "learning_rate": 4.3274123745660345e-05,
      "loss": 3.0118,
      "step": 39600
    },
    {
      "epoch": 0.40560096202841245,
      "grad_norm": 4.3017144203186035,
      "learning_rate": 4.324015381584222e-05,
      "loss": 3.0049,
      "step": 39800
    },
    {
      "epoch": 0.40763915781749993,
      "grad_norm": 2.960871696472168,
      "learning_rate": 4.3206183886024096e-05,
      "loss": 3.0414,
      "step": 40000
    },
    {
      "epoch": 0.40967735360658747,
      "grad_norm": 3.742852210998535,
      "learning_rate": 4.317221395620597e-05,
      "loss": 2.9629,
      "step": 40200
    },
    {
      "epoch": 0.41171554939567495,
      "grad_norm": 3.5822815895080566,
      "learning_rate": 4.313824402638784e-05,
      "loss": 2.9729,
      "step": 40400
    },
    {
      "epoch": 0.41375374518476243,
      "grad_norm": 3.5408456325531006,
      "learning_rate": 4.310427409656972e-05,
      "loss": 2.8998,
      "step": 40600
    },
    {
      "epoch": 0.41579194097384997,
      "grad_norm": 5.705649375915527,
      "learning_rate": 4.307030416675159e-05,
      "loss": 3.049,
      "step": 40800
    },
    {
      "epoch": 0.41783013676293745,
      "grad_norm": 5.625848770141602,
      "learning_rate": 4.3036334236933465e-05,
      "loss": 2.9945,
      "step": 41000
    },
    {
      "epoch": 0.41986833255202494,
      "grad_norm": 4.349392414093018,
      "learning_rate": 4.3002364307115344e-05,
      "loss": 3.0259,
      "step": 41200
    },
    {
      "epoch": 0.4219065283411125,
      "grad_norm": 5.075165748596191,
      "learning_rate": 4.296839437729722e-05,
      "loss": 3.021,
      "step": 41400
    },
    {
      "epoch": 0.42394472413019996,
      "grad_norm": 4.133294582366943,
      "learning_rate": 4.2934424447479096e-05,
      "loss": 3.0502,
      "step": 41600
    },
    {
      "epoch": 0.42598291991928744,
      "grad_norm": 4.651672840118408,
      "learning_rate": 4.290045451766097e-05,
      "loss": 2.9475,
      "step": 41800
    },
    {
      "epoch": 0.4280211157083749,
      "grad_norm": 3.0752816200256348,
      "learning_rate": 4.286648458784284e-05,
      "loss": 3.0137,
      "step": 42000
    },
    {
      "epoch": 0.43005931149746246,
      "grad_norm": 3.5784904956817627,
      "learning_rate": 4.283251465802472e-05,
      "loss": 2.9474,
      "step": 42200
    },
    {
      "epoch": 0.43209750728654994,
      "grad_norm": 3.4314112663269043,
      "learning_rate": 4.279854472820659e-05,
      "loss": 2.9844,
      "step": 42400
    },
    {
      "epoch": 0.4341357030756374,
      "grad_norm": 3.1435091495513916,
      "learning_rate": 4.276457479838847e-05,
      "loss": 2.9148,
      "step": 42600
    },
    {
      "epoch": 0.43617389886472496,
      "grad_norm": 2.845345973968506,
      "learning_rate": 4.2730604868570344e-05,
      "loss": 2.999,
      "step": 42800
    },
    {
      "epoch": 0.43821209465381245,
      "grad_norm": 4.293771743774414,
      "learning_rate": 4.2696634938752216e-05,
      "loss": 2.9722,
      "step": 43000
    },
    {
      "epoch": 0.4402502904428999,
      "grad_norm": 2.712568998336792,
      "learning_rate": 4.2662665008934095e-05,
      "loss": 3.0487,
      "step": 43200
    },
    {
      "epoch": 0.44228848623198747,
      "grad_norm": 3.6006968021392822,
      "learning_rate": 4.2628695079115974e-05,
      "loss": 2.9849,
      "step": 43400
    },
    {
      "epoch": 0.44432668202107495,
      "grad_norm": 3.0054073333740234,
      "learning_rate": 4.259472514929784e-05,
      "loss": 3.019,
      "step": 43600
    },
    {
      "epoch": 0.44636487781016243,
      "grad_norm": 2.5167484283447266,
      "learning_rate": 4.256075521947972e-05,
      "loss": 2.9603,
      "step": 43800
    },
    {
      "epoch": 0.44840307359924997,
      "grad_norm": 4.565790176391602,
      "learning_rate": 4.252678528966159e-05,
      "loss": 2.9618,
      "step": 44000
    },
    {
      "epoch": 0.45044126938833745,
      "grad_norm": 2.869408369064331,
      "learning_rate": 4.249281535984347e-05,
      "loss": 2.9423,
      "step": 44200
    },
    {
      "epoch": 0.45247946517742493,
      "grad_norm": 2.8330843448638916,
      "learning_rate": 4.245884543002534e-05,
      "loss": 2.9585,
      "step": 44400
    },
    {
      "epoch": 0.4545176609665124,
      "grad_norm": 3.1977787017822266,
      "learning_rate": 4.2424875500207215e-05,
      "loss": 2.9491,
      "step": 44600
    },
    {
      "epoch": 0.45655585675559995,
      "grad_norm": 2.504209280014038,
      "learning_rate": 4.2390905570389095e-05,
      "loss": 2.9547,
      "step": 44800
    },
    {
      "epoch": 0.45859405254468744,
      "grad_norm": 3.0056142807006836,
      "learning_rate": 4.235693564057097e-05,
      "loss": 2.9186,
      "step": 45000
    },
    {
      "epoch": 0.4606322483337749,
      "grad_norm": 2.5327982902526855,
      "learning_rate": 4.2322965710752846e-05,
      "loss": 2.9861,
      "step": 45200
    },
    {
      "epoch": 0.46267044412286246,
      "grad_norm": 3.129002809524536,
      "learning_rate": 4.228899578093472e-05,
      "loss": 3.027,
      "step": 45400
    },
    {
      "epoch": 0.46470863991194994,
      "grad_norm": 4.0697174072265625,
      "learning_rate": 4.225502585111659e-05,
      "loss": 2.9635,
      "step": 45600
    },
    {
      "epoch": 0.4667468357010374,
      "grad_norm": 3.812849760055542,
      "learning_rate": 4.222105592129847e-05,
      "loss": 2.9409,
      "step": 45800
    },
    {
      "epoch": 0.46878503149012496,
      "grad_norm": 3.8978641033172607,
      "learning_rate": 4.218708599148035e-05,
      "loss": 2.9147,
      "step": 46000
    },
    {
      "epoch": 0.47082322727921244,
      "grad_norm": 3.70408296585083,
      "learning_rate": 4.2153116061662215e-05,
      "loss": 2.9103,
      "step": 46200
    },
    {
      "epoch": 0.4728614230682999,
      "grad_norm": 4.360507488250732,
      "learning_rate": 4.2119146131844094e-05,
      "loss": 2.9492,
      "step": 46400
    },
    {
      "epoch": 0.47489961885738746,
      "grad_norm": 4.822653293609619,
      "learning_rate": 4.2085176202025966e-05,
      "loss": 2.9402,
      "step": 46600
    },
    {
      "epoch": 0.47693781464647494,
      "grad_norm": 3.7054286003112793,
      "learning_rate": 4.2051206272207846e-05,
      "loss": 3.0125,
      "step": 46800
    },
    {
      "epoch": 0.4789760104355624,
      "grad_norm": 2.707651376724243,
      "learning_rate": 4.201723634238972e-05,
      "loss": 2.9743,
      "step": 47000
    },
    {
      "epoch": 0.48101420622464997,
      "grad_norm": 3.5092263221740723,
      "learning_rate": 4.198326641257159e-05,
      "loss": 2.9404,
      "step": 47200
    },
    {
      "epoch": 0.48305240201373745,
      "grad_norm": 4.531455993652344,
      "learning_rate": 4.194929648275347e-05,
      "loss": 3.04,
      "step": 47400
    },
    {
      "epoch": 0.48509059780282493,
      "grad_norm": 4.530540466308594,
      "learning_rate": 4.191532655293534e-05,
      "loss": 2.9886,
      "step": 47600
    },
    {
      "epoch": 0.4871287935919124,
      "grad_norm": 3.036961793899536,
      "learning_rate": 4.188135662311722e-05,
      "loss": 2.9807,
      "step": 47800
    },
    {
      "epoch": 0.48916698938099995,
      "grad_norm": 3.1458914279937744,
      "learning_rate": 4.184738669329909e-05,
      "loss": 2.9706,
      "step": 48000
    },
    {
      "epoch": 0.49120518517008743,
      "grad_norm": 5.10888671875,
      "learning_rate": 4.1813416763480966e-05,
      "loss": 2.9782,
      "step": 48200
    },
    {
      "epoch": 0.4932433809591749,
      "grad_norm": 3.7169418334960938,
      "learning_rate": 4.1779446833662845e-05,
      "loss": 2.9581,
      "step": 48400
    },
    {
      "epoch": 0.49528157674826245,
      "grad_norm": 3.3279385566711426,
      "learning_rate": 4.1745476903844724e-05,
      "loss": 2.9267,
      "step": 48600
    },
    {
      "epoch": 0.49731977253734994,
      "grad_norm": 5.063906669616699,
      "learning_rate": 4.171150697402659e-05,
      "loss": 2.9837,
      "step": 48800
    },
    {
      "epoch": 0.4993579683264374,
      "grad_norm": 3.428044557571411,
      "learning_rate": 4.167753704420847e-05,
      "loss": 3.0572,
      "step": 49000
    },
    {
      "epoch": 0.5013961641155249,
      "grad_norm": 4.411882400512695,
      "learning_rate": 4.164356711439034e-05,
      "loss": 2.9702,
      "step": 49200
    },
    {
      "epoch": 0.5034343599046124,
      "grad_norm": 4.176815509796143,
      "learning_rate": 4.160959718457222e-05,
      "loss": 2.9439,
      "step": 49400
    },
    {
      "epoch": 0.5054725556937,
      "grad_norm": 4.500669002532959,
      "learning_rate": 4.157562725475409e-05,
      "loss": 2.9445,
      "step": 49600
    },
    {
      "epoch": 0.5075107514827875,
      "grad_norm": 6.213396072387695,
      "learning_rate": 4.1541657324935965e-05,
      "loss": 3.0443,
      "step": 49800
    },
    {
      "epoch": 0.5095489472718749,
      "grad_norm": 3.016636848449707,
      "learning_rate": 4.1507687395117844e-05,
      "loss": 2.9659,
      "step": 50000
    },
    {
      "epoch": 0.5115871430609624,
      "grad_norm": Infinity,
      "learning_rate": 4.147371746529972e-05,
      "loss": 3.005,
      "step": 50200
    },
    {
      "epoch": 0.5136253388500499,
      "grad_norm": 2.6945040225982666,
      "learning_rate": 4.1439747535481596e-05,
      "loss": 2.9831,
      "step": 50400
    },
    {
      "epoch": 0.5156635346391374,
      "grad_norm": 3.379640817642212,
      "learning_rate": 4.140577760566347e-05,
      "loss": 2.9101,
      "step": 50600
    },
    {
      "epoch": 0.517701730428225,
      "grad_norm": 4.516024112701416,
      "learning_rate": 4.137180767584534e-05,
      "loss": 2.9292,
      "step": 50800
    },
    {
      "epoch": 0.5197399262173125,
      "grad_norm": 3.630607843399048,
      "learning_rate": 4.133783774602722e-05,
      "loss": 2.9287,
      "step": 51000
    },
    {
      "epoch": 0.5217781220064,
      "grad_norm": 2.5457615852355957,
      "learning_rate": 4.13038678162091e-05,
      "loss": 2.9268,
      "step": 51200
    },
    {
      "epoch": 0.5238163177954874,
      "grad_norm": 3.9504473209381104,
      "learning_rate": 4.126989788639097e-05,
      "loss": 2.9815,
      "step": 51400
    },
    {
      "epoch": 0.5258545135845749,
      "grad_norm": 2.9003663063049316,
      "learning_rate": 4.1235927956572844e-05,
      "loss": 2.9514,
      "step": 51600
    },
    {
      "epoch": 0.5278927093736624,
      "grad_norm": 5.572474479675293,
      "learning_rate": 4.1201958026754716e-05,
      "loss": 3.0072,
      "step": 51800
    },
    {
      "epoch": 0.52993090516275,
      "grad_norm": 3.3130853176116943,
      "learning_rate": 4.1167988096936595e-05,
      "loss": 3.0058,
      "step": 52000
    },
    {
      "epoch": 0.5319691009518375,
      "grad_norm": 3.2833828926086426,
      "learning_rate": 4.1134018167118474e-05,
      "loss": 3.0027,
      "step": 52200
    },
    {
      "epoch": 0.534007296740925,
      "grad_norm": 2.8344197273254395,
      "learning_rate": 4.110004823730034e-05,
      "loss": 2.9467,
      "step": 52400
    },
    {
      "epoch": 0.5360454925300124,
      "grad_norm": 4.333953380584717,
      "learning_rate": 4.106607830748222e-05,
      "loss": 2.9127,
      "step": 52600
    },
    {
      "epoch": 0.5380836883190999,
      "grad_norm": 3.6435375213623047,
      "learning_rate": 4.103210837766409e-05,
      "loss": 2.9818,
      "step": 52800
    },
    {
      "epoch": 0.5401218841081874,
      "grad_norm": 2.338719129562378,
      "learning_rate": 4.099813844784597e-05,
      "loss": 2.9483,
      "step": 53000
    },
    {
      "epoch": 0.5421600798972749,
      "grad_norm": 3.6328771114349365,
      "learning_rate": 4.096416851802784e-05,
      "loss": 2.9691,
      "step": 53200
    },
    {
      "epoch": 0.5441982756863625,
      "grad_norm": 3.4364824295043945,
      "learning_rate": 4.0930198588209715e-05,
      "loss": 2.9177,
      "step": 53400
    },
    {
      "epoch": 0.54623647147545,
      "grad_norm": 2.865523099899292,
      "learning_rate": 4.0896228658391595e-05,
      "loss": 2.9376,
      "step": 53600
    },
    {
      "epoch": 0.5482746672645374,
      "grad_norm": 4.355450630187988,
      "learning_rate": 4.0862258728573474e-05,
      "loss": 3.0161,
      "step": 53800
    },
    {
      "epoch": 0.5503128630536249,
      "grad_norm": 2.972644805908203,
      "learning_rate": 4.0828288798755346e-05,
      "loss": 2.9691,
      "step": 54000
    },
    {
      "epoch": 0.5523510588427124,
      "grad_norm": 4.201916694641113,
      "learning_rate": 4.079431886893722e-05,
      "loss": 2.9876,
      "step": 54200
    },
    {
      "epoch": 0.5543892546317999,
      "grad_norm": 3.4882311820983887,
      "learning_rate": 4.076034893911909e-05,
      "loss": 2.9815,
      "step": 54400
    },
    {
      "epoch": 0.5564274504208875,
      "grad_norm": 3.7565314769744873,
      "learning_rate": 4.072637900930097e-05,
      "loss": 2.9978,
      "step": 54600
    },
    {
      "epoch": 0.558465646209975,
      "grad_norm": 3.5404489040374756,
      "learning_rate": 4.069240907948285e-05,
      "loss": 2.9774,
      "step": 54800
    },
    {
      "epoch": 0.5605038419990624,
      "grad_norm": 3.1043331623077393,
      "learning_rate": 4.0658439149664715e-05,
      "loss": 2.9652,
      "step": 55000
    },
    {
      "epoch": 0.5625420377881499,
      "grad_norm": 2.3844940662384033,
      "learning_rate": 4.0624469219846594e-05,
      "loss": 2.9605,
      "step": 55200
    },
    {
      "epoch": 0.5645802335772374,
      "grad_norm": 3.2901711463928223,
      "learning_rate": 4.0590499290028466e-05,
      "loss": 3.0171,
      "step": 55400
    },
    {
      "epoch": 0.5666184293663249,
      "grad_norm": 2.2684574127197266,
      "learning_rate": 4.0556529360210346e-05,
      "loss": 2.9178,
      "step": 55600
    },
    {
      "epoch": 0.5686566251554124,
      "grad_norm": 3.5814127922058105,
      "learning_rate": 4.052255943039222e-05,
      "loss": 2.9223,
      "step": 55800
    },
    {
      "epoch": 0.5706948209445,
      "grad_norm": 4.333622455596924,
      "learning_rate": 4.048858950057409e-05,
      "loss": 2.9467,
      "step": 56000
    },
    {
      "epoch": 0.5727330167335875,
      "grad_norm": 6.657859802246094,
      "learning_rate": 4.045461957075597e-05,
      "loss": 2.9933,
      "step": 56200
    },
    {
      "epoch": 0.5747712125226749,
      "grad_norm": 3.5546703338623047,
      "learning_rate": 4.042064964093784e-05,
      "loss": 2.8994,
      "step": 56400
    },
    {
      "epoch": 0.5768094083117624,
      "grad_norm": 2.671731948852539,
      "learning_rate": 4.038667971111972e-05,
      "loss": 2.9415,
      "step": 56600
    },
    {
      "epoch": 0.5788476041008499,
      "grad_norm": 2.7782042026519775,
      "learning_rate": 4.0352709781301593e-05,
      "loss": 2.9616,
      "step": 56800
    },
    {
      "epoch": 0.5808857998899374,
      "grad_norm": 3.991507053375244,
      "learning_rate": 4.0318739851483466e-05,
      "loss": 2.9007,
      "step": 57000
    },
    {
      "epoch": 0.582923995679025,
      "grad_norm": 3.7495243549346924,
      "learning_rate": 4.0284769921665345e-05,
      "loss": 2.9881,
      "step": 57200
    },
    {
      "epoch": 0.5849621914681125,
      "grad_norm": 4.472881317138672,
      "learning_rate": 4.0250799991847224e-05,
      "loss": 2.9196,
      "step": 57400
    },
    {
      "epoch": 0.5870003872571999,
      "grad_norm": 4.7344489097595215,
      "learning_rate": 4.021683006202909e-05,
      "loss": 2.9445,
      "step": 57600
    },
    {
      "epoch": 0.5890385830462874,
      "grad_norm": 3.1975815296173096,
      "learning_rate": 4.018286013221097e-05,
      "loss": 2.9866,
      "step": 57800
    },
    {
      "epoch": 0.5910767788353749,
      "grad_norm": 3.7177605628967285,
      "learning_rate": 4.014889020239284e-05,
      "loss": 2.9668,
      "step": 58000
    },
    {
      "epoch": 0.5931149746244624,
      "grad_norm": 6.456859588623047,
      "learning_rate": 4.011492027257472e-05,
      "loss": 2.9636,
      "step": 58200
    },
    {
      "epoch": 0.5951531704135499,
      "grad_norm": 3.7640106678009033,
      "learning_rate": 4.008095034275659e-05,
      "loss": 2.9601,
      "step": 58400
    },
    {
      "epoch": 0.5971913662026375,
      "grad_norm": 4.335210800170898,
      "learning_rate": 4.0046980412938465e-05,
      "loss": 2.9436,
      "step": 58600
    },
    {
      "epoch": 0.599229561991725,
      "grad_norm": 3.7794766426086426,
      "learning_rate": 4.0013010483120344e-05,
      "loss": 2.9975,
      "step": 58800
    },
    {
      "epoch": 0.6012677577808124,
      "grad_norm": 4.216962814331055,
      "learning_rate": 3.997904055330222e-05,
      "loss": 3.0354,
      "step": 59000
    },
    {
      "epoch": 0.6033059535698999,
      "grad_norm": 3.3578927516937256,
      "learning_rate": 3.9945070623484096e-05,
      "loss": 2.9369,
      "step": 59200
    },
    {
      "epoch": 0.6053441493589874,
      "grad_norm": 3.2932229042053223,
      "learning_rate": 3.991110069366597e-05,
      "loss": 2.9553,
      "step": 59400
    },
    {
      "epoch": 0.6073823451480749,
      "grad_norm": 3.331023931503296,
      "learning_rate": 3.987713076384784e-05,
      "loss": 2.9269,
      "step": 59600
    },
    {
      "epoch": 0.6094205409371625,
      "grad_norm": 2.1570792198181152,
      "learning_rate": 3.984316083402972e-05,
      "loss": 2.97,
      "step": 59800
    },
    {
      "epoch": 0.61145873672625,
      "grad_norm": 4.644363880157471,
      "learning_rate": 3.98091909042116e-05,
      "loss": 2.8983,
      "step": 60000
    },
    {
      "epoch": 0.6134969325153374,
      "grad_norm": 3.465096950531006,
      "learning_rate": 3.9775220974393465e-05,
      "loss": 2.932,
      "step": 60200
    },
    {
      "epoch": 0.6155351283044249,
      "grad_norm": 3.4772145748138428,
      "learning_rate": 3.9741251044575344e-05,
      "loss": 2.9396,
      "step": 60400
    },
    {
      "epoch": 0.6175733240935124,
      "grad_norm": 3.786860704421997,
      "learning_rate": 3.9707281114757216e-05,
      "loss": 2.897,
      "step": 60600
    },
    {
      "epoch": 0.6196115198825999,
      "grad_norm": 3.1177103519439697,
      "learning_rate": 3.9673311184939095e-05,
      "loss": 2.9404,
      "step": 60800
    },
    {
      "epoch": 0.6216497156716875,
      "grad_norm": 3.0359420776367188,
      "learning_rate": 3.963934125512097e-05,
      "loss": 2.9431,
      "step": 61000
    },
    {
      "epoch": 0.623687911460775,
      "grad_norm": 3.9050402641296387,
      "learning_rate": 3.960537132530284e-05,
      "loss": 2.9136,
      "step": 61200
    },
    {
      "epoch": 0.6257261072498624,
      "grad_norm": 4.044665336608887,
      "learning_rate": 3.957140139548472e-05,
      "loss": 2.9608,
      "step": 61400
    },
    {
      "epoch": 0.6277643030389499,
      "grad_norm": 3.2130982875823975,
      "learning_rate": 3.953743146566659e-05,
      "loss": 2.9556,
      "step": 61600
    },
    {
      "epoch": 0.6298024988280374,
      "grad_norm": 5.902356147766113,
      "learning_rate": 3.950346153584847e-05,
      "loss": 2.9772,
      "step": 61800
    },
    {
      "epoch": 0.6318406946171249,
      "grad_norm": 3.2973523139953613,
      "learning_rate": 3.946949160603034e-05,
      "loss": 2.9627,
      "step": 62000
    },
    {
      "epoch": 0.6338788904062124,
      "grad_norm": 2.9783146381378174,
      "learning_rate": 3.9435521676212216e-05,
      "loss": 3.0213,
      "step": 62200
    },
    {
      "epoch": 0.6359170861953,
      "grad_norm": 3.7909815311431885,
      "learning_rate": 3.9401551746394095e-05,
      "loss": 2.9645,
      "step": 62400
    },
    {
      "epoch": 0.6379552819843874,
      "grad_norm": 3.496553659439087,
      "learning_rate": 3.9367581816575974e-05,
      "loss": 2.9366,
      "step": 62600
    },
    {
      "epoch": 0.6399934777734749,
      "grad_norm": 9.111281394958496,
      "learning_rate": 3.9333611886757846e-05,
      "loss": 2.9403,
      "step": 62800
    },
    {
      "epoch": 0.6420316735625624,
      "grad_norm": 2.824331521987915,
      "learning_rate": 3.929964195693972e-05,
      "loss": 2.9459,
      "step": 63000
    },
    {
      "epoch": 0.6440698693516499,
      "grad_norm": 3.4921138286590576,
      "learning_rate": 3.926567202712159e-05,
      "loss": 2.9126,
      "step": 63200
    },
    {
      "epoch": 0.6461080651407374,
      "grad_norm": 3.6825132369995117,
      "learning_rate": 3.923170209730347e-05,
      "loss": 2.9709,
      "step": 63400
    },
    {
      "epoch": 0.648146260929825,
      "grad_norm": 5.313824653625488,
      "learning_rate": 3.919773216748535e-05,
      "loss": 2.9749,
      "step": 63600
    },
    {
      "epoch": 0.6501844567189125,
      "grad_norm": 3.704821825027466,
      "learning_rate": 3.9163762237667215e-05,
      "loss": 2.8605,
      "step": 63800
    },
    {
      "epoch": 0.6522226525079999,
      "grad_norm": 6.98016357421875,
      "learning_rate": 3.9129792307849094e-05,
      "loss": 2.9244,
      "step": 64000
    },
    {
      "epoch": 0.6542608482970874,
      "grad_norm": 11.214323997497559,
      "learning_rate": 3.9095822378030966e-05,
      "loss": 2.9109,
      "step": 64200
    },
    {
      "epoch": 0.6562990440861749,
      "grad_norm": 2.7806320190429688,
      "learning_rate": 3.9061852448212846e-05,
      "loss": 2.9498,
      "step": 64400
    },
    {
      "epoch": 0.6583372398752624,
      "grad_norm": 3.369839906692505,
      "learning_rate": 3.902788251839472e-05,
      "loss": 2.9275,
      "step": 64600
    },
    {
      "epoch": 0.6603754356643499,
      "grad_norm": 4.217713356018066,
      "learning_rate": 3.899391258857659e-05,
      "loss": 2.9574,
      "step": 64800
    },
    {
      "epoch": 0.6624136314534375,
      "grad_norm": 3.1710758209228516,
      "learning_rate": 3.895994265875847e-05,
      "loss": 2.9054,
      "step": 65000
    },
    {
      "epoch": 0.6644518272425249,
      "grad_norm": 11.305243492126465,
      "learning_rate": 3.892597272894035e-05,
      "loss": 2.9549,
      "step": 65200
    },
    {
      "epoch": 0.6664900230316124,
      "grad_norm": 4.263027667999268,
      "learning_rate": 3.889200279912222e-05,
      "loss": 2.9539,
      "step": 65400
    },
    {
      "epoch": 0.6685282188206999,
      "grad_norm": 4.4479660987854,
      "learning_rate": 3.8858032869304093e-05,
      "loss": 2.9074,
      "step": 65600
    },
    {
      "epoch": 0.6705664146097874,
      "grad_norm": 4.279574871063232,
      "learning_rate": 3.8824062939485966e-05,
      "loss": 2.9467,
      "step": 65800
    },
    {
      "epoch": 0.6726046103988749,
      "grad_norm": 2.2370381355285645,
      "learning_rate": 3.8790093009667845e-05,
      "loss": 2.933,
      "step": 66000
    },
    {
      "epoch": 0.6746428061879625,
      "grad_norm": 3.940809488296509,
      "learning_rate": 3.8756123079849724e-05,
      "loss": 2.9762,
      "step": 66200
    },
    {
      "epoch": 0.67668100197705,
      "grad_norm": 8.825657844543457,
      "learning_rate": 3.872215315003159e-05,
      "loss": 2.9328,
      "step": 66400
    },
    {
      "epoch": 0.6787191977661374,
      "grad_norm": 3.04782772064209,
      "learning_rate": 3.868818322021347e-05,
      "loss": 2.9036,
      "step": 66600
    },
    {
      "epoch": 0.6807573935552249,
      "grad_norm": 4.322646141052246,
      "learning_rate": 3.865421329039534e-05,
      "loss": 2.9292,
      "step": 66800
    },
    {
      "epoch": 0.6827955893443124,
      "grad_norm": 3.0551979541778564,
      "learning_rate": 3.862024336057722e-05,
      "loss": 2.923,
      "step": 67000
    },
    {
      "epoch": 0.6848337851333999,
      "grad_norm": 2.027256965637207,
      "learning_rate": 3.858627343075909e-05,
      "loss": 2.9585,
      "step": 67200
    },
    {
      "epoch": 0.6868719809224875,
      "grad_norm": 4.546934604644775,
      "learning_rate": 3.8552303500940965e-05,
      "loss": 2.8863,
      "step": 67400
    },
    {
      "epoch": 0.688910176711575,
      "grad_norm": 3.1931354999542236,
      "learning_rate": 3.8518333571122844e-05,
      "loss": 2.9006,
      "step": 67600
    },
    {
      "epoch": 0.6909483725006624,
      "grad_norm": 5.859183311462402,
      "learning_rate": 3.8484363641304724e-05,
      "loss": 2.9139,
      "step": 67800
    },
    {
      "epoch": 0.6929865682897499,
      "grad_norm": 3.8656229972839355,
      "learning_rate": 3.8450393711486596e-05,
      "loss": 2.9389,
      "step": 68000
    },
    {
      "epoch": 0.6950247640788374,
      "grad_norm": 3.2187306880950928,
      "learning_rate": 3.841642378166847e-05,
      "loss": 2.8918,
      "step": 68200
    },
    {
      "epoch": 0.6970629598679249,
      "grad_norm": 4.008880138397217,
      "learning_rate": 3.838245385185034e-05,
      "loss": 2.9136,
      "step": 68400
    },
    {
      "epoch": 0.6991011556570124,
      "grad_norm": 2.550382614135742,
      "learning_rate": 3.834848392203222e-05,
      "loss": 2.9878,
      "step": 68600
    },
    {
      "epoch": 0.7011393514461,
      "grad_norm": 4.18341064453125,
      "learning_rate": 3.83145139922141e-05,
      "loss": 2.9507,
      "step": 68800
    },
    {
      "epoch": 0.7031775472351874,
      "grad_norm": 3.1468470096588135,
      "learning_rate": 3.8280544062395965e-05,
      "loss": 2.8379,
      "step": 69000
    },
    {
      "epoch": 0.7052157430242749,
      "grad_norm": 4.287761211395264,
      "learning_rate": 3.8246574132577844e-05,
      "loss": 2.9546,
      "step": 69200
    },
    {
      "epoch": 0.7072539388133624,
      "grad_norm": 3.4771289825439453,
      "learning_rate": 3.8212604202759716e-05,
      "loss": 2.8873,
      "step": 69400
    },
    {
      "epoch": 0.7092921346024499,
      "grad_norm": 5.594413757324219,
      "learning_rate": 3.8178634272941595e-05,
      "loss": 2.886,
      "step": 69600
    },
    {
      "epoch": 0.7113303303915374,
      "grad_norm": 3.9218711853027344,
      "learning_rate": 3.814466434312347e-05,
      "loss": 2.9245,
      "step": 69800
    },
    {
      "epoch": 0.713368526180625,
      "grad_norm": 2.8989529609680176,
      "learning_rate": 3.811069441330534e-05,
      "loss": 2.9616,
      "step": 70000
    },
    {
      "epoch": 0.7154067219697124,
      "grad_norm": 4.2630767822265625,
      "learning_rate": 3.807672448348722e-05,
      "loss": 2.9504,
      "step": 70200
    },
    {
      "epoch": 0.7174449177587999,
      "grad_norm": 3.6326513290405273,
      "learning_rate": 3.80427545536691e-05,
      "loss": 2.9142,
      "step": 70400
    },
    {
      "epoch": 0.7194831135478874,
      "grad_norm": 3.465575933456421,
      "learning_rate": 3.800878462385097e-05,
      "loss": 2.9272,
      "step": 70600
    },
    {
      "epoch": 0.7215213093369749,
      "grad_norm": 3.359907865524292,
      "learning_rate": 3.797481469403284e-05,
      "loss": 2.9362,
      "step": 70800
    },
    {
      "epoch": 0.7235595051260624,
      "grad_norm": 2.3914616107940674,
      "learning_rate": 3.7940844764214716e-05,
      "loss": 2.9079,
      "step": 71000
    },
    {
      "epoch": 0.7255977009151499,
      "grad_norm": 5.521712303161621,
      "learning_rate": 3.7906874834396595e-05,
      "loss": 2.8426,
      "step": 71200
    },
    {
      "epoch": 0.7276358967042375,
      "grad_norm": 3.8047072887420654,
      "learning_rate": 3.7872904904578474e-05,
      "loss": 2.9112,
      "step": 71400
    },
    {
      "epoch": 0.7296740924933249,
      "grad_norm": 3.8466567993164062,
      "learning_rate": 3.783893497476034e-05,
      "loss": 2.9423,
      "step": 71600
    },
    {
      "epoch": 0.7317122882824124,
      "grad_norm": 2.038308620452881,
      "learning_rate": 3.780496504494222e-05,
      "loss": 2.9555,
      "step": 71800
    },
    {
      "epoch": 0.7337504840714999,
      "grad_norm": 3.1577751636505127,
      "learning_rate": 3.777099511512409e-05,
      "loss": 2.968,
      "step": 72000
    },
    {
      "epoch": 0.7357886798605874,
      "grad_norm": 2.598849058151245,
      "learning_rate": 3.773702518530597e-05,
      "loss": 2.9362,
      "step": 72200
    },
    {
      "epoch": 0.7378268756496749,
      "grad_norm": 2.445427417755127,
      "learning_rate": 3.770305525548784e-05,
      "loss": 2.9089,
      "step": 72400
    },
    {
      "epoch": 0.7398650714387625,
      "grad_norm": 2.980144500732422,
      "learning_rate": 3.7669085325669715e-05,
      "loss": 2.845,
      "step": 72600
    },
    {
      "epoch": 0.7419032672278499,
      "grad_norm": 3.532294273376465,
      "learning_rate": 3.7635115395851594e-05,
      "loss": 2.876,
      "step": 72800
    },
    {
      "epoch": 0.7439414630169374,
      "grad_norm": 3.808506727218628,
      "learning_rate": 3.760114546603347e-05,
      "loss": 2.909,
      "step": 73000
    },
    {
      "epoch": 0.7459796588060249,
      "grad_norm": 3.3241214752197266,
      "learning_rate": 3.7567175536215346e-05,
      "loss": 2.9352,
      "step": 73200
    },
    {
      "epoch": 0.7480178545951124,
      "grad_norm": 2.807528495788574,
      "learning_rate": 3.753320560639722e-05,
      "loss": 2.9445,
      "step": 73400
    },
    {
      "epoch": 0.7500560503841999,
      "grad_norm": 3.8522510528564453,
      "learning_rate": 3.749923567657909e-05,
      "loss": 2.9051,
      "step": 73600
    },
    {
      "epoch": 0.7520942461732874,
      "grad_norm": 2.772217273712158,
      "learning_rate": 3.746526574676097e-05,
      "loss": 2.9486,
      "step": 73800
    },
    {
      "epoch": 0.7541324419623749,
      "grad_norm": 3.48118257522583,
      "learning_rate": 3.743129581694285e-05,
      "loss": 2.9106,
      "step": 74000
    },
    {
      "epoch": 0.7561706377514624,
      "grad_norm": 2.3935048580169678,
      "learning_rate": 3.739732588712472e-05,
      "loss": 2.9205,
      "step": 74200
    },
    {
      "epoch": 0.7582088335405499,
      "grad_norm": 2.6973228454589844,
      "learning_rate": 3.7363355957306594e-05,
      "loss": 2.9494,
      "step": 74400
    },
    {
      "epoch": 0.7602470293296374,
      "grad_norm": 3.2489712238311768,
      "learning_rate": 3.7329386027488466e-05,
      "loss": 2.9069,
      "step": 74600
    },
    {
      "epoch": 0.7622852251187249,
      "grad_norm": 3.411689043045044,
      "learning_rate": 3.7295416097670345e-05,
      "loss": 2.9112,
      "step": 74800
    },
    {
      "epoch": 0.7643234209078124,
      "grad_norm": 3.6398394107818604,
      "learning_rate": 3.7261446167852224e-05,
      "loss": 2.988,
      "step": 75000
    },
    {
      "epoch": 0.7663616166969,
      "grad_norm": 3.1691339015960693,
      "learning_rate": 3.722747623803409e-05,
      "loss": 2.9175,
      "step": 75200
    },
    {
      "epoch": 0.7683998124859874,
      "grad_norm": 2.9085440635681152,
      "learning_rate": 3.719350630821597e-05,
      "loss": 2.9536,
      "step": 75400
    },
    {
      "epoch": 0.7704380082750749,
      "grad_norm": 2.619685649871826,
      "learning_rate": 3.715953637839785e-05,
      "loss": 3.0409,
      "step": 75600
    },
    {
      "epoch": 0.7724762040641624,
      "grad_norm": 4.425082683563232,
      "learning_rate": 3.712556644857972e-05,
      "loss": 2.8743,
      "step": 75800
    },
    {
      "epoch": 0.7745143998532499,
      "grad_norm": 3.4096946716308594,
      "learning_rate": 3.709159651876159e-05,
      "loss": 2.9074,
      "step": 76000
    },
    {
      "epoch": 0.7765525956423374,
      "grad_norm": 4.3262619972229,
      "learning_rate": 3.7057626588943465e-05,
      "loss": 2.9137,
      "step": 76200
    },
    {
      "epoch": 0.778590791431425,
      "grad_norm": 3.2193872928619385,
      "learning_rate": 3.7023656659125344e-05,
      "loss": 2.8855,
      "step": 76400
    },
    {
      "epoch": 0.7806289872205124,
      "grad_norm": 3.8949625492095947,
      "learning_rate": 3.6989686729307224e-05,
      "loss": 2.9392,
      "step": 76600
    },
    {
      "epoch": 0.7826671830095999,
      "grad_norm": 2.7409684658050537,
      "learning_rate": 3.6955716799489096e-05,
      "loss": 2.8977,
      "step": 76800
    },
    {
      "epoch": 0.7847053787986874,
      "grad_norm": 3.2202260494232178,
      "learning_rate": 3.692174686967097e-05,
      "loss": 2.929,
      "step": 77000
    },
    {
      "epoch": 0.7867435745877749,
      "grad_norm": 5.444798469543457,
      "learning_rate": 3.688777693985284e-05,
      "loss": 2.9309,
      "step": 77200
    },
    {
      "epoch": 0.7887817703768624,
      "grad_norm": 3.6580088138580322,
      "learning_rate": 3.685380701003472e-05,
      "loss": 2.9721,
      "step": 77400
    },
    {
      "epoch": 0.7908199661659499,
      "grad_norm": 2.662574052810669,
      "learning_rate": 3.68198370802166e-05,
      "loss": 2.9484,
      "step": 77600
    },
    {
      "epoch": 0.7928581619550374,
      "grad_norm": 6.726067066192627,
      "learning_rate": 3.6785867150398465e-05,
      "loss": 2.8333,
      "step": 77800
    },
    {
      "epoch": 0.7948963577441249,
      "grad_norm": 3.779736042022705,
      "learning_rate": 3.6751897220580344e-05,
      "loss": 2.8995,
      "step": 78000
    },
    {
      "epoch": 0.7969345535332124,
      "grad_norm": 3.1331939697265625,
      "learning_rate": 3.671792729076222e-05,
      "loss": 2.9092,
      "step": 78200
    },
    {
      "epoch": 0.7989727493222999,
      "grad_norm": 3.584373712539673,
      "learning_rate": 3.6683957360944095e-05,
      "loss": 2.9311,
      "step": 78400
    },
    {
      "epoch": 0.8010109451113874,
      "grad_norm": 3.873058557510376,
      "learning_rate": 3.664998743112597e-05,
      "loss": 2.9159,
      "step": 78600
    },
    {
      "epoch": 0.8030491409004749,
      "grad_norm": 2.846797227859497,
      "learning_rate": 3.661601750130784e-05,
      "loss": 2.9236,
      "step": 78800
    },
    {
      "epoch": 0.8050873366895624,
      "grad_norm": 12.030570030212402,
      "learning_rate": 3.658204757148972e-05,
      "loss": 2.967,
      "step": 79000
    },
    {
      "epoch": 0.8071255324786499,
      "grad_norm": 5.407756805419922,
      "learning_rate": 3.65480776416716e-05,
      "loss": 2.9283,
      "step": 79200
    },
    {
      "epoch": 0.8091637282677374,
      "grad_norm": 6.925889015197754,
      "learning_rate": 3.651410771185347e-05,
      "loss": 2.9383,
      "step": 79400
    },
    {
      "epoch": 0.8112019240568249,
      "grad_norm": 3.9963104724884033,
      "learning_rate": 3.648013778203534e-05,
      "loss": 2.939,
      "step": 79600
    },
    {
      "epoch": 0.8132401198459124,
      "grad_norm": 3.1926891803741455,
      "learning_rate": 3.6446167852217216e-05,
      "loss": 2.9529,
      "step": 79800
    },
    {
      "epoch": 0.8152783156349999,
      "grad_norm": 2.6422383785247803,
      "learning_rate": 3.6412197922399095e-05,
      "loss": 2.8315,
      "step": 80000
    },
    {
      "epoch": 0.8173165114240873,
      "grad_norm": 3.2122762203216553,
      "learning_rate": 3.6378227992580974e-05,
      "loss": 2.9143,
      "step": 80200
    },
    {
      "epoch": 0.8193547072131749,
      "grad_norm": 3.633171319961548,
      "learning_rate": 3.634425806276284e-05,
      "loss": 2.8989,
      "step": 80400
    },
    {
      "epoch": 0.8213929030022624,
      "grad_norm": 3.4571077823638916,
      "learning_rate": 3.631028813294472e-05,
      "loss": 2.9835,
      "step": 80600
    },
    {
      "epoch": 0.8234310987913499,
      "grad_norm": 3.448089361190796,
      "learning_rate": 3.627631820312659e-05,
      "loss": 2.9282,
      "step": 80800
    },
    {
      "epoch": 0.8254692945804374,
      "grad_norm": 3.1630804538726807,
      "learning_rate": 3.624234827330847e-05,
      "loss": 2.9354,
      "step": 81000
    },
    {
      "epoch": 0.8275074903695249,
      "grad_norm": 4.568580150604248,
      "learning_rate": 3.620837834349034e-05,
      "loss": 3.0076,
      "step": 81200
    },
    {
      "epoch": 0.8295456861586123,
      "grad_norm": 2.1578855514526367,
      "learning_rate": 3.6174408413672215e-05,
      "loss": 2.9038,
      "step": 81400
    },
    {
      "epoch": 0.8315838819476999,
      "grad_norm": 2.7456235885620117,
      "learning_rate": 3.6140438483854094e-05,
      "loss": 2.9605,
      "step": 81600
    },
    {
      "epoch": 0.8336220777367874,
      "grad_norm": 4.917807102203369,
      "learning_rate": 3.610646855403597e-05,
      "loss": 2.898,
      "step": 81800
    },
    {
      "epoch": 0.8356602735258749,
      "grad_norm": 5.28176736831665,
      "learning_rate": 3.6072498624217846e-05,
      "loss": 2.903,
      "step": 82000
    },
    {
      "epoch": 0.8376984693149624,
      "grad_norm": 2.7256600856781006,
      "learning_rate": 3.603852869439972e-05,
      "loss": 2.9432,
      "step": 82200
    },
    {
      "epoch": 0.8397366651040499,
      "grad_norm": 2.7377398014068604,
      "learning_rate": 3.600455876458159e-05,
      "loss": 2.9216,
      "step": 82400
    },
    {
      "epoch": 0.8417748608931374,
      "grad_norm": 3.008876323699951,
      "learning_rate": 3.597058883476347e-05,
      "loss": 2.954,
      "step": 82600
    },
    {
      "epoch": 0.843813056682225,
      "grad_norm": 3.339839458465576,
      "learning_rate": 3.593661890494535e-05,
      "loss": 2.907,
      "step": 82800
    },
    {
      "epoch": 0.8458512524713124,
      "grad_norm": 2.971201181411743,
      "learning_rate": 3.5902648975127214e-05,
      "loss": 2.9381,
      "step": 83000
    },
    {
      "epoch": 0.8478894482603999,
      "grad_norm": 4.326699256896973,
      "learning_rate": 3.5868679045309094e-05,
      "loss": 2.9019,
      "step": 83200
    },
    {
      "epoch": 0.8499276440494874,
      "grad_norm": 3.7712836265563965,
      "learning_rate": 3.5834709115490966e-05,
      "loss": 2.9599,
      "step": 83400
    },
    {
      "epoch": 0.8519658398385749,
      "grad_norm": 3.7140536308288574,
      "learning_rate": 3.5800739185672845e-05,
      "loss": 2.9149,
      "step": 83600
    },
    {
      "epoch": 0.8540040356276624,
      "grad_norm": 2.2572736740112305,
      "learning_rate": 3.576676925585472e-05,
      "loss": 2.8833,
      "step": 83800
    },
    {
      "epoch": 0.8560422314167498,
      "grad_norm": 2.1271886825561523,
      "learning_rate": 3.573279932603659e-05,
      "loss": 2.9233,
      "step": 84000
    },
    {
      "epoch": 0.8580804272058374,
      "grad_norm": 3.015906810760498,
      "learning_rate": 3.569882939621847e-05,
      "loss": 2.9257,
      "step": 84200
    },
    {
      "epoch": 0.8601186229949249,
      "grad_norm": 3.292593002319336,
      "learning_rate": 3.566485946640035e-05,
      "loss": 2.9282,
      "step": 84400
    },
    {
      "epoch": 0.8621568187840124,
      "grad_norm": 4.5907301902771,
      "learning_rate": 3.563088953658222e-05,
      "loss": 2.9815,
      "step": 84600
    },
    {
      "epoch": 0.8641950145730999,
      "grad_norm": 3.2567298412323,
      "learning_rate": 3.559691960676409e-05,
      "loss": 2.9306,
      "step": 84800
    },
    {
      "epoch": 0.8662332103621874,
      "grad_norm": 4.00239896774292,
      "learning_rate": 3.5562949676945965e-05,
      "loss": 2.9019,
      "step": 85000
    },
    {
      "epoch": 0.8682714061512748,
      "grad_norm": 2.497948169708252,
      "learning_rate": 3.5528979747127845e-05,
      "loss": 2.9879,
      "step": 85200
    },
    {
      "epoch": 0.8703096019403624,
      "grad_norm": 2.8052937984466553,
      "learning_rate": 3.5495009817309724e-05,
      "loss": 2.9613,
      "step": 85400
    },
    {
      "epoch": 0.8723477977294499,
      "grad_norm": 5.282684326171875,
      "learning_rate": 3.5461039887491596e-05,
      "loss": 2.9643,
      "step": 85600
    },
    {
      "epoch": 0.8743859935185374,
      "grad_norm": 3.0173187255859375,
      "learning_rate": 3.542706995767347e-05,
      "loss": 2.8973,
      "step": 85800
    },
    {
      "epoch": 0.8764241893076249,
      "grad_norm": 2.3466989994049072,
      "learning_rate": 3.539310002785534e-05,
      "loss": 2.8861,
      "step": 86000
    },
    {
      "epoch": 0.8784623850967124,
      "grad_norm": 6.242801666259766,
      "learning_rate": 3.535913009803722e-05,
      "loss": 2.9025,
      "step": 86200
    },
    {
      "epoch": 0.8805005808857999,
      "grad_norm": 3.0986533164978027,
      "learning_rate": 3.53251601682191e-05,
      "loss": 2.8964,
      "step": 86400
    },
    {
      "epoch": 0.8825387766748873,
      "grad_norm": 4.847229957580566,
      "learning_rate": 3.5291190238400965e-05,
      "loss": 2.9092,
      "step": 86600
    },
    {
      "epoch": 0.8845769724639749,
      "grad_norm": 3.008718967437744,
      "learning_rate": 3.5257220308582844e-05,
      "loss": 2.9352,
      "step": 86800
    },
    {
      "epoch": 0.8866151682530624,
      "grad_norm": 3.910748243331909,
      "learning_rate": 3.522325037876472e-05,
      "loss": 2.9748,
      "step": 87000
    },
    {
      "epoch": 0.8886533640421499,
      "grad_norm": 3.3470749855041504,
      "learning_rate": 3.5189280448946595e-05,
      "loss": 2.8969,
      "step": 87200
    },
    {
      "epoch": 0.8906915598312374,
      "grad_norm": 5.969723701477051,
      "learning_rate": 3.515531051912847e-05,
      "loss": 2.9524,
      "step": 87400
    },
    {
      "epoch": 0.8927297556203249,
      "grad_norm": 3.254903793334961,
      "learning_rate": 3.512134058931034e-05,
      "loss": 2.918,
      "step": 87600
    },
    {
      "epoch": 0.8947679514094123,
      "grad_norm": 2.8338851928710938,
      "learning_rate": 3.508737065949222e-05,
      "loss": 2.9453,
      "step": 87800
    },
    {
      "epoch": 0.8968061471984999,
      "grad_norm": 2.8310487270355225,
      "learning_rate": 3.50534007296741e-05,
      "loss": 2.8691,
      "step": 88000
    },
    {
      "epoch": 0.8988443429875874,
      "grad_norm": 4.723783016204834,
      "learning_rate": 3.501943079985597e-05,
      "loss": 2.9868,
      "step": 88200
    },
    {
      "epoch": 0.9008825387766749,
      "grad_norm": 3.1164002418518066,
      "learning_rate": 3.498546087003784e-05,
      "loss": 2.8506,
      "step": 88400
    },
    {
      "epoch": 0.9029207345657624,
      "grad_norm": 4.246589660644531,
      "learning_rate": 3.4951490940219716e-05,
      "loss": 2.8339,
      "step": 88600
    },
    {
      "epoch": 0.9049589303548499,
      "grad_norm": 4.60411262512207,
      "learning_rate": 3.4917521010401595e-05,
      "loss": 2.8909,
      "step": 88800
    },
    {
      "epoch": 0.9069971261439373,
      "grad_norm": 3.445003032684326,
      "learning_rate": 3.4883551080583474e-05,
      "loss": 2.9229,
      "step": 89000
    },
    {
      "epoch": 0.9090353219330248,
      "grad_norm": 6.233829975128174,
      "learning_rate": 3.484958115076534e-05,
      "loss": 2.9318,
      "step": 89200
    },
    {
      "epoch": 0.9110735177221124,
      "grad_norm": 3.561079978942871,
      "learning_rate": 3.481561122094722e-05,
      "loss": 2.8446,
      "step": 89400
    },
    {
      "epoch": 0.9131117135111999,
      "grad_norm": 2.6933164596557617,
      "learning_rate": 3.47816412911291e-05,
      "loss": 2.925,
      "step": 89600
    },
    {
      "epoch": 0.9151499093002874,
      "grad_norm": 4.9277191162109375,
      "learning_rate": 3.474767136131097e-05,
      "loss": 2.9042,
      "step": 89800
    },
    {
      "epoch": 0.9171881050893749,
      "grad_norm": 3.960556983947754,
      "learning_rate": 3.471370143149284e-05,
      "loss": 2.9654,
      "step": 90000
    },
    {
      "epoch": 0.9192263008784624,
      "grad_norm": 5.064991474151611,
      "learning_rate": 3.4679731501674715e-05,
      "loss": 2.9165,
      "step": 90200
    },
    {
      "epoch": 0.9212644966675498,
      "grad_norm": 2.8004608154296875,
      "learning_rate": 3.4645761571856594e-05,
      "loss": 2.9407,
      "step": 90400
    },
    {
      "epoch": 0.9233026924566374,
      "grad_norm": 6.047524929046631,
      "learning_rate": 3.4611791642038473e-05,
      "loss": 2.9151,
      "step": 90600
    },
    {
      "epoch": 0.9253408882457249,
      "grad_norm": 3.1152307987213135,
      "learning_rate": 3.4577821712220346e-05,
      "loss": 2.9019,
      "step": 90800
    },
    {
      "epoch": 0.9273790840348124,
      "grad_norm": 5.773683547973633,
      "learning_rate": 3.454385178240222e-05,
      "loss": 2.8721,
      "step": 91000
    },
    {
      "epoch": 0.9294172798238999,
      "grad_norm": 4.13181209564209,
      "learning_rate": 3.450988185258409e-05,
      "loss": 2.8975,
      "step": 91200
    },
    {
      "epoch": 0.9314554756129874,
      "grad_norm": 3.3142919540405273,
      "learning_rate": 3.447591192276597e-05,
      "loss": 2.93,
      "step": 91400
    },
    {
      "epoch": 0.9334936714020748,
      "grad_norm": 3.8968019485473633,
      "learning_rate": 3.444194199294785e-05,
      "loss": 2.8908,
      "step": 91600
    },
    {
      "epoch": 0.9355318671911624,
      "grad_norm": 6.188445568084717,
      "learning_rate": 3.4407972063129715e-05,
      "loss": 2.9358,
      "step": 91800
    },
    {
      "epoch": 0.9375700629802499,
      "grad_norm": 2.5578055381774902,
      "learning_rate": 3.4374002133311594e-05,
      "loss": 2.9731,
      "step": 92000
    },
    {
      "epoch": 0.9396082587693374,
      "grad_norm": 3.0300347805023193,
      "learning_rate": 3.434003220349347e-05,
      "loss": 2.8768,
      "step": 92200
    },
    {
      "epoch": 0.9416464545584249,
      "grad_norm": 2.262619972229004,
      "learning_rate": 3.4306062273675345e-05,
      "loss": 2.9382,
      "step": 92400
    },
    {
      "epoch": 0.9436846503475124,
      "grad_norm": 4.548341751098633,
      "learning_rate": 3.427209234385722e-05,
      "loss": 2.9503,
      "step": 92600
    },
    {
      "epoch": 0.9457228461365998,
      "grad_norm": 3.7136528491973877,
      "learning_rate": 3.423812241403909e-05,
      "loss": 2.8668,
      "step": 92800
    },
    {
      "epoch": 0.9477610419256873,
      "grad_norm": 2.5954365730285645,
      "learning_rate": 3.420415248422097e-05,
      "loss": 2.9378,
      "step": 93000
    },
    {
      "epoch": 0.9497992377147749,
      "grad_norm": 3.7246017456054688,
      "learning_rate": 3.417018255440285e-05,
      "loss": 2.942,
      "step": 93200
    },
    {
      "epoch": 0.9518374335038624,
      "grad_norm": 3.1695196628570557,
      "learning_rate": 3.413621262458472e-05,
      "loss": 2.8278,
      "step": 93400
    },
    {
      "epoch": 0.9538756292929499,
      "grad_norm": 4.049266338348389,
      "learning_rate": 3.410224269476659e-05,
      "loss": 2.8951,
      "step": 93600
    },
    {
      "epoch": 0.9559138250820374,
      "grad_norm": 2.3955259323120117,
      "learning_rate": 3.4068272764948465e-05,
      "loss": 2.94,
      "step": 93800
    },
    {
      "epoch": 0.9579520208711249,
      "grad_norm": 2.917396306991577,
      "learning_rate": 3.4034302835130345e-05,
      "loss": 2.8667,
      "step": 94000
    },
    {
      "epoch": 0.9599902166602123,
      "grad_norm": 2.4835011959075928,
      "learning_rate": 3.4000332905312224e-05,
      "loss": 2.959,
      "step": 94200
    },
    {
      "epoch": 0.9620284124492999,
      "grad_norm": 3.3225765228271484,
      "learning_rate": 3.396636297549409e-05,
      "loss": 2.9491,
      "step": 94400
    },
    {
      "epoch": 0.9640666082383874,
      "grad_norm": 4.484922885894775,
      "learning_rate": 3.393239304567597e-05,
      "loss": 2.8727,
      "step": 94600
    },
    {
      "epoch": 0.9661048040274749,
      "grad_norm": 4.0211992263793945,
      "learning_rate": 3.389842311585785e-05,
      "loss": 2.9233,
      "step": 94800
    },
    {
      "epoch": 0.9681429998165624,
      "grad_norm": 3.5366930961608887,
      "learning_rate": 3.386445318603972e-05,
      "loss": 2.8628,
      "step": 95000
    },
    {
      "epoch": 0.9701811956056499,
      "grad_norm": 4.073864936828613,
      "learning_rate": 3.383048325622159e-05,
      "loss": 2.9433,
      "step": 95200
    },
    {
      "epoch": 0.9722193913947373,
      "grad_norm": 2.605644702911377,
      "learning_rate": 3.3796513326403465e-05,
      "loss": 2.9382,
      "step": 95400
    },
    {
      "epoch": 0.9742575871838248,
      "grad_norm": 3.902508020401001,
      "learning_rate": 3.3762543396585344e-05,
      "loss": 2.9114,
      "step": 95600
    },
    {
      "epoch": 0.9762957829729124,
      "grad_norm": 2.9367589950561523,
      "learning_rate": 3.372857346676722e-05,
      "loss": 2.8723,
      "step": 95800
    },
    {
      "epoch": 0.9783339787619999,
      "grad_norm": 4.392055034637451,
      "learning_rate": 3.3694603536949096e-05,
      "loss": 3.0054,
      "step": 96000
    },
    {
      "epoch": 0.9803721745510874,
      "grad_norm": 2.909435272216797,
      "learning_rate": 3.366063360713097e-05,
      "loss": 2.927,
      "step": 96200
    },
    {
      "epoch": 0.9824103703401749,
      "grad_norm": 3.364367723464966,
      "learning_rate": 3.362666367731284e-05,
      "loss": 2.9052,
      "step": 96400
    },
    {
      "epoch": 0.9844485661292623,
      "grad_norm": 3.9992623329162598,
      "learning_rate": 3.359269374749472e-05,
      "loss": 2.8246,
      "step": 96600
    },
    {
      "epoch": 0.9864867619183498,
      "grad_norm": 4.915529727935791,
      "learning_rate": 3.35587238176766e-05,
      "loss": 2.8677,
      "step": 96800
    },
    {
      "epoch": 0.9885249577074374,
      "grad_norm": 3.181907892227173,
      "learning_rate": 3.352475388785847e-05,
      "loss": 2.8418,
      "step": 97000
    },
    {
      "epoch": 0.9905631534965249,
      "grad_norm": 4.297199249267578,
      "learning_rate": 3.3490783958040343e-05,
      "loss": 2.9195,
      "step": 97200
    },
    {
      "epoch": 0.9926013492856124,
      "grad_norm": 2.3776538372039795,
      "learning_rate": 3.345681402822222e-05,
      "loss": 2.8796,
      "step": 97400
    },
    {
      "epoch": 0.9946395450746999,
      "grad_norm": 2.779326915740967,
      "learning_rate": 3.3422844098404095e-05,
      "loss": 2.8746,
      "step": 97600
    },
    {
      "epoch": 0.9966777408637874,
      "grad_norm": 2.882323741912842,
      "learning_rate": 3.3388874168585974e-05,
      "loss": 2.8776,
      "step": 97800
    },
    {
      "epoch": 0.9987159366528748,
      "grad_norm": 3.9213640689849854,
      "learning_rate": 3.335490423876784e-05,
      "loss": 2.9597,
      "step": 98000
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.8509573936462402,
      "eval_runtime": 194.4357,
      "eval_samples_per_second": 437.116,
      "eval_steps_per_second": 54.64,
      "step": 98126
    },
    {
      "epoch": 1.0007541324419624,
      "grad_norm": 3.7255115509033203,
      "learning_rate": 3.332093430894972e-05,
      "loss": 2.8754,
      "step": 98200
    },
    {
      "epoch": 1.0027923282310498,
      "grad_norm": 3.5759212970733643,
      "learning_rate": 3.32869643791316e-05,
      "loss": 2.8309,
      "step": 98400
    },
    {
      "epoch": 1.0048305240201374,
      "grad_norm": 3.5759317874908447,
      "learning_rate": 3.325299444931347e-05,
      "loss": 2.7591,
      "step": 98600
    },
    {
      "epoch": 1.0068687198092248,
      "grad_norm": 2.7601330280303955,
      "learning_rate": 3.321902451949534e-05,
      "loss": 2.7887,
      "step": 98800
    },
    {
      "epoch": 1.0089069155983124,
      "grad_norm": 5.038531303405762,
      "learning_rate": 3.3185054589677215e-05,
      "loss": 2.7616,
      "step": 99000
    },
    {
      "epoch": 1.0109451113874,
      "grad_norm": 4.231172561645508,
      "learning_rate": 3.3151084659859094e-05,
      "loss": 2.7743,
      "step": 99200
    },
    {
      "epoch": 1.0129833071764873,
      "grad_norm": 3.2774240970611572,
      "learning_rate": 3.3117114730040973e-05,
      "loss": 2.8149,
      "step": 99400
    },
    {
      "epoch": 1.015021502965575,
      "grad_norm": 2.982208013534546,
      "learning_rate": 3.3083144800222846e-05,
      "loss": 2.8244,
      "step": 99600
    },
    {
      "epoch": 1.0170596987546623,
      "grad_norm": 8.267600059509277,
      "learning_rate": 3.304917487040472e-05,
      "loss": 2.7976,
      "step": 99800
    },
    {
      "epoch": 1.0190978945437499,
      "grad_norm": 2.8864290714263916,
      "learning_rate": 3.30152049405866e-05,
      "loss": 2.8469,
      "step": 100000
    },
    {
      "epoch": 1.0211360903328375,
      "grad_norm": 2.703646659851074,
      "learning_rate": 3.298123501076847e-05,
      "loss": 2.8015,
      "step": 100200
    },
    {
      "epoch": 1.0231742861219248,
      "grad_norm": 10.087688446044922,
      "learning_rate": 3.294726508095035e-05,
      "loss": 2.8087,
      "step": 100400
    },
    {
      "epoch": 1.0252124819110124,
      "grad_norm": 3.7494633197784424,
      "learning_rate": 3.2913295151132215e-05,
      "loss": 2.7203,
      "step": 100600
    },
    {
      "epoch": 1.0272506777000998,
      "grad_norm": 3.0558652877807617,
      "learning_rate": 3.2879325221314094e-05,
      "loss": 2.8071,
      "step": 100800
    },
    {
      "epoch": 1.0292888734891874,
      "grad_norm": 6.175501823425293,
      "learning_rate": 3.284535529149597e-05,
      "loss": 2.8109,
      "step": 101000
    },
    {
      "epoch": 1.0313270692782748,
      "grad_norm": 2.9643306732177734,
      "learning_rate": 3.2811385361677845e-05,
      "loss": 2.7918,
      "step": 101200
    },
    {
      "epoch": 1.0333652650673624,
      "grad_norm": 3.932492971420288,
      "learning_rate": 3.277741543185972e-05,
      "loss": 2.8399,
      "step": 101400
    },
    {
      "epoch": 1.03540346085645,
      "grad_norm": 2.716470956802368,
      "learning_rate": 3.274344550204159e-05,
      "loss": 2.7614,
      "step": 101600
    },
    {
      "epoch": 1.0374416566455373,
      "grad_norm": 3.7036430835723877,
      "learning_rate": 3.270947557222347e-05,
      "loss": 2.7997,
      "step": 101800
    },
    {
      "epoch": 1.039479852434625,
      "grad_norm": 2.575326919555664,
      "learning_rate": 3.267550564240535e-05,
      "loss": 2.7899,
      "step": 102000
    },
    {
      "epoch": 1.0415180482237123,
      "grad_norm": 3.238408088684082,
      "learning_rate": 3.264153571258722e-05,
      "loss": 2.7689,
      "step": 102200
    },
    {
      "epoch": 1.0435562440128,
      "grad_norm": 3.9400627613067627,
      "learning_rate": 3.260756578276909e-05,
      "loss": 2.8576,
      "step": 102400
    },
    {
      "epoch": 1.0455944398018873,
      "grad_norm": 4.802270412445068,
      "learning_rate": 3.2573595852950966e-05,
      "loss": 2.7109,
      "step": 102600
    },
    {
      "epoch": 1.0476326355909749,
      "grad_norm": 3.6462271213531494,
      "learning_rate": 3.2539625923132845e-05,
      "loss": 2.7843,
      "step": 102800
    },
    {
      "epoch": 1.0496708313800625,
      "grad_norm": 2.1339070796966553,
      "learning_rate": 3.2505655993314724e-05,
      "loss": 2.7897,
      "step": 103000
    },
    {
      "epoch": 1.0517090271691498,
      "grad_norm": 3.9094650745391846,
      "learning_rate": 3.247168606349659e-05,
      "loss": 2.7152,
      "step": 103200
    },
    {
      "epoch": 1.0537472229582374,
      "grad_norm": 3.730020761489868,
      "learning_rate": 3.243771613367847e-05,
      "loss": 2.765,
      "step": 103400
    },
    {
      "epoch": 1.0557854187473248,
      "grad_norm": 3.5425095558166504,
      "learning_rate": 3.240374620386035e-05,
      "loss": 2.7531,
      "step": 103600
    },
    {
      "epoch": 1.0578236145364124,
      "grad_norm": 4.132384300231934,
      "learning_rate": 3.236977627404222e-05,
      "loss": 2.7515,
      "step": 103800
    },
    {
      "epoch": 1.0598618103254998,
      "grad_norm": 5.5955071449279785,
      "learning_rate": 3.233580634422409e-05,
      "loss": 2.8644,
      "step": 104000
    },
    {
      "epoch": 1.0619000061145873,
      "grad_norm": 3.0291459560394287,
      "learning_rate": 3.2301836414405965e-05,
      "loss": 2.8655,
      "step": 104200
    },
    {
      "epoch": 1.063938201903675,
      "grad_norm": 5.927578449249268,
      "learning_rate": 3.2267866484587844e-05,
      "loss": 2.8421,
      "step": 104400
    },
    {
      "epoch": 1.0659763976927623,
      "grad_norm": 2.8062586784362793,
      "learning_rate": 3.223389655476972e-05,
      "loss": 2.8011,
      "step": 104600
    },
    {
      "epoch": 1.06801459348185,
      "grad_norm": 3.5747830867767334,
      "learning_rate": 3.2199926624951596e-05,
      "loss": 2.793,
      "step": 104800
    },
    {
      "epoch": 1.0700527892709373,
      "grad_norm": 3.7833683490753174,
      "learning_rate": 3.216595669513347e-05,
      "loss": 2.8319,
      "step": 105000
    },
    {
      "epoch": 1.0720909850600249,
      "grad_norm": 3.944023370742798,
      "learning_rate": 3.213198676531534e-05,
      "loss": 2.8748,
      "step": 105200
    },
    {
      "epoch": 1.0741291808491125,
      "grad_norm": 6.016095161437988,
      "learning_rate": 3.209801683549722e-05,
      "loss": 2.8445,
      "step": 105400
    },
    {
      "epoch": 1.0761673766381998,
      "grad_norm": 3.141970157623291,
      "learning_rate": 3.20640469056791e-05,
      "loss": 2.7619,
      "step": 105600
    },
    {
      "epoch": 1.0782055724272874,
      "grad_norm": 3.804123878479004,
      "learning_rate": 3.2030076975860964e-05,
      "loss": 2.7863,
      "step": 105800
    },
    {
      "epoch": 1.0802437682163748,
      "grad_norm": 4.094171047210693,
      "learning_rate": 3.1996107046042843e-05,
      "loss": 2.8023,
      "step": 106000
    },
    {
      "epoch": 1.0822819640054624,
      "grad_norm": 3.4172229766845703,
      "learning_rate": 3.196213711622472e-05,
      "loss": 2.7751,
      "step": 106200
    },
    {
      "epoch": 1.0843201597945498,
      "grad_norm": 2.529125690460205,
      "learning_rate": 3.1928167186406595e-05,
      "loss": 2.8127,
      "step": 106400
    },
    {
      "epoch": 1.0863583555836374,
      "grad_norm": 3.5906267166137695,
      "learning_rate": 3.189419725658847e-05,
      "loss": 2.812,
      "step": 106600
    },
    {
      "epoch": 1.088396551372725,
      "grad_norm": 3.3404929637908936,
      "learning_rate": 3.186022732677034e-05,
      "loss": 2.8246,
      "step": 106800
    },
    {
      "epoch": 1.0904347471618123,
      "grad_norm": 3.5338783264160156,
      "learning_rate": 3.182625739695222e-05,
      "loss": 2.7957,
      "step": 107000
    },
    {
      "epoch": 1.0924729429509,
      "grad_norm": 7.607287883758545,
      "learning_rate": 3.17922874671341e-05,
      "loss": 2.7559,
      "step": 107200
    },
    {
      "epoch": 1.0945111387399873,
      "grad_norm": 2.81722092628479,
      "learning_rate": 3.175831753731597e-05,
      "loss": 2.867,
      "step": 107400
    },
    {
      "epoch": 1.0965493345290749,
      "grad_norm": 4.191081523895264,
      "learning_rate": 3.172434760749784e-05,
      "loss": 2.8165,
      "step": 107600
    },
    {
      "epoch": 1.0985875303181625,
      "grad_norm": 4.092825412750244,
      "learning_rate": 3.1690377677679715e-05,
      "loss": 2.7702,
      "step": 107800
    },
    {
      "epoch": 1.1006257261072498,
      "grad_norm": 7.145462989807129,
      "learning_rate": 3.1656407747861594e-05,
      "loss": 2.7628,
      "step": 108000
    },
    {
      "epoch": 1.1026639218963374,
      "grad_norm": 4.492555618286133,
      "learning_rate": 3.1622437818043474e-05,
      "loss": 2.8069,
      "step": 108200
    },
    {
      "epoch": 1.1047021176854248,
      "grad_norm": 3.0884311199188232,
      "learning_rate": 3.1588467888225346e-05,
      "loss": 2.8003,
      "step": 108400
    },
    {
      "epoch": 1.1067403134745124,
      "grad_norm": 4.502293109893799,
      "learning_rate": 3.155449795840722e-05,
      "loss": 2.7873,
      "step": 108600
    },
    {
      "epoch": 1.1087785092635998,
      "grad_norm": 3.5756194591522217,
      "learning_rate": 3.15205280285891e-05,
      "loss": 2.8131,
      "step": 108800
    },
    {
      "epoch": 1.1108167050526874,
      "grad_norm": 3.3516147136688232,
      "learning_rate": 3.148655809877097e-05,
      "loss": 2.7895,
      "step": 109000
    },
    {
      "epoch": 1.112854900841775,
      "grad_norm": 2.649704694747925,
      "learning_rate": 3.145258816895285e-05,
      "loss": 2.7681,
      "step": 109200
    },
    {
      "epoch": 1.1148930966308623,
      "grad_norm": 4.492183685302734,
      "learning_rate": 3.1418618239134715e-05,
      "loss": 2.8186,
      "step": 109400
    },
    {
      "epoch": 1.11693129241995,
      "grad_norm": 3.09011173248291,
      "learning_rate": 3.1384648309316594e-05,
      "loss": 2.8127,
      "step": 109600
    },
    {
      "epoch": 1.1189694882090373,
      "grad_norm": 3.410057544708252,
      "learning_rate": 3.135067837949847e-05,
      "loss": 2.8251,
      "step": 109800
    },
    {
      "epoch": 1.121007683998125,
      "grad_norm": 3.245466947555542,
      "learning_rate": 3.1316708449680345e-05,
      "loss": 2.7822,
      "step": 110000
    },
    {
      "epoch": 1.1230458797872123,
      "grad_norm": 3.0826311111450195,
      "learning_rate": 3.128273851986222e-05,
      "loss": 2.7399,
      "step": 110200
    },
    {
      "epoch": 1.1250840755762999,
      "grad_norm": 4.795541286468506,
      "learning_rate": 3.124876859004409e-05,
      "loss": 2.851,
      "step": 110400
    },
    {
      "epoch": 1.1271222713653875,
      "grad_norm": 3.5382444858551025,
      "learning_rate": 3.121479866022597e-05,
      "loss": 2.7975,
      "step": 110600
    },
    {
      "epoch": 1.1291604671544748,
      "grad_norm": 5.118971824645996,
      "learning_rate": 3.118082873040785e-05,
      "loss": 2.8208,
      "step": 110800
    },
    {
      "epoch": 1.1311986629435624,
      "grad_norm": 3.786842107772827,
      "learning_rate": 3.114685880058972e-05,
      "loss": 2.8168,
      "step": 111000
    },
    {
      "epoch": 1.1332368587326498,
      "grad_norm": 4.0656046867370605,
      "learning_rate": 3.111288887077159e-05,
      "loss": 2.772,
      "step": 111200
    },
    {
      "epoch": 1.1352750545217374,
      "grad_norm": 5.3492560386657715,
      "learning_rate": 3.107891894095347e-05,
      "loss": 2.8135,
      "step": 111400
    },
    {
      "epoch": 1.1373132503108248,
      "grad_norm": 3.6290030479431152,
      "learning_rate": 3.1044949011135345e-05,
      "loss": 2.8241,
      "step": 111600
    },
    {
      "epoch": 1.1393514460999123,
      "grad_norm": 2.7827906608581543,
      "learning_rate": 3.1010979081317224e-05,
      "loss": 2.7198,
      "step": 111800
    },
    {
      "epoch": 1.141389641889,
      "grad_norm": 3.783837080001831,
      "learning_rate": 3.097700915149909e-05,
      "loss": 2.7953,
      "step": 112000
    },
    {
      "epoch": 1.1434278376780873,
      "grad_norm": 2.9607274532318115,
      "learning_rate": 3.094303922168097e-05,
      "loss": 2.7685,
      "step": 112200
    },
    {
      "epoch": 1.145466033467175,
      "grad_norm": 3.2316486835479736,
      "learning_rate": 3.090906929186285e-05,
      "loss": 2.8086,
      "step": 112400
    },
    {
      "epoch": 1.1475042292562623,
      "grad_norm": 3.7653748989105225,
      "learning_rate": 3.087509936204472e-05,
      "loss": 2.8089,
      "step": 112600
    },
    {
      "epoch": 1.1495424250453499,
      "grad_norm": 4.342544078826904,
      "learning_rate": 3.084112943222659e-05,
      "loss": 2.8294,
      "step": 112800
    },
    {
      "epoch": 1.1515806208344372,
      "grad_norm": 3.34075927734375,
      "learning_rate": 3.0807159502408465e-05,
      "loss": 2.8311,
      "step": 113000
    },
    {
      "epoch": 1.1536188166235248,
      "grad_norm": 3.6418628692626953,
      "learning_rate": 3.0773189572590344e-05,
      "loss": 2.763,
      "step": 113200
    },
    {
      "epoch": 1.1556570124126124,
      "grad_norm": 4.0436320304870605,
      "learning_rate": 3.073921964277222e-05,
      "loss": 2.758,
      "step": 113400
    },
    {
      "epoch": 1.1576952082016998,
      "grad_norm": 3.2267985343933105,
      "learning_rate": 3.0705249712954096e-05,
      "loss": 2.815,
      "step": 113600
    },
    {
      "epoch": 1.1597334039907874,
      "grad_norm": 3.7379841804504395,
      "learning_rate": 3.067127978313597e-05,
      "loss": 2.8357,
      "step": 113800
    },
    {
      "epoch": 1.1617715997798748,
      "grad_norm": 3.6569457054138184,
      "learning_rate": 3.063730985331785e-05,
      "loss": 2.7257,
      "step": 114000
    },
    {
      "epoch": 1.1638097955689624,
      "grad_norm": 4.329366683959961,
      "learning_rate": 3.060333992349972e-05,
      "loss": 2.8022,
      "step": 114200
    },
    {
      "epoch": 1.16584799135805,
      "grad_norm": 2.5472755432128906,
      "learning_rate": 3.05693699936816e-05,
      "loss": 2.8183,
      "step": 114400
    },
    {
      "epoch": 1.1678861871471373,
      "grad_norm": 3.2501516342163086,
      "learning_rate": 3.0535400063863464e-05,
      "loss": 2.7878,
      "step": 114600
    },
    {
      "epoch": 1.169924382936225,
      "grad_norm": 3.2918550968170166,
      "learning_rate": 3.0501430134045344e-05,
      "loss": 2.7366,
      "step": 114800
    },
    {
      "epoch": 1.1719625787253123,
      "grad_norm": 3.244377374649048,
      "learning_rate": 3.0467460204227223e-05,
      "loss": 2.7628,
      "step": 115000
    },
    {
      "epoch": 1.1740007745143999,
      "grad_norm": 2.6993627548217773,
      "learning_rate": 3.043349027440909e-05,
      "loss": 2.8302,
      "step": 115200
    },
    {
      "epoch": 1.1760389703034875,
      "grad_norm": 3.3656082153320312,
      "learning_rate": 3.039952034459097e-05,
      "loss": 2.8192,
      "step": 115400
    },
    {
      "epoch": 1.1780771660925748,
      "grad_norm": 4.71453332901001,
      "learning_rate": 3.0365550414772843e-05,
      "loss": 2.8787,
      "step": 115600
    },
    {
      "epoch": 1.1801153618816624,
      "grad_norm": 5.496123790740967,
      "learning_rate": 3.033158048495472e-05,
      "loss": 2.8728,
      "step": 115800
    },
    {
      "epoch": 1.1821535576707498,
      "grad_norm": 3.632754325866699,
      "learning_rate": 3.0297610555136595e-05,
      "loss": 2.8081,
      "step": 116000
    },
    {
      "epoch": 1.1841917534598374,
      "grad_norm": 10.402645111083984,
      "learning_rate": 3.0263640625318467e-05,
      "loss": 2.8072,
      "step": 116200
    },
    {
      "epoch": 1.1862299492489248,
      "grad_norm": 3.4993972778320312,
      "learning_rate": 3.0229670695500346e-05,
      "loss": 2.8424,
      "step": 116400
    },
    {
      "epoch": 1.1882681450380124,
      "grad_norm": 2.842207908630371,
      "learning_rate": 3.0195700765682222e-05,
      "loss": 2.7641,
      "step": 116600
    },
    {
      "epoch": 1.1903063408271,
      "grad_norm": 4.245999813079834,
      "learning_rate": 3.0161730835864094e-05,
      "loss": 2.8046,
      "step": 116800
    },
    {
      "epoch": 1.1923445366161873,
      "grad_norm": 2.755722999572754,
      "learning_rate": 3.012776090604597e-05,
      "loss": 2.8087,
      "step": 117000
    },
    {
      "epoch": 1.194382732405275,
      "grad_norm": 3.1676506996154785,
      "learning_rate": 3.0093790976227843e-05,
      "loss": 2.8361,
      "step": 117200
    },
    {
      "epoch": 1.1964209281943623,
      "grad_norm": 3.8182735443115234,
      "learning_rate": 3.005982104640972e-05,
      "loss": 2.7529,
      "step": 117400
    },
    {
      "epoch": 1.19845912398345,
      "grad_norm": 5.425423622131348,
      "learning_rate": 3.0025851116591598e-05,
      "loss": 2.766,
      "step": 117600
    },
    {
      "epoch": 1.2004973197725373,
      "grad_norm": 3.6412246227264404,
      "learning_rate": 2.999188118677347e-05,
      "loss": 2.8183,
      "step": 117800
    },
    {
      "epoch": 1.2025355155616249,
      "grad_norm": 4.993340492248535,
      "learning_rate": 2.9957911256955346e-05,
      "loss": 2.8199,
      "step": 118000
    },
    {
      "epoch": 1.2045737113507125,
      "grad_norm": 3.213315010070801,
      "learning_rate": 2.9923941327137218e-05,
      "loss": 2.7899,
      "step": 118200
    },
    {
      "epoch": 1.2066119071397998,
      "grad_norm": 3.4295482635498047,
      "learning_rate": 2.9889971397319094e-05,
      "loss": 2.785,
      "step": 118400
    },
    {
      "epoch": 1.2086501029288874,
      "grad_norm": 2.92850923538208,
      "learning_rate": 2.9856001467500973e-05,
      "loss": 2.7698,
      "step": 118600
    },
    {
      "epoch": 1.2106882987179748,
      "grad_norm": 4.492220401763916,
      "learning_rate": 2.9822031537682842e-05,
      "loss": 2.8423,
      "step": 118800
    },
    {
      "epoch": 1.2127264945070624,
      "grad_norm": 4.352733612060547,
      "learning_rate": 2.978806160786472e-05,
      "loss": 2.7635,
      "step": 119000
    },
    {
      "epoch": 1.2147646902961498,
      "grad_norm": 4.572826862335205,
      "learning_rate": 2.9754091678046597e-05,
      "loss": 2.8387,
      "step": 119200
    },
    {
      "epoch": 1.2168028860852373,
      "grad_norm": 3.9005696773529053,
      "learning_rate": 2.972012174822847e-05,
      "loss": 2.8303,
      "step": 119400
    },
    {
      "epoch": 1.218841081874325,
      "grad_norm": 2.870012044906616,
      "learning_rate": 2.9686151818410345e-05,
      "loss": 2.8035,
      "step": 119600
    },
    {
      "epoch": 1.2208792776634123,
      "grad_norm": 4.563855171203613,
      "learning_rate": 2.9652181888592217e-05,
      "loss": 2.7799,
      "step": 119800
    },
    {
      "epoch": 1.2229174734525,
      "grad_norm": 6.296120643615723,
      "learning_rate": 2.9618211958774093e-05,
      "loss": 2.7955,
      "step": 120000
    },
    {
      "epoch": 1.2249556692415873,
      "grad_norm": 3.4213666915893555,
      "learning_rate": 2.9584242028955972e-05,
      "loss": 2.8248,
      "step": 120200
    },
    {
      "epoch": 1.2269938650306749,
      "grad_norm": 6.768848896026611,
      "learning_rate": 2.9550272099137845e-05,
      "loss": 2.7973,
      "step": 120400
    },
    {
      "epoch": 1.2290320608197622,
      "grad_norm": 4.567325592041016,
      "learning_rate": 2.951630216931972e-05,
      "loss": 2.808,
      "step": 120600
    },
    {
      "epoch": 1.2310702566088498,
      "grad_norm": 8.493359565734863,
      "learning_rate": 2.9482332239501593e-05,
      "loss": 2.8017,
      "step": 120800
    },
    {
      "epoch": 1.2331084523979374,
      "grad_norm": 3.4786810874938965,
      "learning_rate": 2.944836230968347e-05,
      "loss": 2.7866,
      "step": 121000
    },
    {
      "epoch": 1.2351466481870248,
      "grad_norm": 3.572676658630371,
      "learning_rate": 2.9414392379865348e-05,
      "loss": 2.7556,
      "step": 121200
    },
    {
      "epoch": 1.2371848439761124,
      "grad_norm": 3.701120376586914,
      "learning_rate": 2.9380422450047217e-05,
      "loss": 2.7743,
      "step": 121400
    },
    {
      "epoch": 1.2392230397651998,
      "grad_norm": 3.5713043212890625,
      "learning_rate": 2.9346452520229096e-05,
      "loss": 2.7427,
      "step": 121600
    },
    {
      "epoch": 1.2412612355542874,
      "grad_norm": 7.089296340942383,
      "learning_rate": 2.9312482590410972e-05,
      "loss": 2.7381,
      "step": 121800
    },
    {
      "epoch": 1.2432994313433747,
      "grad_norm": 6.035057067871094,
      "learning_rate": 2.9278512660592844e-05,
      "loss": 2.8078,
      "step": 122000
    },
    {
      "epoch": 1.2453376271324623,
      "grad_norm": 5.200030326843262,
      "learning_rate": 2.924454273077472e-05,
      "loss": 2.7924,
      "step": 122200
    },
    {
      "epoch": 1.24737582292155,
      "grad_norm": 3.8127024173736572,
      "learning_rate": 2.9210572800956592e-05,
      "loss": 2.736,
      "step": 122400
    },
    {
      "epoch": 1.2494140187106373,
      "grad_norm": 4.2749786376953125,
      "learning_rate": 2.9176602871138468e-05,
      "loss": 2.8052,
      "step": 122600
    },
    {
      "epoch": 1.2514522144997249,
      "grad_norm": 3.7425007820129395,
      "learning_rate": 2.9142632941320347e-05,
      "loss": 2.787,
      "step": 122800
    },
    {
      "epoch": 1.2534904102888125,
      "grad_norm": 5.994971752166748,
      "learning_rate": 2.910866301150222e-05,
      "loss": 2.7607,
      "step": 123000
    },
    {
      "epoch": 1.2555286060778998,
      "grad_norm": 4.468596458435059,
      "learning_rate": 2.9074693081684095e-05,
      "loss": 2.8007,
      "step": 123200
    },
    {
      "epoch": 1.2575668018669872,
      "grad_norm": 3.5641963481903076,
      "learning_rate": 2.9040723151865968e-05,
      "loss": 2.7851,
      "step": 123400
    },
    {
      "epoch": 1.2596049976560748,
      "grad_norm": 3.150827646255493,
      "learning_rate": 2.9006753222047844e-05,
      "loss": 2.7675,
      "step": 123600
    },
    {
      "epoch": 1.2616431934451624,
      "grad_norm": 4.19735050201416,
      "learning_rate": 2.8972783292229723e-05,
      "loss": 2.7387,
      "step": 123800
    },
    {
      "epoch": 1.2636813892342498,
      "grad_norm": 2.999969244003296,
      "learning_rate": 2.8938813362411592e-05,
      "loss": 2.7985,
      "step": 124000
    },
    {
      "epoch": 1.2657195850233374,
      "grad_norm": 6.125536918640137,
      "learning_rate": 2.890484343259347e-05,
      "loss": 2.876,
      "step": 124200
    },
    {
      "epoch": 1.267757780812425,
      "grad_norm": 5.883357048034668,
      "learning_rate": 2.8870873502775347e-05,
      "loss": 2.824,
      "step": 124400
    },
    {
      "epoch": 1.2697959766015123,
      "grad_norm": 4.792079448699951,
      "learning_rate": 2.883690357295722e-05,
      "loss": 2.787,
      "step": 124600
    },
    {
      "epoch": 1.2718341723905997,
      "grad_norm": 2.7337799072265625,
      "learning_rate": 2.8802933643139095e-05,
      "loss": 2.7736,
      "step": 124800
    },
    {
      "epoch": 1.2738723681796873,
      "grad_norm": 11.146282196044922,
      "learning_rate": 2.8768963713320967e-05,
      "loss": 2.7867,
      "step": 125000
    },
    {
      "epoch": 1.275910563968775,
      "grad_norm": 3.8376047611236572,
      "learning_rate": 2.8734993783502846e-05,
      "loss": 2.746,
      "step": 125200
    },
    {
      "epoch": 1.2779487597578623,
      "grad_norm": 2.412426471710205,
      "learning_rate": 2.8701023853684722e-05,
      "loss": 2.8211,
      "step": 125400
    },
    {
      "epoch": 1.2799869555469499,
      "grad_norm": 14.044364929199219,
      "learning_rate": 2.8667053923866595e-05,
      "loss": 2.8397,
      "step": 125600
    },
    {
      "epoch": 1.2820251513360374,
      "grad_norm": 5.090047836303711,
      "learning_rate": 2.863308399404847e-05,
      "loss": 2.8533,
      "step": 125800
    },
    {
      "epoch": 1.2840633471251248,
      "grad_norm": 3.9909098148345947,
      "learning_rate": 2.8599114064230343e-05,
      "loss": 2.795,
      "step": 126000
    },
    {
      "epoch": 1.2861015429142124,
      "grad_norm": 3.4689085483551025,
      "learning_rate": 2.856514413441222e-05,
      "loss": 2.7605,
      "step": 126200
    },
    {
      "epoch": 1.2881397387032998,
      "grad_norm": 3.648536205291748,
      "learning_rate": 2.8531174204594098e-05,
      "loss": 2.7764,
      "step": 126400
    },
    {
      "epoch": 1.2901779344923874,
      "grad_norm": 5.46500825881958,
      "learning_rate": 2.8497204274775967e-05,
      "loss": 2.758,
      "step": 126600
    },
    {
      "epoch": 1.2922161302814748,
      "grad_norm": 2.7773189544677734,
      "learning_rate": 2.8463234344957846e-05,
      "loss": 2.777,
      "step": 126800
    },
    {
      "epoch": 1.2942543260705623,
      "grad_norm": 4.611245632171631,
      "learning_rate": 2.8429264415139718e-05,
      "loss": 2.7986,
      "step": 127000
    },
    {
      "epoch": 1.29629252185965,
      "grad_norm": 3.6994545459747314,
      "learning_rate": 2.8395294485321594e-05,
      "loss": 2.8096,
      "step": 127200
    },
    {
      "epoch": 1.2983307176487373,
      "grad_norm": 3.47835111618042,
      "learning_rate": 2.836132455550347e-05,
      "loss": 2.7948,
      "step": 127400
    },
    {
      "epoch": 1.300368913437825,
      "grad_norm": 3.0459702014923096,
      "learning_rate": 2.8327354625685342e-05,
      "loss": 2.8059,
      "step": 127600
    },
    {
      "epoch": 1.3024071092269123,
      "grad_norm": 2.445066213607788,
      "learning_rate": 2.829338469586722e-05,
      "loss": 2.8087,
      "step": 127800
    },
    {
      "epoch": 1.3044453050159999,
      "grad_norm": 3.6974029541015625,
      "learning_rate": 2.8259414766049097e-05,
      "loss": 2.8014,
      "step": 128000
    },
    {
      "epoch": 1.3064835008050872,
      "grad_norm": 3.8844103813171387,
      "learning_rate": 2.822544483623097e-05,
      "loss": 2.7569,
      "step": 128200
    },
    {
      "epoch": 1.3085216965941748,
      "grad_norm": 8.879800796508789,
      "learning_rate": 2.8191474906412845e-05,
      "loss": 2.8144,
      "step": 128400
    },
    {
      "epoch": 1.3105598923832624,
      "grad_norm": 3.1060028076171875,
      "learning_rate": 2.8157504976594718e-05,
      "loss": 2.7906,
      "step": 128600
    },
    {
      "epoch": 1.3125980881723498,
      "grad_norm": 3.1955068111419678,
      "learning_rate": 2.8123535046776593e-05,
      "loss": 2.8303,
      "step": 128800
    },
    {
      "epoch": 1.3146362839614374,
      "grad_norm": 3.1721982955932617,
      "learning_rate": 2.8089565116958472e-05,
      "loss": 2.762,
      "step": 129000
    },
    {
      "epoch": 1.3166744797505248,
      "grad_norm": 4.052091121673584,
      "learning_rate": 2.8055595187140345e-05,
      "loss": 2.7996,
      "step": 129200
    },
    {
      "epoch": 1.3187126755396124,
      "grad_norm": 3.603166103363037,
      "learning_rate": 2.802162525732222e-05,
      "loss": 2.8431,
      "step": 129400
    },
    {
      "epoch": 1.3207508713286997,
      "grad_norm": 3.4236578941345215,
      "learning_rate": 2.7987655327504093e-05,
      "loss": 2.8335,
      "step": 129600
    },
    {
      "epoch": 1.3227890671177873,
      "grad_norm": 3.6945672035217285,
      "learning_rate": 2.795368539768597e-05,
      "loss": 2.7672,
      "step": 129800
    },
    {
      "epoch": 1.324827262906875,
      "grad_norm": 3.479315996170044,
      "learning_rate": 2.7919715467867848e-05,
      "loss": 2.8411,
      "step": 130000
    },
    {
      "epoch": 1.3268654586959623,
      "grad_norm": 4.272213935852051,
      "learning_rate": 2.7885745538049717e-05,
      "loss": 2.7911,
      "step": 130200
    },
    {
      "epoch": 1.3289036544850499,
      "grad_norm": 4.259757041931152,
      "learning_rate": 2.7851775608231596e-05,
      "loss": 2.8616,
      "step": 130400
    },
    {
      "epoch": 1.3309418502741375,
      "grad_norm": 3.4342386722564697,
      "learning_rate": 2.7817805678413472e-05,
      "loss": 2.7802,
      "step": 130600
    },
    {
      "epoch": 1.3329800460632248,
      "grad_norm": 3.391420841217041,
      "learning_rate": 2.7783835748595344e-05,
      "loss": 2.8237,
      "step": 130800
    },
    {
      "epoch": 1.3350182418523122,
      "grad_norm": 3.922518491744995,
      "learning_rate": 2.774986581877722e-05,
      "loss": 2.7747,
      "step": 131000
    },
    {
      "epoch": 1.3370564376413998,
      "grad_norm": 3.478022336959839,
      "learning_rate": 2.7715895888959092e-05,
      "loss": 2.7775,
      "step": 131200
    },
    {
      "epoch": 1.3390946334304874,
      "grad_norm": 4.757335186004639,
      "learning_rate": 2.7681925959140968e-05,
      "loss": 2.7927,
      "step": 131400
    },
    {
      "epoch": 1.3411328292195748,
      "grad_norm": 3.3490309715270996,
      "learning_rate": 2.7647956029322847e-05,
      "loss": 2.8097,
      "step": 131600
    },
    {
      "epoch": 1.3431710250086624,
      "grad_norm": 2.535917043685913,
      "learning_rate": 2.761398609950472e-05,
      "loss": 2.8113,
      "step": 131800
    },
    {
      "epoch": 1.34520922079775,
      "grad_norm": 4.417048931121826,
      "learning_rate": 2.7580016169686595e-05,
      "loss": 2.7229,
      "step": 132000
    },
    {
      "epoch": 1.3472474165868373,
      "grad_norm": 2.560427665710449,
      "learning_rate": 2.7546046239868468e-05,
      "loss": 2.7675,
      "step": 132200
    },
    {
      "epoch": 1.3492856123759247,
      "grad_norm": 2.5731098651885986,
      "learning_rate": 2.7512076310050344e-05,
      "loss": 2.767,
      "step": 132400
    },
    {
      "epoch": 1.3513238081650123,
      "grad_norm": 4.813544750213623,
      "learning_rate": 2.7478106380232223e-05,
      "loss": 2.7642,
      "step": 132600
    },
    {
      "epoch": 1.3533620039541,
      "grad_norm": 3.87744402885437,
      "learning_rate": 2.7444136450414092e-05,
      "loss": 2.7912,
      "step": 132800
    },
    {
      "epoch": 1.3554001997431873,
      "grad_norm": 4.182324409484863,
      "learning_rate": 2.741016652059597e-05,
      "loss": 2.8006,
      "step": 133000
    },
    {
      "epoch": 1.3574383955322749,
      "grad_norm": 3.0133535861968994,
      "learning_rate": 2.7376196590777847e-05,
      "loss": 2.8453,
      "step": 133200
    },
    {
      "epoch": 1.3594765913213624,
      "grad_norm": 2.590618848800659,
      "learning_rate": 2.734222666095972e-05,
      "loss": 2.7615,
      "step": 133400
    },
    {
      "epoch": 1.3615147871104498,
      "grad_norm": 7.970355987548828,
      "learning_rate": 2.7308256731141595e-05,
      "loss": 2.7645,
      "step": 133600
    },
    {
      "epoch": 1.3635529828995372,
      "grad_norm": 4.076469421386719,
      "learning_rate": 2.7274286801323467e-05,
      "loss": 2.829,
      "step": 133800
    },
    {
      "epoch": 1.3655911786886248,
      "grad_norm": 3.667879343032837,
      "learning_rate": 2.7240316871505346e-05,
      "loss": 2.7856,
      "step": 134000
    },
    {
      "epoch": 1.3676293744777124,
      "grad_norm": 4.81649923324585,
      "learning_rate": 2.7206346941687222e-05,
      "loss": 2.8308,
      "step": 134200
    },
    {
      "epoch": 1.3696675702667997,
      "grad_norm": 2.9841268062591553,
      "learning_rate": 2.7172377011869095e-05,
      "loss": 2.7702,
      "step": 134400
    },
    {
      "epoch": 1.3717057660558873,
      "grad_norm": 3.3774585723876953,
      "learning_rate": 2.713840708205097e-05,
      "loss": 2.8371,
      "step": 134600
    },
    {
      "epoch": 1.373743961844975,
      "grad_norm": 4.156287670135498,
      "learning_rate": 2.7104437152232843e-05,
      "loss": 2.8177,
      "step": 134800
    },
    {
      "epoch": 1.3757821576340623,
      "grad_norm": 5.42727518081665,
      "learning_rate": 2.707046722241472e-05,
      "loss": 2.7731,
      "step": 135000
    },
    {
      "epoch": 1.37782035342315,
      "grad_norm": 3.7094407081604004,
      "learning_rate": 2.7036497292596598e-05,
      "loss": 2.8367,
      "step": 135200
    },
    {
      "epoch": 1.3798585492122373,
      "grad_norm": 3.554776430130005,
      "learning_rate": 2.7002527362778467e-05,
      "loss": 2.76,
      "step": 135400
    },
    {
      "epoch": 1.3818967450013249,
      "grad_norm": 3.2242367267608643,
      "learning_rate": 2.6968557432960346e-05,
      "loss": 2.7732,
      "step": 135600
    },
    {
      "epoch": 1.3839349407904122,
      "grad_norm": 4.177988052368164,
      "learning_rate": 2.693458750314222e-05,
      "loss": 2.7473,
      "step": 135800
    },
    {
      "epoch": 1.3859731365794998,
      "grad_norm": 2.633357524871826,
      "learning_rate": 2.6900617573324094e-05,
      "loss": 2.7888,
      "step": 136000
    },
    {
      "epoch": 1.3880113323685874,
      "grad_norm": 3.8948590755462646,
      "learning_rate": 2.686664764350597e-05,
      "loss": 2.7904,
      "step": 136200
    },
    {
      "epoch": 1.3900495281576748,
      "grad_norm": 3.0978875160217285,
      "learning_rate": 2.6832677713687842e-05,
      "loss": 2.8079,
      "step": 136400
    },
    {
      "epoch": 1.3920877239467624,
      "grad_norm": 4.474449634552002,
      "learning_rate": 2.679870778386972e-05,
      "loss": 2.8358,
      "step": 136600
    },
    {
      "epoch": 1.3941259197358498,
      "grad_norm": 3.568626642227173,
      "learning_rate": 2.6764737854051597e-05,
      "loss": 2.8407,
      "step": 136800
    },
    {
      "epoch": 1.3961641155249374,
      "grad_norm": 5.590539932250977,
      "learning_rate": 2.673076792423347e-05,
      "loss": 2.8258,
      "step": 137000
    },
    {
      "epoch": 1.3982023113140247,
      "grad_norm": 2.9772400856018066,
      "learning_rate": 2.6696797994415345e-05,
      "loss": 2.7707,
      "step": 137200
    },
    {
      "epoch": 1.4002405071031123,
      "grad_norm": 4.467590808868408,
      "learning_rate": 2.6662828064597218e-05,
      "loss": 2.8152,
      "step": 137400
    },
    {
      "epoch": 1.4022787028922,
      "grad_norm": 5.4745893478393555,
      "learning_rate": 2.6628858134779093e-05,
      "loss": 2.8056,
      "step": 137600
    },
    {
      "epoch": 1.4043168986812873,
      "grad_norm": 7.749890327453613,
      "learning_rate": 2.6594888204960973e-05,
      "loss": 2.8187,
      "step": 137800
    },
    {
      "epoch": 1.4063550944703749,
      "grad_norm": 4.2193522453308105,
      "learning_rate": 2.656091827514284e-05,
      "loss": 2.8501,
      "step": 138000
    },
    {
      "epoch": 1.4083932902594622,
      "grad_norm": 4.890689849853516,
      "learning_rate": 2.652694834532472e-05,
      "loss": 2.8013,
      "step": 138200
    },
    {
      "epoch": 1.4104314860485498,
      "grad_norm": 6.146521091461182,
      "learning_rate": 2.6492978415506596e-05,
      "loss": 2.7799,
      "step": 138400
    },
    {
      "epoch": 1.4124696818376372,
      "grad_norm": 4.903788089752197,
      "learning_rate": 2.645900848568847e-05,
      "loss": 2.8349,
      "step": 138600
    },
    {
      "epoch": 1.4145078776267248,
      "grad_norm": 6.887089252471924,
      "learning_rate": 2.6425038555870345e-05,
      "loss": 2.7646,
      "step": 138800
    },
    {
      "epoch": 1.4165460734158124,
      "grad_norm": 2.7332541942596436,
      "learning_rate": 2.6391068626052217e-05,
      "loss": 2.7879,
      "step": 139000
    },
    {
      "epoch": 1.4185842692048998,
      "grad_norm": 6.7633056640625,
      "learning_rate": 2.6357098696234096e-05,
      "loss": 2.8132,
      "step": 139200
    },
    {
      "epoch": 1.4206224649939874,
      "grad_norm": 5.588972091674805,
      "learning_rate": 2.6323128766415972e-05,
      "loss": 2.785,
      "step": 139400
    },
    {
      "epoch": 1.422660660783075,
      "grad_norm": 4.176382064819336,
      "learning_rate": 2.6289158836597844e-05,
      "loss": 2.8044,
      "step": 139600
    },
    {
      "epoch": 1.4246988565721623,
      "grad_norm": 3.536524772644043,
      "learning_rate": 2.625518890677972e-05,
      "loss": 2.8077,
      "step": 139800
    },
    {
      "epoch": 1.4267370523612497,
      "grad_norm": 3.7861034870147705,
      "learning_rate": 2.6221218976961592e-05,
      "loss": 2.8151,
      "step": 140000
    },
    {
      "epoch": 1.4287752481503373,
      "grad_norm": 3.2299306392669678,
      "learning_rate": 2.6187249047143468e-05,
      "loss": 2.8241,
      "step": 140200
    },
    {
      "epoch": 1.430813443939425,
      "grad_norm": 3.9183969497680664,
      "learning_rate": 2.6153279117325347e-05,
      "loss": 2.7884,
      "step": 140400
    },
    {
      "epoch": 1.4328516397285123,
      "grad_norm": 4.39133358001709,
      "learning_rate": 2.611930918750722e-05,
      "loss": 2.7481,
      "step": 140600
    },
    {
      "epoch": 1.4348898355175999,
      "grad_norm": 2.6635286808013916,
      "learning_rate": 2.6085339257689096e-05,
      "loss": 2.7965,
      "step": 140800
    },
    {
      "epoch": 1.4369280313066874,
      "grad_norm": 5.483116626739502,
      "learning_rate": 2.605136932787097e-05,
      "loss": 2.7393,
      "step": 141000
    },
    {
      "epoch": 1.4389662270957748,
      "grad_norm": 2.71246337890625,
      "learning_rate": 2.6017399398052844e-05,
      "loss": 2.8116,
      "step": 141200
    },
    {
      "epoch": 1.4410044228848622,
      "grad_norm": 5.207278728485107,
      "learning_rate": 2.5983429468234723e-05,
      "loss": 2.8486,
      "step": 141400
    },
    {
      "epoch": 1.4430426186739498,
      "grad_norm": 3.1173095703125,
      "learning_rate": 2.5949459538416592e-05,
      "loss": 2.7071,
      "step": 141600
    },
    {
      "epoch": 1.4450808144630374,
      "grad_norm": 4.515846252441406,
      "learning_rate": 2.591548960859847e-05,
      "loss": 2.7936,
      "step": 141800
    },
    {
      "epoch": 1.4471190102521247,
      "grad_norm": 3.4080917835235596,
      "learning_rate": 2.5881519678780347e-05,
      "loss": 2.801,
      "step": 142000
    },
    {
      "epoch": 1.4491572060412123,
      "grad_norm": 3.871311664581299,
      "learning_rate": 2.584754974896222e-05,
      "loss": 2.7595,
      "step": 142200
    },
    {
      "epoch": 1.4511954018303,
      "grad_norm": 2.508666753768921,
      "learning_rate": 2.5813579819144095e-05,
      "loss": 2.7781,
      "step": 142400
    },
    {
      "epoch": 1.4532335976193873,
      "grad_norm": 4.465661525726318,
      "learning_rate": 2.5779609889325967e-05,
      "loss": 2.7578,
      "step": 142600
    },
    {
      "epoch": 1.455271793408475,
      "grad_norm": 3.4782416820526123,
      "learning_rate": 2.5745639959507843e-05,
      "loss": 2.814,
      "step": 142800
    },
    {
      "epoch": 1.4573099891975623,
      "grad_norm": 2.8140673637390137,
      "learning_rate": 2.5711670029689722e-05,
      "loss": 2.8218,
      "step": 143000
    },
    {
      "epoch": 1.4593481849866499,
      "grad_norm": 3.1012187004089355,
      "learning_rate": 2.5677700099871595e-05,
      "loss": 2.8706,
      "step": 143200
    },
    {
      "epoch": 1.4613863807757372,
      "grad_norm": 2.983726739883423,
      "learning_rate": 2.564373017005347e-05,
      "loss": 2.8106,
      "step": 143400
    },
    {
      "epoch": 1.4634245765648248,
      "grad_norm": 4.315713882446289,
      "learning_rate": 2.5609760240235346e-05,
      "loss": 2.8031,
      "step": 143600
    },
    {
      "epoch": 1.4654627723539124,
      "grad_norm": 3.267803430557251,
      "learning_rate": 2.557579031041722e-05,
      "loss": 2.7392,
      "step": 143800
    },
    {
      "epoch": 1.4675009681429998,
      "grad_norm": 3.6576364040374756,
      "learning_rate": 2.5541820380599098e-05,
      "loss": 2.7981,
      "step": 144000
    },
    {
      "epoch": 1.4695391639320874,
      "grad_norm": 3.020533800125122,
      "learning_rate": 2.5507850450780967e-05,
      "loss": 2.7643,
      "step": 144200
    },
    {
      "epoch": 1.4715773597211748,
      "grad_norm": 3.2939300537109375,
      "learning_rate": 2.5473880520962846e-05,
      "loss": 2.7435,
      "step": 144400
    },
    {
      "epoch": 1.4736155555102624,
      "grad_norm": 4.783214092254639,
      "learning_rate": 2.543991059114472e-05,
      "loss": 2.8031,
      "step": 144600
    },
    {
      "epoch": 1.4756537512993497,
      "grad_norm": 3.7040457725524902,
      "learning_rate": 2.5405940661326594e-05,
      "loss": 2.8553,
      "step": 144800
    },
    {
      "epoch": 1.4776919470884373,
      "grad_norm": 3.4100425243377686,
      "learning_rate": 2.537197073150847e-05,
      "loss": 2.7492,
      "step": 145000
    },
    {
      "epoch": 1.479730142877525,
      "grad_norm": 3.360975742340088,
      "learning_rate": 2.5338000801690342e-05,
      "loss": 2.7573,
      "step": 145200
    },
    {
      "epoch": 1.4817683386666123,
      "grad_norm": 2.9003725051879883,
      "learning_rate": 2.530403087187222e-05,
      "loss": 2.8571,
      "step": 145400
    },
    {
      "epoch": 1.4838065344556999,
      "grad_norm": 3.2213544845581055,
      "learning_rate": 2.5270060942054097e-05,
      "loss": 2.788,
      "step": 145600
    },
    {
      "epoch": 1.4858447302447872,
      "grad_norm": 4.451949596405029,
      "learning_rate": 2.523609101223597e-05,
      "loss": 2.7529,
      "step": 145800
    },
    {
      "epoch": 1.4878829260338748,
      "grad_norm": 3.0363986492156982,
      "learning_rate": 2.5202121082417845e-05,
      "loss": 2.7938,
      "step": 146000
    },
    {
      "epoch": 1.4899211218229622,
      "grad_norm": 2.8266189098358154,
      "learning_rate": 2.516815115259972e-05,
      "loss": 2.8005,
      "step": 146200
    },
    {
      "epoch": 1.4919593176120498,
      "grad_norm": 3.3410630226135254,
      "learning_rate": 2.5134181222781593e-05,
      "loss": 2.7943,
      "step": 146400
    },
    {
      "epoch": 1.4939975134011374,
      "grad_norm": 3.7984933853149414,
      "learning_rate": 2.5100211292963473e-05,
      "loss": 2.7986,
      "step": 146600
    },
    {
      "epoch": 1.4960357091902248,
      "grad_norm": 3.013014554977417,
      "learning_rate": 2.506624136314534e-05,
      "loss": 2.7979,
      "step": 146800
    },
    {
      "epoch": 1.4980739049793124,
      "grad_norm": 3.8052282333374023,
      "learning_rate": 2.503227143332722e-05,
      "loss": 2.7841,
      "step": 147000
    },
    {
      "epoch": 1.5001121007684,
      "grad_norm": 3.0800187587738037,
      "learning_rate": 2.4998301503509093e-05,
      "loss": 2.8236,
      "step": 147200
    },
    {
      "epoch": 1.5021502965574873,
      "grad_norm": 2.5951039791107178,
      "learning_rate": 2.496433157369097e-05,
      "loss": 2.7935,
      "step": 147400
    },
    {
      "epoch": 1.5041884923465747,
      "grad_norm": 3.0581254959106445,
      "learning_rate": 2.4930361643872845e-05,
      "loss": 2.8131,
      "step": 147600
    },
    {
      "epoch": 1.5062266881356623,
      "grad_norm": 3.3124115467071533,
      "learning_rate": 2.489639171405472e-05,
      "loss": 2.7391,
      "step": 147800
    },
    {
      "epoch": 1.5082648839247499,
      "grad_norm": 3.0404152870178223,
      "learning_rate": 2.4862421784236596e-05,
      "loss": 2.7452,
      "step": 148000
    },
    {
      "epoch": 1.5103030797138373,
      "grad_norm": 3.585844039916992,
      "learning_rate": 2.482845185441847e-05,
      "loss": 2.7337,
      "step": 148200
    },
    {
      "epoch": 1.5123412755029249,
      "grad_norm": 4.424163341522217,
      "learning_rate": 2.4794481924600348e-05,
      "loss": 2.8181,
      "step": 148400
    },
    {
      "epoch": 1.5143794712920124,
      "grad_norm": 5.128882884979248,
      "learning_rate": 2.476051199478222e-05,
      "loss": 2.7914,
      "step": 148600
    },
    {
      "epoch": 1.5164176670810998,
      "grad_norm": 2.559717893600464,
      "learning_rate": 2.4726542064964096e-05,
      "loss": 2.7944,
      "step": 148800
    },
    {
      "epoch": 1.5184558628701872,
      "grad_norm": 2.562492609024048,
      "learning_rate": 2.4692572135145968e-05,
      "loss": 2.8487,
      "step": 149000
    },
    {
      "epoch": 1.5204940586592748,
      "grad_norm": 5.480463027954102,
      "learning_rate": 2.4658602205327844e-05,
      "loss": 2.7607,
      "step": 149200
    },
    {
      "epoch": 1.5225322544483624,
      "grad_norm": 7.486266136169434,
      "learning_rate": 2.462463227550972e-05,
      "loss": 2.7517,
      "step": 149400
    },
    {
      "epoch": 1.5245704502374497,
      "grad_norm": 3.451239824295044,
      "learning_rate": 2.4590662345691596e-05,
      "loss": 2.8229,
      "step": 149600
    },
    {
      "epoch": 1.5266086460265373,
      "grad_norm": 3.382127046585083,
      "learning_rate": 2.4556692415873468e-05,
      "loss": 2.7812,
      "step": 149800
    },
    {
      "epoch": 1.528646841815625,
      "grad_norm": 3.725557804107666,
      "learning_rate": 2.4522722486055344e-05,
      "loss": 2.7286,
      "step": 150000
    },
    {
      "epoch": 1.5306850376047123,
      "grad_norm": 4.42121696472168,
      "learning_rate": 2.448875255623722e-05,
      "loss": 2.7871,
      "step": 150200
    },
    {
      "epoch": 1.5327232333937997,
      "grad_norm": 5.874940395355225,
      "learning_rate": 2.4454782626419095e-05,
      "loss": 2.8173,
      "step": 150400
    },
    {
      "epoch": 1.5347614291828873,
      "grad_norm": 2.00579571723938,
      "learning_rate": 2.442081269660097e-05,
      "loss": 2.8378,
      "step": 150600
    },
    {
      "epoch": 1.5367996249719749,
      "grad_norm": 3.426154851913452,
      "learning_rate": 2.4386842766782843e-05,
      "loss": 2.7305,
      "step": 150800
    },
    {
      "epoch": 1.5388378207610622,
      "grad_norm": 3.183638572692871,
      "learning_rate": 2.4352872836964723e-05,
      "loss": 2.7792,
      "step": 151000
    },
    {
      "epoch": 1.5408760165501498,
      "grad_norm": 2.5688722133636475,
      "learning_rate": 2.4318902907146595e-05,
      "loss": 2.8078,
      "step": 151200
    },
    {
      "epoch": 1.5429142123392374,
      "grad_norm": 3.71600604057312,
      "learning_rate": 2.428493297732847e-05,
      "loss": 2.7505,
      "step": 151400
    },
    {
      "epoch": 1.5449524081283248,
      "grad_norm": 4.426657676696777,
      "learning_rate": 2.4250963047510343e-05,
      "loss": 2.7825,
      "step": 151600
    },
    {
      "epoch": 1.5469906039174122,
      "grad_norm": 4.124386787414551,
      "learning_rate": 2.421699311769222e-05,
      "loss": 2.7633,
      "step": 151800
    },
    {
      "epoch": 1.5490287997064998,
      "grad_norm": 3.4793312549591064,
      "learning_rate": 2.4183023187874095e-05,
      "loss": 2.77,
      "step": 152000
    },
    {
      "epoch": 1.5510669954955874,
      "grad_norm": 3.984792470932007,
      "learning_rate": 2.414905325805597e-05,
      "loss": 2.7956,
      "step": 152200
    },
    {
      "epoch": 1.5531051912846747,
      "grad_norm": 3.1276774406433105,
      "learning_rate": 2.4115083328237846e-05,
      "loss": 2.8003,
      "step": 152400
    },
    {
      "epoch": 1.5551433870737623,
      "grad_norm": 6.3164801597595215,
      "learning_rate": 2.408111339841972e-05,
      "loss": 2.7744,
      "step": 152600
    },
    {
      "epoch": 1.55718158286285,
      "grad_norm": 8.191272735595703,
      "learning_rate": 2.4047143468601598e-05,
      "loss": 2.7883,
      "step": 152800
    },
    {
      "epoch": 1.5592197786519373,
      "grad_norm": 2.661482572555542,
      "learning_rate": 2.401317353878347e-05,
      "loss": 2.8537,
      "step": 153000
    },
    {
      "epoch": 1.5612579744410247,
      "grad_norm": 2.703493356704712,
      "learning_rate": 2.3979203608965346e-05,
      "loss": 2.8224,
      "step": 153200
    },
    {
      "epoch": 1.5632961702301125,
      "grad_norm": 3.600832223892212,
      "learning_rate": 2.394523367914722e-05,
      "loss": 2.7999,
      "step": 153400
    },
    {
      "epoch": 1.5653343660191998,
      "grad_norm": 2.514446973800659,
      "learning_rate": 2.3911263749329097e-05,
      "loss": 2.7826,
      "step": 153600
    },
    {
      "epoch": 1.5673725618082872,
      "grad_norm": 3.4845597743988037,
      "learning_rate": 2.387729381951097e-05,
      "loss": 2.8167,
      "step": 153800
    },
    {
      "epoch": 1.5694107575973748,
      "grad_norm": 3.019904136657715,
      "learning_rate": 2.3843323889692846e-05,
      "loss": 2.8655,
      "step": 154000
    },
    {
      "epoch": 1.5714489533864624,
      "grad_norm": 3.7930567264556885,
      "learning_rate": 2.3809353959874718e-05,
      "loss": 2.7713,
      "step": 154200
    },
    {
      "epoch": 1.5734871491755498,
      "grad_norm": 5.57610559463501,
      "learning_rate": 2.3775384030056594e-05,
      "loss": 2.7956,
      "step": 154400
    },
    {
      "epoch": 1.5755253449646371,
      "grad_norm": 5.5630269050598145,
      "learning_rate": 2.374141410023847e-05,
      "loss": 2.7419,
      "step": 154600
    },
    {
      "epoch": 1.577563540753725,
      "grad_norm": 4.1832075119018555,
      "learning_rate": 2.3707444170420345e-05,
      "loss": 2.8145,
      "step": 154800
    },
    {
      "epoch": 1.5796017365428123,
      "grad_norm": 4.559085845947266,
      "learning_rate": 2.367347424060222e-05,
      "loss": 2.7856,
      "step": 155000
    },
    {
      "epoch": 1.5816399323318997,
      "grad_norm": 4.4401044845581055,
      "learning_rate": 2.3639504310784093e-05,
      "loss": 2.7631,
      "step": 155200
    },
    {
      "epoch": 1.5836781281209873,
      "grad_norm": 3.797184705734253,
      "learning_rate": 2.3605534380965973e-05,
      "loss": 2.7455,
      "step": 155400
    },
    {
      "epoch": 1.5857163239100749,
      "grad_norm": 7.077930450439453,
      "learning_rate": 2.3571564451147845e-05,
      "loss": 2.7411,
      "step": 155600
    },
    {
      "epoch": 1.5877545196991623,
      "grad_norm": 4.013368129730225,
      "learning_rate": 2.353759452132972e-05,
      "loss": 2.7654,
      "step": 155800
    },
    {
      "epoch": 1.5897927154882499,
      "grad_norm": 2.656524419784546,
      "learning_rate": 2.3503624591511593e-05,
      "loss": 2.7616,
      "step": 156000
    },
    {
      "epoch": 1.5918309112773374,
      "grad_norm": 3.388636350631714,
      "learning_rate": 2.346965466169347e-05,
      "loss": 2.8425,
      "step": 156200
    },
    {
      "epoch": 1.5938691070664248,
      "grad_norm": 4.947645664215088,
      "learning_rate": 2.3435684731875345e-05,
      "loss": 2.7753,
      "step": 156400
    },
    {
      "epoch": 1.5959073028555122,
      "grad_norm": 4.277750015258789,
      "learning_rate": 2.340171480205722e-05,
      "loss": 2.7403,
      "step": 156600
    },
    {
      "epoch": 1.5979454986445998,
      "grad_norm": 2.646623373031616,
      "learning_rate": 2.3367744872239096e-05,
      "loss": 2.7761,
      "step": 156800
    },
    {
      "epoch": 1.5999836944336874,
      "grad_norm": 2.8880486488342285,
      "learning_rate": 2.333377494242097e-05,
      "loss": 2.801,
      "step": 157000
    },
    {
      "epoch": 1.6020218902227747,
      "grad_norm": 5.012478351593018,
      "learning_rate": 2.3299805012602844e-05,
      "loss": 2.8134,
      "step": 157200
    },
    {
      "epoch": 1.6040600860118623,
      "grad_norm": 5.505459785461426,
      "learning_rate": 2.326583508278472e-05,
      "loss": 2.8048,
      "step": 157400
    },
    {
      "epoch": 1.60609828180095,
      "grad_norm": 7.114149570465088,
      "learning_rate": 2.3231865152966596e-05,
      "loss": 2.8216,
      "step": 157600
    },
    {
      "epoch": 1.6081364775900373,
      "grad_norm": 2.5762972831726074,
      "learning_rate": 2.319789522314847e-05,
      "loss": 2.7946,
      "step": 157800
    },
    {
      "epoch": 1.6101746733791247,
      "grad_norm": 8.871098518371582,
      "learning_rate": 2.3163925293330348e-05,
      "loss": 2.7881,
      "step": 158000
    },
    {
      "epoch": 1.6122128691682123,
      "grad_norm": 4.601423263549805,
      "learning_rate": 2.312995536351222e-05,
      "loss": 2.8134,
      "step": 158200
    },
    {
      "epoch": 1.6142510649572999,
      "grad_norm": 10.753267288208008,
      "learning_rate": 2.3095985433694096e-05,
      "loss": 2.7277,
      "step": 158400
    },
    {
      "epoch": 1.6162892607463872,
      "grad_norm": 3.726609468460083,
      "learning_rate": 2.3062015503875968e-05,
      "loss": 2.7605,
      "step": 158600
    },
    {
      "epoch": 1.6183274565354748,
      "grad_norm": 2.797373056411743,
      "learning_rate": 2.3028045574057844e-05,
      "loss": 2.8064,
      "step": 158800
    },
    {
      "epoch": 1.6203656523245624,
      "grad_norm": 6.131628036499023,
      "learning_rate": 2.299407564423972e-05,
      "loss": 2.8446,
      "step": 159000
    },
    {
      "epoch": 1.6224038481136498,
      "grad_norm": 2.563032627105713,
      "learning_rate": 2.2960105714421595e-05,
      "loss": 2.8354,
      "step": 159200
    },
    {
      "epoch": 1.6244420439027372,
      "grad_norm": 4.130604267120361,
      "learning_rate": 2.292613578460347e-05,
      "loss": 2.8196,
      "step": 159400
    },
    {
      "epoch": 1.6264802396918248,
      "grad_norm": 5.2562103271484375,
      "learning_rate": 2.2892165854785344e-05,
      "loss": 2.7458,
      "step": 159600
    },
    {
      "epoch": 1.6285184354809124,
      "grad_norm": 2.5361199378967285,
      "learning_rate": 2.2858195924967223e-05,
      "loss": 2.7995,
      "step": 159800
    },
    {
      "epoch": 1.6305566312699997,
      "grad_norm": 3.7901628017425537,
      "learning_rate": 2.2824225995149095e-05,
      "loss": 2.7504,
      "step": 160000
    },
    {
      "epoch": 1.6325948270590873,
      "grad_norm": 8.439234733581543,
      "learning_rate": 2.279025606533097e-05,
      "loss": 2.7694,
      "step": 160200
    },
    {
      "epoch": 1.634633022848175,
      "grad_norm": 2.504936456680298,
      "learning_rate": 2.2756286135512843e-05,
      "loss": 2.8103,
      "step": 160400
    },
    {
      "epoch": 1.6366712186372623,
      "grad_norm": 2.4878365993499756,
      "learning_rate": 2.2722316205694722e-05,
      "loss": 2.7919,
      "step": 160600
    },
    {
      "epoch": 1.6387094144263497,
      "grad_norm": 6.5654144287109375,
      "learning_rate": 2.2688346275876595e-05,
      "loss": 2.8279,
      "step": 160800
    },
    {
      "epoch": 1.6407476102154372,
      "grad_norm": 4.430654525756836,
      "learning_rate": 2.265437634605847e-05,
      "loss": 2.7937,
      "step": 161000
    },
    {
      "epoch": 1.6427858060045248,
      "grad_norm": 2.9933831691741943,
      "learning_rate": 2.2620406416240343e-05,
      "loss": 2.7466,
      "step": 161200
    },
    {
      "epoch": 1.6448240017936122,
      "grad_norm": 6.17447566986084,
      "learning_rate": 2.258643648642222e-05,
      "loss": 2.7717,
      "step": 161400
    },
    {
      "epoch": 1.6468621975826998,
      "grad_norm": 3.355987310409546,
      "learning_rate": 2.2552466556604094e-05,
      "loss": 2.7434,
      "step": 161600
    },
    {
      "epoch": 1.6489003933717874,
      "grad_norm": 5.06859016418457,
      "learning_rate": 2.251849662678597e-05,
      "loss": 2.83,
      "step": 161800
    },
    {
      "epoch": 1.6509385891608748,
      "grad_norm": 3.4822986125946045,
      "learning_rate": 2.2484526696967846e-05,
      "loss": 2.8107,
      "step": 162000
    },
    {
      "epoch": 1.6529767849499621,
      "grad_norm": 4.294699192047119,
      "learning_rate": 2.245055676714972e-05,
      "loss": 2.7996,
      "step": 162200
    },
    {
      "epoch": 1.65501498073905,
      "grad_norm": 2.587991714477539,
      "learning_rate": 2.2416586837331598e-05,
      "loss": 2.6827,
      "step": 162400
    },
    {
      "epoch": 1.6570531765281373,
      "grad_norm": 3.4586868286132812,
      "learning_rate": 2.238261690751347e-05,
      "loss": 2.8333,
      "step": 162600
    },
    {
      "epoch": 1.6590913723172247,
      "grad_norm": 3.083519458770752,
      "learning_rate": 2.2348646977695346e-05,
      "loss": 2.8097,
      "step": 162800
    },
    {
      "epoch": 1.6611295681063123,
      "grad_norm": 4.759479522705078,
      "learning_rate": 2.2314677047877218e-05,
      "loss": 2.7268,
      "step": 163000
    },
    {
      "epoch": 1.6631677638953999,
      "grad_norm": 3.835216760635376,
      "learning_rate": 2.2280707118059097e-05,
      "loss": 2.8059,
      "step": 163200
    },
    {
      "epoch": 1.6652059596844873,
      "grad_norm": 4.5415425300598145,
      "learning_rate": 2.224673718824097e-05,
      "loss": 2.7618,
      "step": 163400
    },
    {
      "epoch": 1.6672441554735746,
      "grad_norm": 2.685518264770508,
      "learning_rate": 2.2212767258422845e-05,
      "loss": 2.7961,
      "step": 163600
    },
    {
      "epoch": 1.6692823512626624,
      "grad_norm": 3.6450068950653076,
      "learning_rate": 2.217879732860472e-05,
      "loss": 2.7737,
      "step": 163800
    },
    {
      "epoch": 1.6713205470517498,
      "grad_norm": 3.488355875015259,
      "learning_rate": 2.2144827398786594e-05,
      "loss": 2.825,
      "step": 164000
    },
    {
      "epoch": 1.6733587428408372,
      "grad_norm": 3.1241464614868164,
      "learning_rate": 2.2110857468968473e-05,
      "loss": 2.7848,
      "step": 164200
    },
    {
      "epoch": 1.6753969386299248,
      "grad_norm": 3.0737247467041016,
      "learning_rate": 2.2076887539150345e-05,
      "loss": 2.7906,
      "step": 164400
    },
    {
      "epoch": 1.6774351344190124,
      "grad_norm": 2.5448455810546875,
      "learning_rate": 2.204291760933222e-05,
      "loss": 2.7457,
      "step": 164600
    },
    {
      "epoch": 1.6794733302080997,
      "grad_norm": 6.819726467132568,
      "learning_rate": 2.2008947679514093e-05,
      "loss": 2.7121,
      "step": 164800
    },
    {
      "epoch": 1.6815115259971873,
      "grad_norm": 3.862837553024292,
      "learning_rate": 2.1974977749695972e-05,
      "loss": 2.794,
      "step": 165000
    },
    {
      "epoch": 1.683549721786275,
      "grad_norm": 3.332259178161621,
      "learning_rate": 2.1941007819877845e-05,
      "loss": 2.762,
      "step": 165200
    },
    {
      "epoch": 1.6855879175753623,
      "grad_norm": 3.117309808731079,
      "learning_rate": 2.190703789005972e-05,
      "loss": 2.8165,
      "step": 165400
    },
    {
      "epoch": 1.6876261133644497,
      "grad_norm": 4.169917106628418,
      "learning_rate": 2.1873067960241593e-05,
      "loss": 2.8334,
      "step": 165600
    },
    {
      "epoch": 1.6896643091535373,
      "grad_norm": 14.460103034973145,
      "learning_rate": 2.1839098030423472e-05,
      "loss": 2.847,
      "step": 165800
    },
    {
      "epoch": 1.6917025049426249,
      "grad_norm": 4.006240367889404,
      "learning_rate": 2.1805128100605344e-05,
      "loss": 2.783,
      "step": 166000
    },
    {
      "epoch": 1.6937407007317122,
      "grad_norm": 3.974720001220703,
      "learning_rate": 2.177115817078722e-05,
      "loss": 2.8729,
      "step": 166200
    },
    {
      "epoch": 1.6957788965207998,
      "grad_norm": 2.717442750930786,
      "learning_rate": 2.1737188240969096e-05,
      "loss": 2.8089,
      "step": 166400
    },
    {
      "epoch": 1.6978170923098874,
      "grad_norm": 2.6577489376068115,
      "learning_rate": 2.170321831115097e-05,
      "loss": 2.828,
      "step": 166600
    },
    {
      "epoch": 1.6998552880989748,
      "grad_norm": 4.04443359375,
      "learning_rate": 2.1669248381332848e-05,
      "loss": 2.8147,
      "step": 166800
    },
    {
      "epoch": 1.7018934838880622,
      "grad_norm": 5.300879955291748,
      "learning_rate": 2.163527845151472e-05,
      "loss": 2.7506,
      "step": 167000
    },
    {
      "epoch": 1.7039316796771498,
      "grad_norm": 5.483158588409424,
      "learning_rate": 2.1601308521696596e-05,
      "loss": 2.7752,
      "step": 167200
    },
    {
      "epoch": 1.7059698754662374,
      "grad_norm": 4.0930681228637695,
      "learning_rate": 2.1567338591878468e-05,
      "loss": 2.7644,
      "step": 167400
    },
    {
      "epoch": 1.7080080712553247,
      "grad_norm": 5.368070125579834,
      "learning_rate": 2.1533368662060347e-05,
      "loss": 2.7621,
      "step": 167600
    },
    {
      "epoch": 1.7100462670444123,
      "grad_norm": 3.693690538406372,
      "learning_rate": 2.149939873224222e-05,
      "loss": 2.7921,
      "step": 167800
    },
    {
      "epoch": 1.7120844628335,
      "grad_norm": 2.4214465618133545,
      "learning_rate": 2.1465428802424095e-05,
      "loss": 2.7305,
      "step": 168000
    },
    {
      "epoch": 1.7141226586225873,
      "grad_norm": 5.35261344909668,
      "learning_rate": 2.143145887260597e-05,
      "loss": 2.7563,
      "step": 168200
    },
    {
      "epoch": 1.7161608544116747,
      "grad_norm": 5.148556232452393,
      "learning_rate": 2.1397488942787844e-05,
      "loss": 2.8228,
      "step": 168400
    },
    {
      "epoch": 1.7181990502007622,
      "grad_norm": 3.2646257877349854,
      "learning_rate": 2.136351901296972e-05,
      "loss": 2.7939,
      "step": 168600
    },
    {
      "epoch": 1.7202372459898498,
      "grad_norm": 3.3927199840545654,
      "learning_rate": 2.1329549083151595e-05,
      "loss": 2.805,
      "step": 168800
    },
    {
      "epoch": 1.7222754417789372,
      "grad_norm": 3.019742727279663,
      "learning_rate": 2.129557915333347e-05,
      "loss": 2.8062,
      "step": 169000
    },
    {
      "epoch": 1.7243136375680248,
      "grad_norm": 4.334753513336182,
      "learning_rate": 2.1261609223515343e-05,
      "loss": 2.8046,
      "step": 169200
    },
    {
      "epoch": 1.7263518333571124,
      "grad_norm": 5.3326029777526855,
      "learning_rate": 2.1227639293697222e-05,
      "loss": 2.7858,
      "step": 169400
    },
    {
      "epoch": 1.7283900291461998,
      "grad_norm": 2.8196182250976562,
      "learning_rate": 2.1193669363879095e-05,
      "loss": 2.7261,
      "step": 169600
    },
    {
      "epoch": 1.7304282249352871,
      "grad_norm": 2.5618104934692383,
      "learning_rate": 2.115969943406097e-05,
      "loss": 2.8103,
      "step": 169800
    },
    {
      "epoch": 1.7324664207243747,
      "grad_norm": 4.982825756072998,
      "learning_rate": 2.1125729504242843e-05,
      "loss": 2.8209,
      "step": 170000
    },
    {
      "epoch": 1.7345046165134623,
      "grad_norm": 3.1065759658813477,
      "learning_rate": 2.1091759574424722e-05,
      "loss": 2.8068,
      "step": 170200
    },
    {
      "epoch": 1.7365428123025497,
      "grad_norm": 7.330056190490723,
      "learning_rate": 2.1057789644606595e-05,
      "loss": 2.783,
      "step": 170400
    },
    {
      "epoch": 1.7385810080916373,
      "grad_norm": 2.4661998748779297,
      "learning_rate": 2.102381971478847e-05,
      "loss": 2.8113,
      "step": 170600
    },
    {
      "epoch": 1.7406192038807249,
      "grad_norm": 3.6523194313049316,
      "learning_rate": 2.0989849784970346e-05,
      "loss": 2.82,
      "step": 170800
    },
    {
      "epoch": 1.7426573996698123,
      "grad_norm": 3.093851089477539,
      "learning_rate": 2.095587985515222e-05,
      "loss": 2.8159,
      "step": 171000
    },
    {
      "epoch": 1.7446955954588996,
      "grad_norm": 3.232919931411743,
      "learning_rate": 2.0921909925334098e-05,
      "loss": 2.8084,
      "step": 171200
    },
    {
      "epoch": 1.7467337912479874,
      "grad_norm": 3.0422823429107666,
      "learning_rate": 2.088793999551597e-05,
      "loss": 2.7162,
      "step": 171400
    },
    {
      "epoch": 1.7487719870370748,
      "grad_norm": 3.827481508255005,
      "learning_rate": 2.0853970065697846e-05,
      "loss": 2.8197,
      "step": 171600
    },
    {
      "epoch": 1.7508101828261622,
      "grad_norm": 3.524271011352539,
      "learning_rate": 2.0820000135879718e-05,
      "loss": 2.8224,
      "step": 171800
    },
    {
      "epoch": 1.7528483786152498,
      "grad_norm": 4.091549873352051,
      "learning_rate": 2.0786030206061597e-05,
      "loss": 2.7915,
      "step": 172000
    },
    {
      "epoch": 1.7548865744043374,
      "grad_norm": 4.071974754333496,
      "learning_rate": 2.075206027624347e-05,
      "loss": 2.7948,
      "step": 172200
    },
    {
      "epoch": 1.7569247701934247,
      "grad_norm": 3.274961233139038,
      "learning_rate": 2.0718090346425345e-05,
      "loss": 2.7933,
      "step": 172400
    },
    {
      "epoch": 1.7589629659825121,
      "grad_norm": 3.163984537124634,
      "learning_rate": 2.0684120416607218e-05,
      "loss": 2.7576,
      "step": 172600
    },
    {
      "epoch": 1.7610011617716,
      "grad_norm": 4.383056640625,
      "learning_rate": 2.0650150486789097e-05,
      "loss": 2.7493,
      "step": 172800
    },
    {
      "epoch": 1.7630393575606873,
      "grad_norm": 3.1375999450683594,
      "learning_rate": 2.061618055697097e-05,
      "loss": 2.8095,
      "step": 173000
    },
    {
      "epoch": 1.7650775533497747,
      "grad_norm": 3.1882176399230957,
      "learning_rate": 2.0582210627152845e-05,
      "loss": 2.8098,
      "step": 173200
    },
    {
      "epoch": 1.7671157491388623,
      "grad_norm": 4.745082378387451,
      "learning_rate": 2.054824069733472e-05,
      "loss": 2.7948,
      "step": 173400
    },
    {
      "epoch": 1.7691539449279499,
      "grad_norm": 3.6158182621002197,
      "learning_rate": 2.0514270767516593e-05,
      "loss": 2.8362,
      "step": 173600
    },
    {
      "epoch": 1.7711921407170372,
      "grad_norm": 2.9538369178771973,
      "learning_rate": 2.0480300837698472e-05,
      "loss": 2.7508,
      "step": 173800
    },
    {
      "epoch": 1.7732303365061248,
      "grad_norm": 8.11663818359375,
      "learning_rate": 2.0446330907880345e-05,
      "loss": 2.7243,
      "step": 174000
    },
    {
      "epoch": 1.7752685322952124,
      "grad_norm": 2.421095132827759,
      "learning_rate": 2.041236097806222e-05,
      "loss": 2.7945,
      "step": 174200
    },
    {
      "epoch": 1.7773067280842998,
      "grad_norm": 5.2193603515625,
      "learning_rate": 2.0378391048244093e-05,
      "loss": 2.7488,
      "step": 174400
    },
    {
      "epoch": 1.7793449238733872,
      "grad_norm": 3.7824432849884033,
      "learning_rate": 2.0344421118425972e-05,
      "loss": 2.7686,
      "step": 174600
    },
    {
      "epoch": 1.7813831196624748,
      "grad_norm": 2.656790256500244,
      "learning_rate": 2.0310451188607845e-05,
      "loss": 2.8206,
      "step": 174800
    },
    {
      "epoch": 1.7834213154515624,
      "grad_norm": 4.219142913818359,
      "learning_rate": 2.027648125878972e-05,
      "loss": 2.8076,
      "step": 175000
    },
    {
      "epoch": 1.7854595112406497,
      "grad_norm": 5.32737922668457,
      "learning_rate": 2.0242511328971596e-05,
      "loss": 2.785,
      "step": 175200
    },
    {
      "epoch": 1.7874977070297373,
      "grad_norm": 6.126087188720703,
      "learning_rate": 2.0208541399153472e-05,
      "loss": 2.754,
      "step": 175400
    },
    {
      "epoch": 1.789535902818825,
      "grad_norm": 3.5332908630371094,
      "learning_rate": 2.0174571469335348e-05,
      "loss": 2.6672,
      "step": 175600
    },
    {
      "epoch": 1.7915740986079123,
      "grad_norm": 3.5266366004943848,
      "learning_rate": 2.014060153951722e-05,
      "loss": 2.8295,
      "step": 175800
    },
    {
      "epoch": 1.7936122943969997,
      "grad_norm": 6.215550422668457,
      "learning_rate": 2.0106631609699096e-05,
      "loss": 2.7551,
      "step": 176000
    },
    {
      "epoch": 1.7956504901860872,
      "grad_norm": 6.898767471313477,
      "learning_rate": 2.0072661679880968e-05,
      "loss": 2.792,
      "step": 176200
    },
    {
      "epoch": 1.7976886859751748,
      "grad_norm": 3.996828317642212,
      "learning_rate": 2.0038691750062847e-05,
      "loss": 2.8296,
      "step": 176400
    },
    {
      "epoch": 1.7997268817642622,
      "grad_norm": 3.2283546924591064,
      "learning_rate": 2.000472182024472e-05,
      "loss": 2.806,
      "step": 176600
    },
    {
      "epoch": 1.8017650775533498,
      "grad_norm": 3.8888397216796875,
      "learning_rate": 1.9970751890426595e-05,
      "loss": 2.7776,
      "step": 176800
    },
    {
      "epoch": 1.8038032733424374,
      "grad_norm": 2.9104292392730713,
      "learning_rate": 1.9936781960608468e-05,
      "loss": 2.8024,
      "step": 177000
    },
    {
      "epoch": 1.8058414691315248,
      "grad_norm": 3.4743683338165283,
      "learning_rate": 1.9902812030790347e-05,
      "loss": 2.7653,
      "step": 177200
    },
    {
      "epoch": 1.8078796649206121,
      "grad_norm": 2.5155866146087646,
      "learning_rate": 1.986884210097222e-05,
      "loss": 2.7103,
      "step": 177400
    },
    {
      "epoch": 1.8099178607096997,
      "grad_norm": 6.268560886383057,
      "learning_rate": 1.9834872171154095e-05,
      "loss": 2.7881,
      "step": 177600
    },
    {
      "epoch": 1.8119560564987873,
      "grad_norm": 5.003525733947754,
      "learning_rate": 1.980090224133597e-05,
      "loss": 2.8014,
      "step": 177800
    },
    {
      "epoch": 1.8139942522878747,
      "grad_norm": 4.559625148773193,
      "learning_rate": 1.9766932311517843e-05,
      "loss": 2.7653,
      "step": 178000
    },
    {
      "epoch": 1.8160324480769623,
      "grad_norm": 5.702240943908691,
      "learning_rate": 1.9732962381699722e-05,
      "loss": 2.7606,
      "step": 178200
    },
    {
      "epoch": 1.8180706438660499,
      "grad_norm": 4.266539096832275,
      "learning_rate": 1.9698992451881595e-05,
      "loss": 2.7957,
      "step": 178400
    },
    {
      "epoch": 1.8201088396551373,
      "grad_norm": 4.679856300354004,
      "learning_rate": 1.966502252206347e-05,
      "loss": 2.7776,
      "step": 178600
    },
    {
      "epoch": 1.8221470354442246,
      "grad_norm": 4.523270606994629,
      "learning_rate": 1.9631052592245343e-05,
      "loss": 2.7637,
      "step": 178800
    },
    {
      "epoch": 1.8241852312333122,
      "grad_norm": 4.365779399871826,
      "learning_rate": 1.9597082662427222e-05,
      "loss": 2.7859,
      "step": 179000
    },
    {
      "epoch": 1.8262234270223998,
      "grad_norm": 4.860141754150391,
      "learning_rate": 1.9563112732609095e-05,
      "loss": 2.821,
      "step": 179200
    },
    {
      "epoch": 1.8282616228114872,
      "grad_norm": 4.730323791503906,
      "learning_rate": 1.952914280279097e-05,
      "loss": 2.7309,
      "step": 179400
    },
    {
      "epoch": 1.8302998186005748,
      "grad_norm": 3.711771011352539,
      "learning_rate": 1.9495172872972846e-05,
      "loss": 2.7262,
      "step": 179600
    },
    {
      "epoch": 1.8323380143896624,
      "grad_norm": 2.438781499862671,
      "learning_rate": 1.9461202943154722e-05,
      "loss": 2.7209,
      "step": 179800
    },
    {
      "epoch": 1.8343762101787497,
      "grad_norm": 2.454969644546509,
      "learning_rate": 1.9427233013336594e-05,
      "loss": 2.8343,
      "step": 180000
    },
    {
      "epoch": 1.8364144059678371,
      "grad_norm": 4.542341709136963,
      "learning_rate": 1.939326308351847e-05,
      "loss": 2.7545,
      "step": 180200
    },
    {
      "epoch": 1.838452601756925,
      "grad_norm": 4.222282409667969,
      "learning_rate": 1.9359293153700346e-05,
      "loss": 2.7925,
      "step": 180400
    },
    {
      "epoch": 1.8404907975460123,
      "grad_norm": 4.1638898849487305,
      "learning_rate": 1.9325323223882218e-05,
      "loss": 2.8293,
      "step": 180600
    },
    {
      "epoch": 1.8425289933350997,
      "grad_norm": 3.102202892303467,
      "learning_rate": 1.9291353294064097e-05,
      "loss": 2.7337,
      "step": 180800
    },
    {
      "epoch": 1.8445671891241873,
      "grad_norm": 3.5547118186950684,
      "learning_rate": 1.925738336424597e-05,
      "loss": 2.7398,
      "step": 181000
    },
    {
      "epoch": 1.8466053849132749,
      "grad_norm": 5.452940464019775,
      "learning_rate": 1.9223413434427846e-05,
      "loss": 2.8055,
      "step": 181200
    },
    {
      "epoch": 1.8486435807023622,
      "grad_norm": 3.606092691421509,
      "learning_rate": 1.9189443504609718e-05,
      "loss": 2.7897,
      "step": 181400
    },
    {
      "epoch": 1.8506817764914496,
      "grad_norm": 3.1785149574279785,
      "learning_rate": 1.9155473574791597e-05,
      "loss": 2.8157,
      "step": 181600
    },
    {
      "epoch": 1.8527199722805374,
      "grad_norm": 6.239540100097656,
      "learning_rate": 1.912150364497347e-05,
      "loss": 2.7559,
      "step": 181800
    },
    {
      "epoch": 1.8547581680696248,
      "grad_norm": 3.8920791149139404,
      "learning_rate": 1.9087533715155345e-05,
      "loss": 2.7682,
      "step": 182000
    },
    {
      "epoch": 1.8567963638587122,
      "grad_norm": 2.978282928466797,
      "learning_rate": 1.905356378533722e-05,
      "loss": 2.7481,
      "step": 182200
    },
    {
      "epoch": 1.8588345596477998,
      "grad_norm": 3.652186632156372,
      "learning_rate": 1.9019593855519097e-05,
      "loss": 2.8241,
      "step": 182400
    },
    {
      "epoch": 1.8608727554368873,
      "grad_norm": 3.2990903854370117,
      "learning_rate": 1.8985623925700973e-05,
      "loss": 2.7559,
      "step": 182600
    },
    {
      "epoch": 1.8629109512259747,
      "grad_norm": 2.631829261779785,
      "learning_rate": 1.8951653995882845e-05,
      "loss": 2.8714,
      "step": 182800
    },
    {
      "epoch": 1.8649491470150623,
      "grad_norm": 4.4598708152771,
      "learning_rate": 1.891768406606472e-05,
      "loss": 2.6826,
      "step": 183000
    },
    {
      "epoch": 1.86698734280415,
      "grad_norm": 3.890915870666504,
      "learning_rate": 1.8883714136246593e-05,
      "loss": 2.7526,
      "step": 183200
    },
    {
      "epoch": 1.8690255385932373,
      "grad_norm": 3.904573440551758,
      "learning_rate": 1.8849744206428472e-05,
      "loss": 2.7614,
      "step": 183400
    },
    {
      "epoch": 1.8710637343823247,
      "grad_norm": 3.8599929809570312,
      "learning_rate": 1.8815774276610345e-05,
      "loss": 2.7925,
      "step": 183600
    },
    {
      "epoch": 1.8731019301714122,
      "grad_norm": 5.437251091003418,
      "learning_rate": 1.878180434679222e-05,
      "loss": 2.7585,
      "step": 183800
    },
    {
      "epoch": 1.8751401259604998,
      "grad_norm": 4.130099296569824,
      "learning_rate": 1.8747834416974093e-05,
      "loss": 2.78,
      "step": 184000
    },
    {
      "epoch": 1.8771783217495872,
      "grad_norm": 3.9488327503204346,
      "learning_rate": 1.8713864487155972e-05,
      "loss": 2.7725,
      "step": 184200
    },
    {
      "epoch": 1.8792165175386748,
      "grad_norm": 3.167466402053833,
      "learning_rate": 1.8679894557337844e-05,
      "loss": 2.7433,
      "step": 184400
    },
    {
      "epoch": 1.8812547133277624,
      "grad_norm": 3.1620774269104004,
      "learning_rate": 1.864592462751972e-05,
      "loss": 2.7253,
      "step": 184600
    },
    {
      "epoch": 1.8832929091168498,
      "grad_norm": 2.8305764198303223,
      "learning_rate": 1.8611954697701596e-05,
      "loss": 2.7759,
      "step": 184800
    },
    {
      "epoch": 1.8853311049059371,
      "grad_norm": 4.884625434875488,
      "learning_rate": 1.857798476788347e-05,
      "loss": 2.8273,
      "step": 185000
    },
    {
      "epoch": 1.8873693006950247,
      "grad_norm": 2.7282278537750244,
      "learning_rate": 1.8544014838065347e-05,
      "loss": 2.7937,
      "step": 185200
    },
    {
      "epoch": 1.8894074964841123,
      "grad_norm": 5.841241836547852,
      "learning_rate": 1.851004490824722e-05,
      "loss": 2.7715,
      "step": 185400
    },
    {
      "epoch": 1.8914456922731997,
      "grad_norm": 4.223882675170898,
      "learning_rate": 1.8476074978429096e-05,
      "loss": 2.8384,
      "step": 185600
    },
    {
      "epoch": 1.8934838880622873,
      "grad_norm": 3.8070075511932373,
      "learning_rate": 1.8442105048610968e-05,
      "loss": 2.7179,
      "step": 185800
    },
    {
      "epoch": 1.8955220838513749,
      "grad_norm": 3.4350268840789795,
      "learning_rate": 1.8408135118792847e-05,
      "loss": 2.7433,
      "step": 186000
    },
    {
      "epoch": 1.8975602796404623,
      "grad_norm": 2.8344271183013916,
      "learning_rate": 1.837416518897472e-05,
      "loss": 2.7624,
      "step": 186200
    },
    {
      "epoch": 1.8995984754295496,
      "grad_norm": 3.50071120262146,
      "learning_rate": 1.8340195259156595e-05,
      "loss": 2.7201,
      "step": 186400
    },
    {
      "epoch": 1.9016366712186372,
      "grad_norm": 3.3560266494750977,
      "learning_rate": 1.830622532933847e-05,
      "loss": 2.7931,
      "step": 186600
    },
    {
      "epoch": 1.9036748670077248,
      "grad_norm": 3.0217530727386475,
      "learning_rate": 1.8272255399520347e-05,
      "loss": 2.7979,
      "step": 186800
    },
    {
      "epoch": 1.9057130627968122,
      "grad_norm": 3.9167416095733643,
      "learning_rate": 1.8238285469702223e-05,
      "loss": 2.7866,
      "step": 187000
    },
    {
      "epoch": 1.9077512585858998,
      "grad_norm": 2.9607362747192383,
      "learning_rate": 1.8204315539884095e-05,
      "loss": 2.7937,
      "step": 187200
    },
    {
      "epoch": 1.9097894543749874,
      "grad_norm": 3.1583614349365234,
      "learning_rate": 1.817034561006597e-05,
      "loss": 2.7751,
      "step": 187400
    },
    {
      "epoch": 1.9118276501640747,
      "grad_norm": 3.188927412033081,
      "learning_rate": 1.8136375680247846e-05,
      "loss": 2.7512,
      "step": 187600
    },
    {
      "epoch": 1.9138658459531621,
      "grad_norm": 2.9461421966552734,
      "learning_rate": 1.8102405750429722e-05,
      "loss": 2.7402,
      "step": 187800
    },
    {
      "epoch": 1.9159040417422497,
      "grad_norm": 8.555293083190918,
      "learning_rate": 1.8068435820611595e-05,
      "loss": 2.7241,
      "step": 188000
    },
    {
      "epoch": 1.9179422375313373,
      "grad_norm": 3.5881006717681885,
      "learning_rate": 1.803446589079347e-05,
      "loss": 2.7649,
      "step": 188200
    },
    {
      "epoch": 1.9199804333204247,
      "grad_norm": 3.977663040161133,
      "learning_rate": 1.8000495960975343e-05,
      "loss": 2.7701,
      "step": 188400
    },
    {
      "epoch": 1.9220186291095123,
      "grad_norm": 5.893491268157959,
      "learning_rate": 1.7966526031157222e-05,
      "loss": 2.7159,
      "step": 188600
    },
    {
      "epoch": 1.9240568248985999,
      "grad_norm": 4.843561172485352,
      "learning_rate": 1.7932556101339094e-05,
      "loss": 2.7883,
      "step": 188800
    },
    {
      "epoch": 1.9260950206876872,
      "grad_norm": 3.846858501434326,
      "learning_rate": 1.789858617152097e-05,
      "loss": 2.7462,
      "step": 189000
    },
    {
      "epoch": 1.9281332164767746,
      "grad_norm": 6.472926139831543,
      "learning_rate": 1.7864616241702846e-05,
      "loss": 2.7549,
      "step": 189200
    },
    {
      "epoch": 1.9301714122658624,
      "grad_norm": 2.999972105026245,
      "learning_rate": 1.783064631188472e-05,
      "loss": 2.7122,
      "step": 189400
    },
    {
      "epoch": 1.9322096080549498,
      "grad_norm": 3.719993829727173,
      "learning_rate": 1.7796676382066597e-05,
      "loss": 2.7529,
      "step": 189600
    },
    {
      "epoch": 1.9342478038440372,
      "grad_norm": 4.3208208084106445,
      "learning_rate": 1.776270645224847e-05,
      "loss": 2.7879,
      "step": 189800
    },
    {
      "epoch": 1.9362859996331248,
      "grad_norm": 4.324422359466553,
      "learning_rate": 1.7728736522430346e-05,
      "loss": 2.7893,
      "step": 190000
    },
    {
      "epoch": 1.9383241954222123,
      "grad_norm": 3.0217132568359375,
      "learning_rate": 1.7694766592612218e-05,
      "loss": 2.7664,
      "step": 190200
    },
    {
      "epoch": 1.9403623912112997,
      "grad_norm": 3.746307849884033,
      "learning_rate": 1.7660796662794097e-05,
      "loss": 2.7684,
      "step": 190400
    },
    {
      "epoch": 1.942400587000387,
      "grad_norm": 6.222856044769287,
      "learning_rate": 1.762682673297597e-05,
      "loss": 2.7832,
      "step": 190600
    },
    {
      "epoch": 1.944438782789475,
      "grad_norm": 5.7078537940979,
      "learning_rate": 1.7592856803157845e-05,
      "loss": 2.824,
      "step": 190800
    },
    {
      "epoch": 1.9464769785785623,
      "grad_norm": 2.389965534210205,
      "learning_rate": 1.755888687333972e-05,
      "loss": 2.7786,
      "step": 191000
    },
    {
      "epoch": 1.9485151743676496,
      "grad_norm": 7.1305928230285645,
      "learning_rate": 1.7524916943521597e-05,
      "loss": 2.795,
      "step": 191200
    },
    {
      "epoch": 1.9505533701567372,
      "grad_norm": 4.033844947814941,
      "learning_rate": 1.749094701370347e-05,
      "loss": 2.7792,
      "step": 191400
    },
    {
      "epoch": 1.9525915659458248,
      "grad_norm": 3.360320568084717,
      "learning_rate": 1.7456977083885345e-05,
      "loss": 2.7613,
      "step": 191600
    },
    {
      "epoch": 1.9546297617349122,
      "grad_norm": 4.207371711730957,
      "learning_rate": 1.742300715406722e-05,
      "loss": 2.7825,
      "step": 191800
    },
    {
      "epoch": 1.9566679575239998,
      "grad_norm": 2.576615333557129,
      "learning_rate": 1.7389037224249097e-05,
      "loss": 2.7178,
      "step": 192000
    },
    {
      "epoch": 1.9587061533130874,
      "grad_norm": 3.120392084121704,
      "learning_rate": 1.7355067294430972e-05,
      "loss": 2.8257,
      "step": 192200
    },
    {
      "epoch": 1.9607443491021748,
      "grad_norm": 4.077555179595947,
      "learning_rate": 1.7321097364612845e-05,
      "loss": 2.7736,
      "step": 192400
    },
    {
      "epoch": 1.9627825448912621,
      "grad_norm": 3.7132375240325928,
      "learning_rate": 1.728712743479472e-05,
      "loss": 2.7727,
      "step": 192600
    },
    {
      "epoch": 1.9648207406803497,
      "grad_norm": 2.488980770111084,
      "learning_rate": 1.7253157504976593e-05,
      "loss": 2.7817,
      "step": 192800
    },
    {
      "epoch": 1.9668589364694373,
      "grad_norm": 3.7707419395446777,
      "learning_rate": 1.7219187575158472e-05,
      "loss": 2.7568,
      "step": 193000
    },
    {
      "epoch": 1.9688971322585247,
      "grad_norm": 4.848905563354492,
      "learning_rate": 1.7185217645340344e-05,
      "loss": 2.7836,
      "step": 193200
    },
    {
      "epoch": 1.9709353280476123,
      "grad_norm": 6.072458744049072,
      "learning_rate": 1.715124771552222e-05,
      "loss": 2.7372,
      "step": 193400
    },
    {
      "epoch": 1.9729735238366999,
      "grad_norm": 2.499553680419922,
      "learning_rate": 1.7117277785704096e-05,
      "loss": 2.7937,
      "step": 193600
    },
    {
      "epoch": 1.9750117196257873,
      "grad_norm": 3.9558804035186768,
      "learning_rate": 1.708330785588597e-05,
      "loss": 2.7022,
      "step": 193800
    },
    {
      "epoch": 1.9770499154148746,
      "grad_norm": 2.393144369125366,
      "learning_rate": 1.7049337926067847e-05,
      "loss": 2.7757,
      "step": 194000
    },
    {
      "epoch": 1.9790881112039622,
      "grad_norm": 4.830268859863281,
      "learning_rate": 1.701536799624972e-05,
      "loss": 2.7952,
      "step": 194200
    },
    {
      "epoch": 1.9811263069930498,
      "grad_norm": 5.877777099609375,
      "learning_rate": 1.6981398066431596e-05,
      "loss": 2.8275,
      "step": 194400
    },
    {
      "epoch": 1.9831645027821372,
      "grad_norm": 3.673363208770752,
      "learning_rate": 1.694742813661347e-05,
      "loss": 2.8248,
      "step": 194600
    },
    {
      "epoch": 1.9852026985712248,
      "grad_norm": 4.269934177398682,
      "learning_rate": 1.6913458206795347e-05,
      "loss": 2.752,
      "step": 194800
    },
    {
      "epoch": 1.9872408943603124,
      "grad_norm": 2.677738904953003,
      "learning_rate": 1.687948827697722e-05,
      "loss": 2.8245,
      "step": 195000
    },
    {
      "epoch": 1.9892790901493997,
      "grad_norm": 6.536561489105225,
      "learning_rate": 1.6845518347159095e-05,
      "loss": 2.7368,
      "step": 195200
    },
    {
      "epoch": 1.9913172859384871,
      "grad_norm": 3.4246363639831543,
      "learning_rate": 1.6811548417340968e-05,
      "loss": 2.7703,
      "step": 195400
    },
    {
      "epoch": 1.9933554817275747,
      "grad_norm": 2.8880810737609863,
      "learning_rate": 1.6777578487522847e-05,
      "loss": 2.7689,
      "step": 195600
    },
    {
      "epoch": 1.9953936775166623,
      "grad_norm": 3.168529987335205,
      "learning_rate": 1.674360855770472e-05,
      "loss": 2.7473,
      "step": 195800
    },
    {
      "epoch": 1.9974318733057497,
      "grad_norm": 4.678005695343018,
      "learning_rate": 1.6709638627886595e-05,
      "loss": 2.7822,
      "step": 196000
    },
    {
      "epoch": 1.9994700690948373,
      "grad_norm": 3.130925416946411,
      "learning_rate": 1.667566869806847e-05,
      "loss": 2.6895,
      "step": 196200
    },
    {
      "epoch": 2.0,
      "eval_loss": 2.82059907913208,
      "eval_runtime": 194.1501,
      "eval_samples_per_second": 437.759,
      "eval_steps_per_second": 54.721,
      "step": 196252
    },
    {
      "epoch": 2.001508264883925,
      "grad_norm": 3.8379971981048584,
      "learning_rate": 1.6641698768250347e-05,
      "loss": 2.6673,
      "step": 196400
    },
    {
      "epoch": 2.0035464606730122,
      "grad_norm": 3.301880359649658,
      "learning_rate": 1.6607728838432222e-05,
      "loss": 2.693,
      "step": 196600
    },
    {
      "epoch": 2.0055846564620996,
      "grad_norm": 4.671378135681152,
      "learning_rate": 1.6573758908614095e-05,
      "loss": 2.6792,
      "step": 196800
    },
    {
      "epoch": 2.0076228522511874,
      "grad_norm": 3.3082737922668457,
      "learning_rate": 1.653978897879597e-05,
      "loss": 2.6609,
      "step": 197000
    },
    {
      "epoch": 2.009661048040275,
      "grad_norm": 3.0994668006896973,
      "learning_rate": 1.6505819048977846e-05,
      "loss": 2.6577,
      "step": 197200
    },
    {
      "epoch": 2.011699243829362,
      "grad_norm": 4.923946380615234,
      "learning_rate": 1.6471849119159722e-05,
      "loss": 2.6913,
      "step": 197400
    },
    {
      "epoch": 2.0137374396184495,
      "grad_norm": 2.69870924949646,
      "learning_rate": 1.6437879189341594e-05,
      "loss": 2.7,
      "step": 197600
    },
    {
      "epoch": 2.0157756354075373,
      "grad_norm": 3.6223561763763428,
      "learning_rate": 1.640390925952347e-05,
      "loss": 2.665,
      "step": 197800
    },
    {
      "epoch": 2.0178138311966247,
      "grad_norm": 4.54432487487793,
      "learning_rate": 1.6369939329705346e-05,
      "loss": 2.6671,
      "step": 198000
    },
    {
      "epoch": 2.019852026985712,
      "grad_norm": 4.505822658538818,
      "learning_rate": 1.6335969399887222e-05,
      "loss": 2.6896,
      "step": 198200
    },
    {
      "epoch": 2.0218902227748,
      "grad_norm": 3.661574602127075,
      "learning_rate": 1.6301999470069097e-05,
      "loss": 2.6384,
      "step": 198400
    },
    {
      "epoch": 2.0239284185638873,
      "grad_norm": 4.549579620361328,
      "learning_rate": 1.626802954025097e-05,
      "loss": 2.7053,
      "step": 198600
    },
    {
      "epoch": 2.0259666143529746,
      "grad_norm": 3.005967140197754,
      "learning_rate": 1.6234059610432846e-05,
      "loss": 2.669,
      "step": 198800
    },
    {
      "epoch": 2.0280048101420625,
      "grad_norm": 3.6570184230804443,
      "learning_rate": 1.620008968061472e-05,
      "loss": 2.6993,
      "step": 199000
    },
    {
      "epoch": 2.03004300593115,
      "grad_norm": 3.6949973106384277,
      "learning_rate": 1.6166119750796597e-05,
      "loss": 2.6877,
      "step": 199200
    },
    {
      "epoch": 2.032081201720237,
      "grad_norm": 2.1546595096588135,
      "learning_rate": 1.613214982097847e-05,
      "loss": 2.6691,
      "step": 199400
    },
    {
      "epoch": 2.0341193975093246,
      "grad_norm": 2.811012029647827,
      "learning_rate": 1.6098179891160345e-05,
      "loss": 2.6289,
      "step": 199600
    },
    {
      "epoch": 2.0361575932984124,
      "grad_norm": 3.7561023235321045,
      "learning_rate": 1.606420996134222e-05,
      "loss": 2.7255,
      "step": 199800
    },
    {
      "epoch": 2.0381957890874998,
      "grad_norm": 4.171906471252441,
      "learning_rate": 1.6030240031524097e-05,
      "loss": 2.6414,
      "step": 200000
    },
    {
      "epoch": 2.040233984876587,
      "grad_norm": 4.672074317932129,
      "learning_rate": 1.599627010170597e-05,
      "loss": 2.6967,
      "step": 200200
    },
    {
      "epoch": 2.042272180665675,
      "grad_norm": 3.951058864593506,
      "learning_rate": 1.5962300171887845e-05,
      "loss": 2.6797,
      "step": 200400
    },
    {
      "epoch": 2.0443103764547623,
      "grad_norm": 3.0541744232177734,
      "learning_rate": 1.592833024206972e-05,
      "loss": 2.6585,
      "step": 200600
    },
    {
      "epoch": 2.0463485722438497,
      "grad_norm": 3.0811774730682373,
      "learning_rate": 1.5894360312251597e-05,
      "loss": 2.6924,
      "step": 200800
    },
    {
      "epoch": 2.048386768032937,
      "grad_norm": 3.4384944438934326,
      "learning_rate": 1.5860390382433472e-05,
      "loss": 2.6478,
      "step": 201000
    },
    {
      "epoch": 2.050424963822025,
      "grad_norm": 2.934838056564331,
      "learning_rate": 1.5826420452615345e-05,
      "loss": 2.6314,
      "step": 201200
    },
    {
      "epoch": 2.0524631596111123,
      "grad_norm": 3.3195273876190186,
      "learning_rate": 1.579245052279722e-05,
      "loss": 2.682,
      "step": 201400
    },
    {
      "epoch": 2.0545013554001996,
      "grad_norm": 7.013630390167236,
      "learning_rate": 1.5758480592979096e-05,
      "loss": 2.7679,
      "step": 201600
    },
    {
      "epoch": 2.0565395511892874,
      "grad_norm": 5.139005661010742,
      "learning_rate": 1.5724510663160972e-05,
      "loss": 2.6643,
      "step": 201800
    },
    {
      "epoch": 2.058577746978375,
      "grad_norm": 4.205855369567871,
      "learning_rate": 1.5690540733342844e-05,
      "loss": 2.6847,
      "step": 202000
    },
    {
      "epoch": 2.060615942767462,
      "grad_norm": 5.450251579284668,
      "learning_rate": 1.565657080352472e-05,
      "loss": 2.6678,
      "step": 202200
    },
    {
      "epoch": 2.0626541385565496,
      "grad_norm": 3.3940296173095703,
      "learning_rate": 1.5622600873706596e-05,
      "loss": 2.6337,
      "step": 202400
    },
    {
      "epoch": 2.0646923343456374,
      "grad_norm": 2.904116630554199,
      "learning_rate": 1.5588630943888472e-05,
      "loss": 2.6802,
      "step": 202600
    },
    {
      "epoch": 2.0667305301347247,
      "grad_norm": 6.330772399902344,
      "learning_rate": 1.5554661014070344e-05,
      "loss": 2.6318,
      "step": 202800
    },
    {
      "epoch": 2.068768725923812,
      "grad_norm": 4.366308212280273,
      "learning_rate": 1.552069108425222e-05,
      "loss": 2.6328,
      "step": 203000
    },
    {
      "epoch": 2.0708069217129,
      "grad_norm": 4.310024261474609,
      "learning_rate": 1.5486721154434096e-05,
      "loss": 2.773,
      "step": 203200
    },
    {
      "epoch": 2.0728451175019873,
      "grad_norm": 4.178114891052246,
      "learning_rate": 1.545275122461597e-05,
      "loss": 2.7183,
      "step": 203400
    },
    {
      "epoch": 2.0748833132910747,
      "grad_norm": 3.622974157333374,
      "learning_rate": 1.5418781294797847e-05,
      "loss": 2.6557,
      "step": 203600
    },
    {
      "epoch": 2.076921509080162,
      "grad_norm": 3.0211379528045654,
      "learning_rate": 1.538481136497972e-05,
      "loss": 2.7197,
      "step": 203800
    },
    {
      "epoch": 2.07895970486925,
      "grad_norm": 3.812211275100708,
      "learning_rate": 1.5350841435161595e-05,
      "loss": 2.6721,
      "step": 204000
    },
    {
      "epoch": 2.0809979006583372,
      "grad_norm": 2.9468023777008057,
      "learning_rate": 1.531687150534347e-05,
      "loss": 2.6983,
      "step": 204200
    },
    {
      "epoch": 2.0830360964474246,
      "grad_norm": 4.582705020904541,
      "learning_rate": 1.5282901575525347e-05,
      "loss": 2.7773,
      "step": 204400
    },
    {
      "epoch": 2.0850742922365124,
      "grad_norm": 5.877780437469482,
      "learning_rate": 1.5248931645707221e-05,
      "loss": 2.7064,
      "step": 204600
    },
    {
      "epoch": 2.0871124880256,
      "grad_norm": 3.8863065242767334,
      "learning_rate": 1.5214961715889095e-05,
      "loss": 2.6767,
      "step": 204800
    },
    {
      "epoch": 2.089150683814687,
      "grad_norm": 5.061696529388428,
      "learning_rate": 1.5180991786070969e-05,
      "loss": 2.6922,
      "step": 205000
    },
    {
      "epoch": 2.0911888796037745,
      "grad_norm": 6.179668426513672,
      "learning_rate": 1.5147021856252847e-05,
      "loss": 2.706,
      "step": 205200
    },
    {
      "epoch": 2.0932270753928623,
      "grad_norm": 4.245226860046387,
      "learning_rate": 1.511305192643472e-05,
      "loss": 2.7494,
      "step": 205400
    },
    {
      "epoch": 2.0952652711819497,
      "grad_norm": 4.643983364105225,
      "learning_rate": 1.5079081996616595e-05,
      "loss": 2.695,
      "step": 205600
    },
    {
      "epoch": 2.097303466971037,
      "grad_norm": 4.459611415863037,
      "learning_rate": 1.5045112066798469e-05,
      "loss": 2.6658,
      "step": 205800
    },
    {
      "epoch": 2.099341662760125,
      "grad_norm": 3.5540459156036377,
      "learning_rate": 1.5011142136980346e-05,
      "loss": 2.7231,
      "step": 206000
    },
    {
      "epoch": 2.1013798585492123,
      "grad_norm": 8.440991401672363,
      "learning_rate": 1.497717220716222e-05,
      "loss": 2.7253,
      "step": 206200
    },
    {
      "epoch": 2.1034180543382996,
      "grad_norm": 5.144741535186768,
      "learning_rate": 1.4943202277344096e-05,
      "loss": 2.6864,
      "step": 206400
    },
    {
      "epoch": 2.1054562501273875,
      "grad_norm": 3.261610507965088,
      "learning_rate": 1.490923234752597e-05,
      "loss": 2.7129,
      "step": 206600
    },
    {
      "epoch": 2.107494445916475,
      "grad_norm": 3.458369731903076,
      "learning_rate": 1.4875262417707848e-05,
      "loss": 2.6824,
      "step": 206800
    },
    {
      "epoch": 2.109532641705562,
      "grad_norm": 2.5275421142578125,
      "learning_rate": 1.4841292487889722e-05,
      "loss": 2.717,
      "step": 207000
    },
    {
      "epoch": 2.1115708374946496,
      "grad_norm": 2.4282498359680176,
      "learning_rate": 1.4807322558071596e-05,
      "loss": 2.657,
      "step": 207200
    },
    {
      "epoch": 2.1136090332837374,
      "grad_norm": 3.324885129928589,
      "learning_rate": 1.477335262825347e-05,
      "loss": 2.6582,
      "step": 207400
    },
    {
      "epoch": 2.1156472290728248,
      "grad_norm": 3.3453714847564697,
      "learning_rate": 1.4739382698435344e-05,
      "loss": 2.6596,
      "step": 207600
    },
    {
      "epoch": 2.117685424861912,
      "grad_norm": 2.371983051300049,
      "learning_rate": 1.4705412768617221e-05,
      "loss": 2.7253,
      "step": 207800
    },
    {
      "epoch": 2.1197236206509995,
      "grad_norm": 6.066028118133545,
      "learning_rate": 1.4671442838799096e-05,
      "loss": 2.7027,
      "step": 208000
    },
    {
      "epoch": 2.1217618164400873,
      "grad_norm": 2.9849772453308105,
      "learning_rate": 1.463747290898097e-05,
      "loss": 2.6626,
      "step": 208200
    },
    {
      "epoch": 2.1238000122291747,
      "grad_norm": 3.636934995651245,
      "learning_rate": 1.4603502979162845e-05,
      "loss": 2.6756,
      "step": 208400
    },
    {
      "epoch": 2.125838208018262,
      "grad_norm": 2.727328062057495,
      "learning_rate": 1.4569533049344721e-05,
      "loss": 2.7502,
      "step": 208600
    },
    {
      "epoch": 2.12787640380735,
      "grad_norm": 3.8334434032440186,
      "learning_rate": 1.4535563119526597e-05,
      "loss": 2.6454,
      "step": 208800
    },
    {
      "epoch": 2.1299145995964373,
      "grad_norm": 2.649251937866211,
      "learning_rate": 1.4501593189708471e-05,
      "loss": 2.709,
      "step": 209000
    },
    {
      "epoch": 2.1319527953855246,
      "grad_norm": 6.251105785369873,
      "learning_rate": 1.4467623259890345e-05,
      "loss": 2.6808,
      "step": 209200
    },
    {
      "epoch": 2.1339909911746124,
      "grad_norm": 3.653822183609009,
      "learning_rate": 1.4433653330072223e-05,
      "loss": 2.7175,
      "step": 209400
    },
    {
      "epoch": 2.1360291869637,
      "grad_norm": 3.51851487159729,
      "learning_rate": 1.4399683400254097e-05,
      "loss": 2.714,
      "step": 209600
    },
    {
      "epoch": 2.138067382752787,
      "grad_norm": 2.9146931171417236,
      "learning_rate": 1.436571347043597e-05,
      "loss": 2.6484,
      "step": 209800
    },
    {
      "epoch": 2.1401055785418746,
      "grad_norm": 3.6127820014953613,
      "learning_rate": 1.4331743540617845e-05,
      "loss": 2.6725,
      "step": 210000
    },
    {
      "epoch": 2.1421437743309624,
      "grad_norm": 3.438687324523926,
      "learning_rate": 1.4297773610799719e-05,
      "loss": 2.7193,
      "step": 210200
    },
    {
      "epoch": 2.1441819701200497,
      "grad_norm": 4.950996398925781,
      "learning_rate": 1.4263803680981596e-05,
      "loss": 2.6527,
      "step": 210400
    },
    {
      "epoch": 2.146220165909137,
      "grad_norm": 3.0819191932678223,
      "learning_rate": 1.422983375116347e-05,
      "loss": 2.6545,
      "step": 210600
    },
    {
      "epoch": 2.148258361698225,
      "grad_norm": 9.223444938659668,
      "learning_rate": 1.4195863821345346e-05,
      "loss": 2.7789,
      "step": 210800
    },
    {
      "epoch": 2.1502965574873123,
      "grad_norm": 4.50396203994751,
      "learning_rate": 1.416189389152722e-05,
      "loss": 2.7023,
      "step": 211000
    },
    {
      "epoch": 2.1523347532763997,
      "grad_norm": 3.297971248626709,
      "learning_rate": 1.4127923961709098e-05,
      "loss": 2.6473,
      "step": 211200
    },
    {
      "epoch": 2.154372949065487,
      "grad_norm": 9.48082160949707,
      "learning_rate": 1.4093954031890972e-05,
      "loss": 2.682,
      "step": 211400
    },
    {
      "epoch": 2.156411144854575,
      "grad_norm": 5.963827610015869,
      "learning_rate": 1.4059984102072846e-05,
      "loss": 2.6511,
      "step": 211600
    },
    {
      "epoch": 2.1584493406436622,
      "grad_norm": 3.970513343811035,
      "learning_rate": 1.402601417225472e-05,
      "loss": 2.645,
      "step": 211800
    },
    {
      "epoch": 2.1604875364327496,
      "grad_norm": 3.9511218070983887,
      "learning_rate": 1.3992044242436597e-05,
      "loss": 2.6933,
      "step": 212000
    },
    {
      "epoch": 2.1625257322218374,
      "grad_norm": 2.2971737384796143,
      "learning_rate": 1.3958074312618472e-05,
      "loss": 2.6601,
      "step": 212200
    },
    {
      "epoch": 2.164563928010925,
      "grad_norm": 4.402925491333008,
      "learning_rate": 1.3924104382800346e-05,
      "loss": 2.6678,
      "step": 212400
    },
    {
      "epoch": 2.166602123800012,
      "grad_norm": 3.413299798965454,
      "learning_rate": 1.389013445298222e-05,
      "loss": 2.6904,
      "step": 212600
    },
    {
      "epoch": 2.1686403195890995,
      "grad_norm": 4.867201805114746,
      "learning_rate": 1.3856164523164095e-05,
      "loss": 2.76,
      "step": 212800
    },
    {
      "epoch": 2.1706785153781873,
      "grad_norm": 6.872937202453613,
      "learning_rate": 1.3822194593345971e-05,
      "loss": 2.6593,
      "step": 213000
    },
    {
      "epoch": 2.1727167111672747,
      "grad_norm": 2.8814196586608887,
      "learning_rate": 1.3788224663527847e-05,
      "loss": 2.6914,
      "step": 213200
    },
    {
      "epoch": 2.174754906956362,
      "grad_norm": 5.060403823852539,
      "learning_rate": 1.3754254733709721e-05,
      "loss": 2.6932,
      "step": 213400
    },
    {
      "epoch": 2.17679310274545,
      "grad_norm": 2.8237929344177246,
      "learning_rate": 1.3720284803891595e-05,
      "loss": 2.7024,
      "step": 213600
    },
    {
      "epoch": 2.1788312985345373,
      "grad_norm": 2.5435338020324707,
      "learning_rate": 1.3686314874073473e-05,
      "loss": 2.7118,
      "step": 213800
    },
    {
      "epoch": 2.1808694943236246,
      "grad_norm": 10.67947006225586,
      "learning_rate": 1.3652344944255347e-05,
      "loss": 2.7485,
      "step": 214000
    },
    {
      "epoch": 2.182907690112712,
      "grad_norm": 2.516540050506592,
      "learning_rate": 1.361837501443722e-05,
      "loss": 2.6373,
      "step": 214200
    },
    {
      "epoch": 2.1849458859018,
      "grad_norm": 4.148965835571289,
      "learning_rate": 1.3584405084619095e-05,
      "loss": 2.7663,
      "step": 214400
    },
    {
      "epoch": 2.186984081690887,
      "grad_norm": 3.4920012950897217,
      "learning_rate": 1.3550435154800969e-05,
      "loss": 2.6487,
      "step": 214600
    },
    {
      "epoch": 2.1890222774799746,
      "grad_norm": 3.2157676219940186,
      "learning_rate": 1.3516465224982846e-05,
      "loss": 2.6577,
      "step": 214800
    },
    {
      "epoch": 2.1910604732690624,
      "grad_norm": 4.7335734367370605,
      "learning_rate": 1.348249529516472e-05,
      "loss": 2.6631,
      "step": 215000
    },
    {
      "epoch": 2.1930986690581498,
      "grad_norm": 5.867129325866699,
      "learning_rate": 1.3448525365346595e-05,
      "loss": 2.6447,
      "step": 215200
    },
    {
      "epoch": 2.195136864847237,
      "grad_norm": 4.482726097106934,
      "learning_rate": 1.341455543552847e-05,
      "loss": 2.6606,
      "step": 215400
    },
    {
      "epoch": 2.197175060636325,
      "grad_norm": 2.96478009223938,
      "learning_rate": 1.3380585505710346e-05,
      "loss": 2.6743,
      "step": 215600
    },
    {
      "epoch": 2.1992132564254123,
      "grad_norm": 4.849531173706055,
      "learning_rate": 1.3346615575892222e-05,
      "loss": 2.6462,
      "step": 215800
    },
    {
      "epoch": 2.2012514522144997,
      "grad_norm": 3.823145866394043,
      "learning_rate": 1.3312645646074096e-05,
      "loss": 2.7187,
      "step": 216000
    },
    {
      "epoch": 2.203289648003587,
      "grad_norm": 4.018589496612549,
      "learning_rate": 1.327867571625597e-05,
      "loss": 2.6469,
      "step": 216200
    },
    {
      "epoch": 2.205327843792675,
      "grad_norm": 5.640634536743164,
      "learning_rate": 1.3244705786437847e-05,
      "loss": 2.6221,
      "step": 216400
    },
    {
      "epoch": 2.2073660395817623,
      "grad_norm": 2.9716455936431885,
      "learning_rate": 1.3210735856619722e-05,
      "loss": 2.7038,
      "step": 216600
    },
    {
      "epoch": 2.2094042353708496,
      "grad_norm": 3.542628765106201,
      "learning_rate": 1.3176765926801596e-05,
      "loss": 2.6645,
      "step": 216800
    },
    {
      "epoch": 2.211442431159937,
      "grad_norm": 3.847376585006714,
      "learning_rate": 1.314279599698347e-05,
      "loss": 2.7485,
      "step": 217000
    },
    {
      "epoch": 2.213480626949025,
      "grad_norm": 3.9550256729125977,
      "learning_rate": 1.3108826067165344e-05,
      "loss": 2.6871,
      "step": 217200
    },
    {
      "epoch": 2.215518822738112,
      "grad_norm": 13.919891357421875,
      "learning_rate": 1.3074856137347221e-05,
      "loss": 2.6856,
      "step": 217400
    },
    {
      "epoch": 2.2175570185271996,
      "grad_norm": 3.6102914810180664,
      "learning_rate": 1.3040886207529095e-05,
      "loss": 2.6973,
      "step": 217600
    },
    {
      "epoch": 2.2195952143162874,
      "grad_norm": 6.188111305236816,
      "learning_rate": 1.3006916277710971e-05,
      "loss": 2.6704,
      "step": 217800
    },
    {
      "epoch": 2.2216334101053747,
      "grad_norm": 3.3137407302856445,
      "learning_rate": 1.2972946347892845e-05,
      "loss": 2.6657,
      "step": 218000
    },
    {
      "epoch": 2.223671605894462,
      "grad_norm": 10.065842628479004,
      "learning_rate": 1.2938976418074723e-05,
      "loss": 2.6623,
      "step": 218200
    },
    {
      "epoch": 2.22570980168355,
      "grad_norm": 3.5885188579559326,
      "learning_rate": 1.2905006488256597e-05,
      "loss": 2.691,
      "step": 218400
    },
    {
      "epoch": 2.2277479974726373,
      "grad_norm": 5.875724792480469,
      "learning_rate": 1.287103655843847e-05,
      "loss": 2.7195,
      "step": 218600
    },
    {
      "epoch": 2.2297861932617247,
      "grad_norm": 4.920605182647705,
      "learning_rate": 1.2837066628620345e-05,
      "loss": 2.7035,
      "step": 218800
    },
    {
      "epoch": 2.231824389050812,
      "grad_norm": 2.740720748901367,
      "learning_rate": 1.2803096698802222e-05,
      "loss": 2.6795,
      "step": 219000
    },
    {
      "epoch": 2.2338625848399,
      "grad_norm": 2.763418674468994,
      "learning_rate": 1.2769126768984096e-05,
      "loss": 2.6836,
      "step": 219200
    },
    {
      "epoch": 2.2359007806289872,
      "grad_norm": 3.3031251430511475,
      "learning_rate": 1.273515683916597e-05,
      "loss": 2.6974,
      "step": 219400
    },
    {
      "epoch": 2.2379389764180746,
      "grad_norm": 3.832547664642334,
      "learning_rate": 1.2701186909347845e-05,
      "loss": 2.735,
      "step": 219600
    },
    {
      "epoch": 2.2399771722071624,
      "grad_norm": 4.3172783851623535,
      "learning_rate": 1.266721697952972e-05,
      "loss": 2.7096,
      "step": 219800
    },
    {
      "epoch": 2.24201536799625,
      "grad_norm": 5.991254806518555,
      "learning_rate": 1.2633247049711596e-05,
      "loss": 2.6507,
      "step": 220000
    },
    {
      "epoch": 2.244053563785337,
      "grad_norm": 2.3906095027923584,
      "learning_rate": 1.2599277119893472e-05,
      "loss": 2.7267,
      "step": 220200
    },
    {
      "epoch": 2.2460917595744245,
      "grad_norm": 3.545532703399658,
      "learning_rate": 1.2565307190075346e-05,
      "loss": 2.673,
      "step": 220400
    },
    {
      "epoch": 2.2481299553635123,
      "grad_norm": 4.5483903884887695,
      "learning_rate": 1.253133726025722e-05,
      "loss": 2.7084,
      "step": 220600
    },
    {
      "epoch": 2.2501681511525997,
      "grad_norm": 3.443091869354248,
      "learning_rate": 1.2497367330439096e-05,
      "loss": 2.6566,
      "step": 220800
    },
    {
      "epoch": 2.252206346941687,
      "grad_norm": 5.579285621643066,
      "learning_rate": 1.2463397400620972e-05,
      "loss": 2.7478,
      "step": 221000
    },
    {
      "epoch": 2.254244542730775,
      "grad_norm": 4.281118869781494,
      "learning_rate": 1.2429427470802846e-05,
      "loss": 2.665,
      "step": 221200
    },
    {
      "epoch": 2.2562827385198623,
      "grad_norm": 4.594622611999512,
      "learning_rate": 1.2395457540984721e-05,
      "loss": 2.7192,
      "step": 221400
    },
    {
      "epoch": 2.2583209343089496,
      "grad_norm": 5.771159648895264,
      "learning_rate": 1.2361487611166595e-05,
      "loss": 2.7095,
      "step": 221600
    },
    {
      "epoch": 2.2603591300980375,
      "grad_norm": 3.614901542663574,
      "learning_rate": 1.232751768134847e-05,
      "loss": 2.702,
      "step": 221800
    },
    {
      "epoch": 2.262397325887125,
      "grad_norm": 4.228895664215088,
      "learning_rate": 1.2293547751530345e-05,
      "loss": 2.6918,
      "step": 222000
    },
    {
      "epoch": 2.264435521676212,
      "grad_norm": 3.7279717922210693,
      "learning_rate": 1.2259577821712221e-05,
      "loss": 2.6837,
      "step": 222200
    },
    {
      "epoch": 2.2664737174652996,
      "grad_norm": 4.657958030700684,
      "learning_rate": 1.2225607891894097e-05,
      "loss": 2.6653,
      "step": 222400
    },
    {
      "epoch": 2.2685119132543874,
      "grad_norm": 4.932783126831055,
      "learning_rate": 1.2191637962075971e-05,
      "loss": 2.7425,
      "step": 222600
    },
    {
      "epoch": 2.2705501090434748,
      "grad_norm": 3.836574077606201,
      "learning_rate": 1.2157668032257847e-05,
      "loss": 2.6972,
      "step": 222800
    },
    {
      "epoch": 2.272588304832562,
      "grad_norm": 4.712822914123535,
      "learning_rate": 1.212369810243972e-05,
      "loss": 2.7516,
      "step": 223000
    },
    {
      "epoch": 2.2746265006216495,
      "grad_norm": 3.5948069095611572,
      "learning_rate": 1.2089728172621597e-05,
      "loss": 2.6698,
      "step": 223200
    },
    {
      "epoch": 2.2766646964107373,
      "grad_norm": 4.430776596069336,
      "learning_rate": 1.205575824280347e-05,
      "loss": 2.6307,
      "step": 223400
    },
    {
      "epoch": 2.2787028921998247,
      "grad_norm": 4.00706148147583,
      "learning_rate": 1.2021788312985346e-05,
      "loss": 2.6525,
      "step": 223600
    },
    {
      "epoch": 2.280741087988912,
      "grad_norm": 4.072846412658691,
      "learning_rate": 1.198781838316722e-05,
      "loss": 2.6947,
      "step": 223800
    },
    {
      "epoch": 2.282779283778,
      "grad_norm": 3.431596517562866,
      "learning_rate": 1.1953848453349096e-05,
      "loss": 2.6826,
      "step": 224000
    },
    {
      "epoch": 2.2848174795670873,
      "grad_norm": 2.454434633255005,
      "learning_rate": 1.191987852353097e-05,
      "loss": 2.6866,
      "step": 224200
    },
    {
      "epoch": 2.2868556753561746,
      "grad_norm": 4.661773681640625,
      "learning_rate": 1.1885908593712846e-05,
      "loss": 2.7626,
      "step": 224400
    },
    {
      "epoch": 2.2888938711452624,
      "grad_norm": 3.0928735733032227,
      "learning_rate": 1.1851938663894722e-05,
      "loss": 2.6885,
      "step": 224600
    },
    {
      "epoch": 2.29093206693435,
      "grad_norm": 6.472029209136963,
      "learning_rate": 1.1817968734076596e-05,
      "loss": 2.7535,
      "step": 224800
    },
    {
      "epoch": 2.292970262723437,
      "grad_norm": 2.8325977325439453,
      "learning_rate": 1.1783998804258472e-05,
      "loss": 2.6916,
      "step": 225000
    },
    {
      "epoch": 2.2950084585125246,
      "grad_norm": 2.551722288131714,
      "learning_rate": 1.1750028874440346e-05,
      "loss": 2.7189,
      "step": 225200
    },
    {
      "epoch": 2.2970466543016124,
      "grad_norm": 3.790450096130371,
      "learning_rate": 1.1716058944622222e-05,
      "loss": 2.687,
      "step": 225400
    },
    {
      "epoch": 2.2990848500906997,
      "grad_norm": 2.5385758876800537,
      "learning_rate": 1.1682089014804096e-05,
      "loss": 2.719,
      "step": 225600
    },
    {
      "epoch": 2.301123045879787,
      "grad_norm": 5.806585788726807,
      "learning_rate": 1.1648119084985971e-05,
      "loss": 2.6759,
      "step": 225800
    },
    {
      "epoch": 2.3031612416688745,
      "grad_norm": 2.8066086769104004,
      "learning_rate": 1.1614149155167846e-05,
      "loss": 2.6712,
      "step": 226000
    },
    {
      "epoch": 2.3051994374579623,
      "grad_norm": 3.2963480949401855,
      "learning_rate": 1.1580179225349721e-05,
      "loss": 2.6863,
      "step": 226200
    },
    {
      "epoch": 2.3072376332470497,
      "grad_norm": 2.7116122245788574,
      "learning_rate": 1.1546209295531595e-05,
      "loss": 2.747,
      "step": 226400
    },
    {
      "epoch": 2.309275829036137,
      "grad_norm": 2.5851681232452393,
      "learning_rate": 1.1512239365713471e-05,
      "loss": 2.7419,
      "step": 226600
    },
    {
      "epoch": 2.311314024825225,
      "grad_norm": 5.82428503036499,
      "learning_rate": 1.1478269435895345e-05,
      "loss": 2.6334,
      "step": 226800
    },
    {
      "epoch": 2.3133522206143122,
      "grad_norm": 5.841784954071045,
      "learning_rate": 1.1444299506077221e-05,
      "loss": 2.7732,
      "step": 227000
    },
    {
      "epoch": 2.3153904164033996,
      "grad_norm": 4.211447238922119,
      "learning_rate": 1.1410329576259097e-05,
      "loss": 2.6974,
      "step": 227200
    },
    {
      "epoch": 2.3174286121924874,
      "grad_norm": 2.1489014625549316,
      "learning_rate": 1.137635964644097e-05,
      "loss": 2.652,
      "step": 227400
    },
    {
      "epoch": 2.319466807981575,
      "grad_norm": 3.2386138439178467,
      "learning_rate": 1.1342389716622847e-05,
      "loss": 2.6613,
      "step": 227600
    },
    {
      "epoch": 2.321505003770662,
      "grad_norm": 2.795469284057617,
      "learning_rate": 1.130841978680472e-05,
      "loss": 2.74,
      "step": 227800
    },
    {
      "epoch": 2.3235431995597495,
      "grad_norm": 3.4539480209350586,
      "learning_rate": 1.1274449856986596e-05,
      "loss": 2.7173,
      "step": 228000
    },
    {
      "epoch": 2.3255813953488373,
      "grad_norm": 2.9439876079559326,
      "learning_rate": 1.124047992716847e-05,
      "loss": 2.6921,
      "step": 228200
    },
    {
      "epoch": 2.3276195911379247,
      "grad_norm": 4.8276848793029785,
      "learning_rate": 1.1206509997350346e-05,
      "loss": 2.7208,
      "step": 228400
    },
    {
      "epoch": 2.329657786927012,
      "grad_norm": 4.903655052185059,
      "learning_rate": 1.117254006753222e-05,
      "loss": 2.6671,
      "step": 228600
    },
    {
      "epoch": 2.3316959827161,
      "grad_norm": 2.5771596431732178,
      "learning_rate": 1.1138570137714096e-05,
      "loss": 2.7319,
      "step": 228800
    },
    {
      "epoch": 2.3337341785051873,
      "grad_norm": 3.2106380462646484,
      "learning_rate": 1.110460020789597e-05,
      "loss": 2.652,
      "step": 229000
    },
    {
      "epoch": 2.3357723742942746,
      "grad_norm": 3.800682544708252,
      "learning_rate": 1.1070630278077846e-05,
      "loss": 2.7121,
      "step": 229200
    },
    {
      "epoch": 2.337810570083362,
      "grad_norm": 2.860687017440796,
      "learning_rate": 1.1036660348259722e-05,
      "loss": 2.6945,
      "step": 229400
    },
    {
      "epoch": 2.33984876587245,
      "grad_norm": 5.347607612609863,
      "learning_rate": 1.1002690418441596e-05,
      "loss": 2.6652,
      "step": 229600
    },
    {
      "epoch": 2.341886961661537,
      "grad_norm": 3.461029529571533,
      "learning_rate": 1.0968720488623472e-05,
      "loss": 2.6923,
      "step": 229800
    },
    {
      "epoch": 2.3439251574506246,
      "grad_norm": 4.660816669464111,
      "learning_rate": 1.0934750558805346e-05,
      "loss": 2.7087,
      "step": 230000
    },
    {
      "epoch": 2.3459633532397124,
      "grad_norm": 2.9151670932769775,
      "learning_rate": 1.0900780628987221e-05,
      "loss": 2.6651,
      "step": 230200
    },
    {
      "epoch": 2.3480015490287998,
      "grad_norm": 4.409771919250488,
      "learning_rate": 1.0866810699169096e-05,
      "loss": 2.7052,
      "step": 230400
    },
    {
      "epoch": 2.350039744817887,
      "grad_norm": 4.410186767578125,
      "learning_rate": 1.0832840769350971e-05,
      "loss": 2.705,
      "step": 230600
    },
    {
      "epoch": 2.352077940606975,
      "grad_norm": 4.085783958435059,
      "learning_rate": 1.0798870839532845e-05,
      "loss": 2.6562,
      "step": 230800
    },
    {
      "epoch": 2.3541161363960623,
      "grad_norm": 3.80364990234375,
      "learning_rate": 1.0764900909714721e-05,
      "loss": 2.6232,
      "step": 231000
    },
    {
      "epoch": 2.3561543321851497,
      "grad_norm": 7.486030101776123,
      "learning_rate": 1.0730930979896595e-05,
      "loss": 2.6637,
      "step": 231200
    },
    {
      "epoch": 2.358192527974237,
      "grad_norm": 3.6346275806427,
      "learning_rate": 1.0696961050078471e-05,
      "loss": 2.6769,
      "step": 231400
    },
    {
      "epoch": 2.360230723763325,
      "grad_norm": 3.5512301921844482,
      "learning_rate": 1.0662991120260347e-05,
      "loss": 2.6874,
      "step": 231600
    },
    {
      "epoch": 2.3622689195524122,
      "grad_norm": 4.479690074920654,
      "learning_rate": 1.0629021190442221e-05,
      "loss": 2.7002,
      "step": 231800
    },
    {
      "epoch": 2.3643071153414996,
      "grad_norm": 2.4652206897735596,
      "learning_rate": 1.0595051260624097e-05,
      "loss": 2.7226,
      "step": 232000
    },
    {
      "epoch": 2.366345311130587,
      "grad_norm": 5.858077526092529,
      "learning_rate": 1.056108133080597e-05,
      "loss": 2.6529,
      "step": 232200
    },
    {
      "epoch": 2.368383506919675,
      "grad_norm": 4.287156581878662,
      "learning_rate": 1.0527111400987846e-05,
      "loss": 2.743,
      "step": 232400
    },
    {
      "epoch": 2.370421702708762,
      "grad_norm": 4.899918556213379,
      "learning_rate": 1.049314147116972e-05,
      "loss": 2.652,
      "step": 232600
    },
    {
      "epoch": 2.3724598984978496,
      "grad_norm": 6.776330471038818,
      "learning_rate": 1.0459171541351596e-05,
      "loss": 2.7241,
      "step": 232800
    },
    {
      "epoch": 2.3744980942869374,
      "grad_norm": 4.849841594696045,
      "learning_rate": 1.042520161153347e-05,
      "loss": 2.7444,
      "step": 233000
    },
    {
      "epoch": 2.3765362900760247,
      "grad_norm": 3.0701420307159424,
      "learning_rate": 1.0391231681715346e-05,
      "loss": 2.7013,
      "step": 233200
    },
    {
      "epoch": 2.378574485865112,
      "grad_norm": 4.059706211090088,
      "learning_rate": 1.035726175189722e-05,
      "loss": 2.7803,
      "step": 233400
    },
    {
      "epoch": 2.3806126816542,
      "grad_norm": 2.9174468517303467,
      "learning_rate": 1.0323291822079096e-05,
      "loss": 2.7427,
      "step": 233600
    },
    {
      "epoch": 2.3826508774432873,
      "grad_norm": 3.100531578063965,
      "learning_rate": 1.0289321892260972e-05,
      "loss": 2.646,
      "step": 233800
    },
    {
      "epoch": 2.3846890732323747,
      "grad_norm": 4.216711044311523,
      "learning_rate": 1.0255351962442846e-05,
      "loss": 2.6419,
      "step": 234000
    },
    {
      "epoch": 2.386727269021462,
      "grad_norm": 3.54036808013916,
      "learning_rate": 1.0221382032624722e-05,
      "loss": 2.6832,
      "step": 234200
    },
    {
      "epoch": 2.38876546481055,
      "grad_norm": 3.084881067276001,
      "learning_rate": 1.0187412102806596e-05,
      "loss": 2.6521,
      "step": 234400
    },
    {
      "epoch": 2.3908036605996372,
      "grad_norm": 6.646629810333252,
      "learning_rate": 1.0153442172988472e-05,
      "loss": 2.6572,
      "step": 234600
    },
    {
      "epoch": 2.3928418563887246,
      "grad_norm": 4.070645332336426,
      "learning_rate": 1.0119472243170346e-05,
      "loss": 2.6496,
      "step": 234800
    },
    {
      "epoch": 2.394880052177812,
      "grad_norm": 5.0416998863220215,
      "learning_rate": 1.0085502313352221e-05,
      "loss": 2.6948,
      "step": 235000
    },
    {
      "epoch": 2.3969182479669,
      "grad_norm": 6.215917587280273,
      "learning_rate": 1.0051532383534095e-05,
      "loss": 2.6666,
      "step": 235200
    },
    {
      "epoch": 2.398956443755987,
      "grad_norm": 2.865978956222534,
      "learning_rate": 1.0017562453715971e-05,
      "loss": 2.7005,
      "step": 235400
    },
    {
      "epoch": 2.4009946395450745,
      "grad_norm": 3.825308084487915,
      "learning_rate": 9.983592523897845e-06,
      "loss": 2.5974,
      "step": 235600
    },
    {
      "epoch": 2.4030328353341623,
      "grad_norm": 5.093776702880859,
      "learning_rate": 9.949622594079721e-06,
      "loss": 2.6547,
      "step": 235800
    },
    {
      "epoch": 2.4050710311232497,
      "grad_norm": 4.282793998718262,
      "learning_rate": 9.915652664261597e-06,
      "loss": 2.7366,
      "step": 236000
    },
    {
      "epoch": 2.407109226912337,
      "grad_norm": 3.2649905681610107,
      "learning_rate": 9.881682734443473e-06,
      "loss": 2.643,
      "step": 236200
    },
    {
      "epoch": 2.409147422701425,
      "grad_norm": 4.5259623527526855,
      "learning_rate": 9.847712804625347e-06,
      "loss": 2.7414,
      "step": 236400
    },
    {
      "epoch": 2.4111856184905123,
      "grad_norm": 4.7046427726745605,
      "learning_rate": 9.81374287480722e-06,
      "loss": 2.685,
      "step": 236600
    },
    {
      "epoch": 2.4132238142795996,
      "grad_norm": 4.950325012207031,
      "learning_rate": 9.779772944989097e-06,
      "loss": 2.7261,
      "step": 236800
    },
    {
      "epoch": 2.415262010068687,
      "grad_norm": 4.922814846038818,
      "learning_rate": 9.74580301517097e-06,
      "loss": 2.734,
      "step": 237000
    },
    {
      "epoch": 2.417300205857775,
      "grad_norm": 3.3788468837738037,
      "learning_rate": 9.711833085352846e-06,
      "loss": 2.7107,
      "step": 237200
    },
    {
      "epoch": 2.419338401646862,
      "grad_norm": 2.849576950073242,
      "learning_rate": 9.67786315553472e-06,
      "loss": 2.7158,
      "step": 237400
    },
    {
      "epoch": 2.4213765974359496,
      "grad_norm": 3.1030189990997314,
      "learning_rate": 9.643893225716596e-06,
      "loss": 2.6597,
      "step": 237600
    },
    {
      "epoch": 2.4234147932250374,
      "grad_norm": 4.048820495605469,
      "learning_rate": 9.60992329589847e-06,
      "loss": 2.6623,
      "step": 237800
    },
    {
      "epoch": 2.4254529890141248,
      "grad_norm": 3.512970209121704,
      "learning_rate": 9.575953366080346e-06,
      "loss": 2.6179,
      "step": 238000
    },
    {
      "epoch": 2.427491184803212,
      "grad_norm": 3.8551173210144043,
      "learning_rate": 9.54198343626222e-06,
      "loss": 2.6999,
      "step": 238200
    },
    {
      "epoch": 2.4295293805922995,
      "grad_norm": 3.791720151901245,
      "learning_rate": 9.508013506444096e-06,
      "loss": 2.6852,
      "step": 238400
    },
    {
      "epoch": 2.4315675763813873,
      "grad_norm": 2.7943899631500244,
      "learning_rate": 9.474043576625972e-06,
      "loss": 2.7411,
      "step": 238600
    },
    {
      "epoch": 2.4336057721704747,
      "grad_norm": 5.656954765319824,
      "learning_rate": 9.440073646807847e-06,
      "loss": 2.6713,
      "step": 238800
    },
    {
      "epoch": 2.435643967959562,
      "grad_norm": 3.5430848598480225,
      "learning_rate": 9.406103716989722e-06,
      "loss": 2.6403,
      "step": 239000
    },
    {
      "epoch": 2.43768216374865,
      "grad_norm": 5.0321502685546875,
      "learning_rate": 9.372133787171596e-06,
      "loss": 2.6694,
      "step": 239200
    },
    {
      "epoch": 2.4397203595377372,
      "grad_norm": 2.461482048034668,
      "learning_rate": 9.338163857353471e-06,
      "loss": 2.6748,
      "step": 239400
    },
    {
      "epoch": 2.4417585553268246,
      "grad_norm": 6.576355457305908,
      "learning_rate": 9.304193927535345e-06,
      "loss": 2.6668,
      "step": 239600
    },
    {
      "epoch": 2.4437967511159124,
      "grad_norm": 2.9547178745269775,
      "learning_rate": 9.270223997717221e-06,
      "loss": 2.6928,
      "step": 239800
    },
    {
      "epoch": 2.445834946905,
      "grad_norm": 3.505669116973877,
      "learning_rate": 9.236254067899095e-06,
      "loss": 2.6997,
      "step": 240000
    },
    {
      "epoch": 2.447873142694087,
      "grad_norm": 4.124243259429932,
      "learning_rate": 9.202284138080971e-06,
      "loss": 2.7302,
      "step": 240200
    },
    {
      "epoch": 2.4499113384831745,
      "grad_norm": 2.810274839401245,
      "learning_rate": 9.168314208262845e-06,
      "loss": 2.6741,
      "step": 240400
    },
    {
      "epoch": 2.4519495342722624,
      "grad_norm": 4.373473644256592,
      "learning_rate": 9.134344278444721e-06,
      "loss": 2.6419,
      "step": 240600
    },
    {
      "epoch": 2.4539877300613497,
      "grad_norm": 5.021244525909424,
      "learning_rate": 9.100374348626597e-06,
      "loss": 2.7265,
      "step": 240800
    },
    {
      "epoch": 2.456025925850437,
      "grad_norm": 3.602677345275879,
      "learning_rate": 9.066404418808472e-06,
      "loss": 2.7185,
      "step": 241000
    },
    {
      "epoch": 2.4580641216395245,
      "grad_norm": 2.988499879837036,
      "learning_rate": 9.032434488990347e-06,
      "loss": 2.7297,
      "step": 241200
    },
    {
      "epoch": 2.4601023174286123,
      "grad_norm": 3.261666774749756,
      "learning_rate": 8.99846455917222e-06,
      "loss": 2.6821,
      "step": 241400
    },
    {
      "epoch": 2.4621405132176997,
      "grad_norm": 2.828979969024658,
      "learning_rate": 8.964494629354096e-06,
      "loss": 2.6716,
      "step": 241600
    },
    {
      "epoch": 2.464178709006787,
      "grad_norm": 4.827070713043213,
      "learning_rate": 8.93052469953597e-06,
      "loss": 2.6376,
      "step": 241800
    },
    {
      "epoch": 2.466216904795875,
      "grad_norm": 7.376821994781494,
      "learning_rate": 8.896554769717846e-06,
      "loss": 2.6704,
      "step": 242000
    },
    {
      "epoch": 2.4682551005849622,
      "grad_norm": 3.8158488273620605,
      "learning_rate": 8.86258483989972e-06,
      "loss": 2.6552,
      "step": 242200
    },
    {
      "epoch": 2.4702932963740496,
      "grad_norm": 5.901541709899902,
      "learning_rate": 8.828614910081596e-06,
      "loss": 2.7372,
      "step": 242400
    },
    {
      "epoch": 2.4723314921631374,
      "grad_norm": 5.1134233474731445,
      "learning_rate": 8.79464498026347e-06,
      "loss": 2.6568,
      "step": 242600
    },
    {
      "epoch": 2.474369687952225,
      "grad_norm": 3.310120105743408,
      "learning_rate": 8.760675050445346e-06,
      "loss": 2.6979,
      "step": 242800
    },
    {
      "epoch": 2.476407883741312,
      "grad_norm": 4.891802787780762,
      "learning_rate": 8.726705120627222e-06,
      "loss": 2.713,
      "step": 243000
    },
    {
      "epoch": 2.4784460795303995,
      "grad_norm": 3.283158540725708,
      "learning_rate": 8.692735190809097e-06,
      "loss": 2.6835,
      "step": 243200
    },
    {
      "epoch": 2.4804842753194873,
      "grad_norm": 7.550588130950928,
      "learning_rate": 8.658765260990972e-06,
      "loss": 2.6533,
      "step": 243400
    },
    {
      "epoch": 2.4825224711085747,
      "grad_norm": 2.8040452003479004,
      "learning_rate": 8.624795331172847e-06,
      "loss": 2.6467,
      "step": 243600
    },
    {
      "epoch": 2.484560666897662,
      "grad_norm": 4.597075939178467,
      "learning_rate": 8.590825401354721e-06,
      "loss": 2.7254,
      "step": 243800
    },
    {
      "epoch": 2.4865988626867495,
      "grad_norm": 3.4596216678619385,
      "learning_rate": 8.556855471536595e-06,
      "loss": 2.7044,
      "step": 244000
    },
    {
      "epoch": 2.4886370584758373,
      "grad_norm": 5.652083873748779,
      "learning_rate": 8.522885541718471e-06,
      "loss": 2.6824,
      "step": 244200
    },
    {
      "epoch": 2.4906752542649246,
      "grad_norm": 4.218283176422119,
      "learning_rate": 8.488915611900345e-06,
      "loss": 2.7382,
      "step": 244400
    },
    {
      "epoch": 2.492713450054012,
      "grad_norm": 3.5949554443359375,
      "learning_rate": 8.454945682082221e-06,
      "loss": 2.6866,
      "step": 244600
    },
    {
      "epoch": 2.4947516458431,
      "grad_norm": 10.493470191955566,
      "learning_rate": 8.420975752264095e-06,
      "loss": 2.6814,
      "step": 244800
    },
    {
      "epoch": 2.496789841632187,
      "grad_norm": 4.665637493133545,
      "learning_rate": 8.387005822445971e-06,
      "loss": 2.72,
      "step": 245000
    },
    {
      "epoch": 2.4988280374212746,
      "grad_norm": 5.094621658325195,
      "learning_rate": 8.353035892627847e-06,
      "loss": 2.7451,
      "step": 245200
    },
    {
      "epoch": 2.5008662332103624,
      "grad_norm": 4.83525276184082,
      "learning_rate": 8.319065962809723e-06,
      "loss": 2.7339,
      "step": 245400
    },
    {
      "epoch": 2.5029044289994498,
      "grad_norm": 5.4453558921813965,
      "learning_rate": 8.285096032991597e-06,
      "loss": 2.6896,
      "step": 245600
    },
    {
      "epoch": 2.504942624788537,
      "grad_norm": 3.3265318870544434,
      "learning_rate": 8.251126103173472e-06,
      "loss": 2.7528,
      "step": 245800
    },
    {
      "epoch": 2.506980820577625,
      "grad_norm": 4.652145862579346,
      "learning_rate": 8.217156173355346e-06,
      "loss": 2.7424,
      "step": 246000
    },
    {
      "epoch": 2.5090190163667123,
      "grad_norm": 4.766633033752441,
      "learning_rate": 8.18318624353722e-06,
      "loss": 2.6612,
      "step": 246200
    },
    {
      "epoch": 2.5110572121557997,
      "grad_norm": 3.836777687072754,
      "learning_rate": 8.149216313719096e-06,
      "loss": 2.6699,
      "step": 246400
    },
    {
      "epoch": 2.513095407944887,
      "grad_norm": 4.241693019866943,
      "learning_rate": 8.11524638390097e-06,
      "loss": 2.7013,
      "step": 246600
    },
    {
      "epoch": 2.5151336037339744,
      "grad_norm": 3.7338035106658936,
      "learning_rate": 8.081276454082846e-06,
      "loss": 2.6649,
      "step": 246800
    },
    {
      "epoch": 2.5171717995230622,
      "grad_norm": 4.305300235748291,
      "learning_rate": 8.04730652426472e-06,
      "loss": 2.7043,
      "step": 247000
    },
    {
      "epoch": 2.5192099953121496,
      "grad_norm": 2.6470024585723877,
      "learning_rate": 8.013336594446596e-06,
      "loss": 2.6901,
      "step": 247200
    },
    {
      "epoch": 2.521248191101237,
      "grad_norm": 4.439001560211182,
      "learning_rate": 7.979366664628472e-06,
      "loss": 2.6779,
      "step": 247400
    },
    {
      "epoch": 2.523286386890325,
      "grad_norm": 2.2817020416259766,
      "learning_rate": 7.945396734810348e-06,
      "loss": 2.7037,
      "step": 247600
    },
    {
      "epoch": 2.525324582679412,
      "grad_norm": 5.641472816467285,
      "learning_rate": 7.911426804992222e-06,
      "loss": 2.6977,
      "step": 247800
    },
    {
      "epoch": 2.5273627784684995,
      "grad_norm": 4.55875301361084,
      "learning_rate": 7.877456875174097e-06,
      "loss": 2.6531,
      "step": 248000
    },
    {
      "epoch": 2.5294009742575874,
      "grad_norm": 3.207174301147461,
      "learning_rate": 7.843486945355971e-06,
      "loss": 2.6775,
      "step": 248200
    },
    {
      "epoch": 2.5314391700466747,
      "grad_norm": 13.064462661743164,
      "learning_rate": 7.809517015537847e-06,
      "loss": 2.6928,
      "step": 248400
    },
    {
      "epoch": 2.533477365835762,
      "grad_norm": 3.838738441467285,
      "learning_rate": 7.775547085719721e-06,
      "loss": 2.7207,
      "step": 248600
    },
    {
      "epoch": 2.53551556162485,
      "grad_norm": 4.414685249328613,
      "learning_rate": 7.741577155901595e-06,
      "loss": 2.7128,
      "step": 248800
    },
    {
      "epoch": 2.5375537574139373,
      "grad_norm": 2.783580780029297,
      "learning_rate": 7.707607226083471e-06,
      "loss": 2.7183,
      "step": 249000
    },
    {
      "epoch": 2.5395919532030247,
      "grad_norm": 5.444891929626465,
      "learning_rate": 7.673637296265345e-06,
      "loss": 2.7323,
      "step": 249200
    },
    {
      "epoch": 2.541630148992112,
      "grad_norm": 2.6180403232574463,
      "learning_rate": 7.639667366447221e-06,
      "loss": 2.6817,
      "step": 249400
    },
    {
      "epoch": 2.5436683447811994,
      "grad_norm": 3.7441635131835938,
      "learning_rate": 7.605697436629096e-06,
      "loss": 2.6718,
      "step": 249600
    },
    {
      "epoch": 2.5457065405702872,
      "grad_norm": 4.556472301483154,
      "learning_rate": 7.571727506810972e-06,
      "loss": 2.709,
      "step": 249800
    },
    {
      "epoch": 2.5477447363593746,
      "grad_norm": 3.0378899574279785,
      "learning_rate": 7.537757576992846e-06,
      "loss": 2.6725,
      "step": 250000
    },
    {
      "epoch": 2.549782932148462,
      "grad_norm": 3.3914973735809326,
      "learning_rate": 7.5037876471747215e-06,
      "loss": 2.6877,
      "step": 250200
    },
    {
      "epoch": 2.55182112793755,
      "grad_norm": 4.151961326599121,
      "learning_rate": 7.4698177173565965e-06,
      "loss": 2.6759,
      "step": 250400
    },
    {
      "epoch": 2.553859323726637,
      "grad_norm": 3.9725966453552246,
      "learning_rate": 7.435847787538472e-06,
      "loss": 2.6752,
      "step": 250600
    },
    {
      "epoch": 2.5558975195157245,
      "grad_norm": 6.104557514190674,
      "learning_rate": 7.401877857720346e-06,
      "loss": 2.6992,
      "step": 250800
    },
    {
      "epoch": 2.5579357153048123,
      "grad_norm": 3.617025852203369,
      "learning_rate": 7.36790792790222e-06,
      "loss": 2.7283,
      "step": 251000
    },
    {
      "epoch": 2.5599739110938997,
      "grad_norm": 9.21094036102295,
      "learning_rate": 7.333937998084096e-06,
      "loss": 2.7763,
      "step": 251200
    },
    {
      "epoch": 2.562012106882987,
      "grad_norm": 3.6110782623291016,
      "learning_rate": 7.299968068265971e-06,
      "loss": 2.6406,
      "step": 251400
    },
    {
      "epoch": 2.564050302672075,
      "grad_norm": 3.698037624359131,
      "learning_rate": 7.265998138447847e-06,
      "loss": 2.6536,
      "step": 251600
    },
    {
      "epoch": 2.5660884984611623,
      "grad_norm": 4.846705436706543,
      "learning_rate": 7.232028208629721e-06,
      "loss": 2.7547,
      "step": 251800
    },
    {
      "epoch": 2.5681266942502496,
      "grad_norm": 3.8127126693725586,
      "learning_rate": 7.198058278811597e-06,
      "loss": 2.6657,
      "step": 252000
    },
    {
      "epoch": 2.570164890039337,
      "grad_norm": 3.2629427909851074,
      "learning_rate": 7.164088348993471e-06,
      "loss": 2.7502,
      "step": 252200
    },
    {
      "epoch": 2.572203085828425,
      "grad_norm": 4.8715901374816895,
      "learning_rate": 7.1301184191753466e-06,
      "loss": 2.6811,
      "step": 252400
    },
    {
      "epoch": 2.574241281617512,
      "grad_norm": 3.6943886280059814,
      "learning_rate": 7.0961484893572215e-06,
      "loss": 2.7133,
      "step": 252600
    },
    {
      "epoch": 2.5762794774065996,
      "grad_norm": 7.552579402923584,
      "learning_rate": 7.062178559539097e-06,
      "loss": 2.7167,
      "step": 252800
    },
    {
      "epoch": 2.578317673195687,
      "grad_norm": 3.0157318115234375,
      "learning_rate": 7.028208629720971e-06,
      "loss": 2.6898,
      "step": 253000
    },
    {
      "epoch": 2.5803558689847748,
      "grad_norm": 5.969918251037598,
      "learning_rate": 6.994238699902847e-06,
      "loss": 2.6954,
      "step": 253200
    },
    {
      "epoch": 2.582394064773862,
      "grad_norm": 2.9217031002044678,
      "learning_rate": 6.960268770084721e-06,
      "loss": 2.7339,
      "step": 253400
    },
    {
      "epoch": 2.5844322605629495,
      "grad_norm": 3.7622528076171875,
      "learning_rate": 6.926298840266596e-06,
      "loss": 2.6831,
      "step": 253600
    },
    {
      "epoch": 2.5864704563520373,
      "grad_norm": 3.5862510204315186,
      "learning_rate": 6.892328910448472e-06,
      "loss": 2.6465,
      "step": 253800
    },
    {
      "epoch": 2.5885086521411247,
      "grad_norm": 4.396796703338623,
      "learning_rate": 6.858358980630346e-06,
      "loss": 2.7309,
      "step": 254000
    },
    {
      "epoch": 2.590546847930212,
      "grad_norm": 3.3332808017730713,
      "learning_rate": 6.824389050812222e-06,
      "loss": 2.7199,
      "step": 254200
    },
    {
      "epoch": 2.5925850437193,
      "grad_norm": 4.107455730438232,
      "learning_rate": 6.790419120994096e-06,
      "loss": 2.7041,
      "step": 254400
    },
    {
      "epoch": 2.5946232395083872,
      "grad_norm": 4.000187397003174,
      "learning_rate": 6.7564491911759716e-06,
      "loss": 2.6654,
      "step": 254600
    },
    {
      "epoch": 2.5966614352974746,
      "grad_norm": 2.685563802719116,
      "learning_rate": 6.722479261357846e-06,
      "loss": 2.7283,
      "step": 254800
    },
    {
      "epoch": 2.5986996310865624,
      "grad_norm": 4.330329418182373,
      "learning_rate": 6.688509331539721e-06,
      "loss": 2.6617,
      "step": 255000
    },
    {
      "epoch": 2.60073782687565,
      "grad_norm": 5.306848049163818,
      "learning_rate": 6.654539401721596e-06,
      "loss": 2.7488,
      "step": 255200
    },
    {
      "epoch": 2.602776022664737,
      "grad_norm": 4.207979679107666,
      "learning_rate": 6.620569471903472e-06,
      "loss": 2.7032,
      "step": 255400
    },
    {
      "epoch": 2.6048142184538245,
      "grad_norm": 5.2682013511657715,
      "learning_rate": 6.586599542085346e-06,
      "loss": 2.682,
      "step": 255600
    },
    {
      "epoch": 2.606852414242912,
      "grad_norm": 4.616995811462402,
      "learning_rate": 6.552629612267222e-06,
      "loss": 2.6442,
      "step": 255800
    },
    {
      "epoch": 2.6088906100319997,
      "grad_norm": 2.8682940006256104,
      "learning_rate": 6.518659682449096e-06,
      "loss": 2.7368,
      "step": 256000
    },
    {
      "epoch": 2.610928805821087,
      "grad_norm": 3.525573253631592,
      "learning_rate": 6.484689752630971e-06,
      "loss": 2.7316,
      "step": 256200
    },
    {
      "epoch": 2.6129670016101745,
      "grad_norm": 3.6484174728393555,
      "learning_rate": 6.450719822812847e-06,
      "loss": 2.711,
      "step": 256400
    },
    {
      "epoch": 2.6150051973992623,
      "grad_norm": 3.028968095779419,
      "learning_rate": 6.416749892994721e-06,
      "loss": 2.7118,
      "step": 256600
    },
    {
      "epoch": 2.6170433931883497,
      "grad_norm": 11.177583694458008,
      "learning_rate": 6.382779963176597e-06,
      "loss": 2.6864,
      "step": 256800
    },
    {
      "epoch": 2.619081588977437,
      "grad_norm": 4.918911457061768,
      "learning_rate": 6.348810033358471e-06,
      "loss": 2.7018,
      "step": 257000
    },
    {
      "epoch": 2.621119784766525,
      "grad_norm": 3.1726412773132324,
      "learning_rate": 6.3148401035403464e-06,
      "loss": 2.6817,
      "step": 257200
    },
    {
      "epoch": 2.6231579805556122,
      "grad_norm": 5.440394878387451,
      "learning_rate": 6.280870173722221e-06,
      "loss": 2.6847,
      "step": 257400
    },
    {
      "epoch": 2.6251961763446996,
      "grad_norm": 3.525507688522339,
      "learning_rate": 6.246900243904096e-06,
      "loss": 2.6634,
      "step": 257600
    },
    {
      "epoch": 2.6272343721337874,
      "grad_norm": 3.767277956008911,
      "learning_rate": 6.212930314085971e-06,
      "loss": 2.6594,
      "step": 257800
    },
    {
      "epoch": 2.629272567922875,
      "grad_norm": 3.351010322570801,
      "learning_rate": 6.178960384267846e-06,
      "loss": 2.6524,
      "step": 258000
    },
    {
      "epoch": 2.631310763711962,
      "grad_norm": 5.617184162139893,
      "learning_rate": 6.144990454449721e-06,
      "loss": 2.6624,
      "step": 258200
    },
    {
      "epoch": 2.6333489595010495,
      "grad_norm": 2.3863108158111572,
      "learning_rate": 6.111020524631597e-06,
      "loss": 2.6474,
      "step": 258400
    },
    {
      "epoch": 2.635387155290137,
      "grad_norm": 2.899663209915161,
      "learning_rate": 6.077050594813472e-06,
      "loss": 2.6522,
      "step": 258600
    },
    {
      "epoch": 2.6374253510792247,
      "grad_norm": 4.651034355163574,
      "learning_rate": 6.043080664995347e-06,
      "loss": 2.6659,
      "step": 258800
    },
    {
      "epoch": 2.639463546868312,
      "grad_norm": 3.729179859161377,
      "learning_rate": 6.009110735177222e-06,
      "loss": 2.6768,
      "step": 259000
    },
    {
      "epoch": 2.6415017426573995,
      "grad_norm": 5.809887886047363,
      "learning_rate": 5.9751408053590965e-06,
      "loss": 2.6793,
      "step": 259200
    },
    {
      "epoch": 2.6435399384464873,
      "grad_norm": 6.661855220794678,
      "learning_rate": 5.9411708755409714e-06,
      "loss": 2.6556,
      "step": 259400
    },
    {
      "epoch": 2.6455781342355746,
      "grad_norm": 7.045406341552734,
      "learning_rate": 5.907200945722846e-06,
      "loss": 2.6662,
      "step": 259600
    },
    {
      "epoch": 2.647616330024662,
      "grad_norm": 3.2809348106384277,
      "learning_rate": 5.873231015904721e-06,
      "loss": 2.7153,
      "step": 259800
    },
    {
      "epoch": 2.64965452581375,
      "grad_norm": 3.9146995544433594,
      "learning_rate": 5.839261086086596e-06,
      "loss": 2.7397,
      "step": 260000
    },
    {
      "epoch": 2.651692721602837,
      "grad_norm": 6.143965244293213,
      "learning_rate": 5.805291156268471e-06,
      "loss": 2.7598,
      "step": 260200
    },
    {
      "epoch": 2.6537309173919246,
      "grad_norm": 4.614848613739014,
      "learning_rate": 5.771321226450346e-06,
      "loss": 2.6906,
      "step": 260400
    },
    {
      "epoch": 2.6557691131810124,
      "grad_norm": 8.088373184204102,
      "learning_rate": 5.737351296632221e-06,
      "loss": 2.6804,
      "step": 260600
    },
    {
      "epoch": 2.6578073089700998,
      "grad_norm": 3.8144655227661133,
      "learning_rate": 5.703381366814097e-06,
      "loss": 2.6878,
      "step": 260800
    },
    {
      "epoch": 2.659845504759187,
      "grad_norm": 3.381150245666504,
      "learning_rate": 5.669411436995972e-06,
      "loss": 2.7246,
      "step": 261000
    },
    {
      "epoch": 2.661883700548275,
      "grad_norm": 6.131280422210693,
      "learning_rate": 5.635441507177847e-06,
      "loss": 2.7231,
      "step": 261200
    },
    {
      "epoch": 2.6639218963373623,
      "grad_norm": 3.5740559101104736,
      "learning_rate": 5.6014715773597215e-06,
      "loss": 2.6725,
      "step": 261400
    },
    {
      "epoch": 2.6659600921264497,
      "grad_norm": 3.3035714626312256,
      "learning_rate": 5.5675016475415965e-06,
      "loss": 2.6531,
      "step": 261600
    },
    {
      "epoch": 2.667998287915537,
      "grad_norm": 5.20297908782959,
      "learning_rate": 5.533531717723471e-06,
      "loss": 2.667,
      "step": 261800
    },
    {
      "epoch": 2.6700364837046244,
      "grad_norm": 3.3951144218444824,
      "learning_rate": 5.499561787905346e-06,
      "loss": 2.687,
      "step": 262000
    },
    {
      "epoch": 2.6720746794937122,
      "grad_norm": 4.027554035186768,
      "learning_rate": 5.465591858087221e-06,
      "loss": 2.7653,
      "step": 262200
    },
    {
      "epoch": 2.6741128752827996,
      "grad_norm": 6.167198657989502,
      "learning_rate": 5.431621928269096e-06,
      "loss": 2.6981,
      "step": 262400
    },
    {
      "epoch": 2.676151071071887,
      "grad_norm": 4.830431938171387,
      "learning_rate": 5.397651998450971e-06,
      "loss": 2.6674,
      "step": 262600
    },
    {
      "epoch": 2.678189266860975,
      "grad_norm": 4.062410831451416,
      "learning_rate": 5.363682068632846e-06,
      "loss": 2.68,
      "step": 262800
    },
    {
      "epoch": 2.680227462650062,
      "grad_norm": 4.723470687866211,
      "learning_rate": 5.329712138814722e-06,
      "loss": 2.6681,
      "step": 263000
    },
    {
      "epoch": 2.6822656584391495,
      "grad_norm": 5.490209579467773,
      "learning_rate": 5.295742208996597e-06,
      "loss": 2.7203,
      "step": 263200
    },
    {
      "epoch": 2.6843038542282374,
      "grad_norm": 4.768346309661865,
      "learning_rate": 5.261772279178472e-06,
      "loss": 2.6883,
      "step": 263400
    },
    {
      "epoch": 2.6863420500173247,
      "grad_norm": 4.4481520652771,
      "learning_rate": 5.2278023493603466e-06,
      "loss": 2.608,
      "step": 263600
    },
    {
      "epoch": 2.688380245806412,
      "grad_norm": 2.470195770263672,
      "learning_rate": 5.1938324195422215e-06,
      "loss": 2.6615,
      "step": 263800
    },
    {
      "epoch": 2.6904184415955,
      "grad_norm": 5.665065288543701,
      "learning_rate": 5.159862489724096e-06,
      "loss": 2.7493,
      "step": 264000
    },
    {
      "epoch": 2.6924566373845873,
      "grad_norm": 3.9922375679016113,
      "learning_rate": 5.125892559905972e-06,
      "loss": 2.6876,
      "step": 264200
    },
    {
      "epoch": 2.6944948331736747,
      "grad_norm": 2.5752060413360596,
      "learning_rate": 5.091922630087846e-06,
      "loss": 2.6646,
      "step": 264400
    },
    {
      "epoch": 2.696533028962762,
      "grad_norm": 4.06433629989624,
      "learning_rate": 5.057952700269721e-06,
      "loss": 2.7502,
      "step": 264600
    },
    {
      "epoch": 2.6985712247518494,
      "grad_norm": 3.367499351501465,
      "learning_rate": 5.023982770451596e-06,
      "loss": 2.6279,
      "step": 264800
    },
    {
      "epoch": 2.7006094205409372,
      "grad_norm": 7.001542568206787,
      "learning_rate": 4.990012840633471e-06,
      "loss": 2.6181,
      "step": 265000
    },
    {
      "epoch": 2.7026476163300246,
      "grad_norm": 3.8083324432373047,
      "learning_rate": 4.956042910815347e-06,
      "loss": 2.644,
      "step": 265200
    },
    {
      "epoch": 2.704685812119112,
      "grad_norm": 4.012723922729492,
      "learning_rate": 4.922072980997222e-06,
      "loss": 2.691,
      "step": 265400
    },
    {
      "epoch": 2.7067240079082,
      "grad_norm": 3.260606288909912,
      "learning_rate": 4.888103051179097e-06,
      "loss": 2.6914,
      "step": 265600
    },
    {
      "epoch": 2.708762203697287,
      "grad_norm": 4.414688587188721,
      "learning_rate": 4.8541331213609716e-06,
      "loss": 2.7335,
      "step": 265800
    },
    {
      "epoch": 2.7108003994863745,
      "grad_norm": 3.825406551361084,
      "learning_rate": 4.8201631915428465e-06,
      "loss": 2.6642,
      "step": 266000
    },
    {
      "epoch": 2.7128385952754623,
      "grad_norm": 3.7558999061584473,
      "learning_rate": 4.786193261724721e-06,
      "loss": 2.6429,
      "step": 266200
    },
    {
      "epoch": 2.7148767910645497,
      "grad_norm": 3.0454933643341064,
      "learning_rate": 4.752223331906596e-06,
      "loss": 2.6648,
      "step": 266400
    },
    {
      "epoch": 2.716914986853637,
      "grad_norm": 8.506939888000488,
      "learning_rate": 4.718253402088472e-06,
      "loss": 2.7031,
      "step": 266600
    },
    {
      "epoch": 2.718953182642725,
      "grad_norm": 9.19083309173584,
      "learning_rate": 4.684283472270346e-06,
      "loss": 2.6772,
      "step": 266800
    },
    {
      "epoch": 2.7209913784318123,
      "grad_norm": 4.740117073059082,
      "learning_rate": 4.650313542452221e-06,
      "loss": 2.6885,
      "step": 267000
    },
    {
      "epoch": 2.7230295742208996,
      "grad_norm": 2.7917263507843018,
      "learning_rate": 4.616343612634096e-06,
      "loss": 2.6661,
      "step": 267200
    },
    {
      "epoch": 2.725067770009987,
      "grad_norm": 5.856988906860352,
      "learning_rate": 4.582373682815971e-06,
      "loss": 2.6818,
      "step": 267400
    },
    {
      "epoch": 2.7271059657990744,
      "grad_norm": 3.8704514503479004,
      "learning_rate": 4.548403752997847e-06,
      "loss": 2.6573,
      "step": 267600
    },
    {
      "epoch": 2.729144161588162,
      "grad_norm": 4.176896095275879,
      "learning_rate": 4.514433823179722e-06,
      "loss": 2.6973,
      "step": 267800
    },
    {
      "epoch": 2.7311823573772496,
      "grad_norm": 3.125221014022827,
      "learning_rate": 4.480463893361597e-06,
      "loss": 2.6958,
      "step": 268000
    },
    {
      "epoch": 2.733220553166337,
      "grad_norm": 3.3162901401519775,
      "learning_rate": 4.4464939635434715e-06,
      "loss": 2.6424,
      "step": 268200
    },
    {
      "epoch": 2.7352587489554248,
      "grad_norm": 4.3071370124816895,
      "learning_rate": 4.4125240337253464e-06,
      "loss": 2.6998,
      "step": 268400
    },
    {
      "epoch": 2.737296944744512,
      "grad_norm": 2.965749979019165,
      "learning_rate": 4.378554103907221e-06,
      "loss": 2.6982,
      "step": 268600
    },
    {
      "epoch": 2.7393351405335995,
      "grad_norm": 4.652376651763916,
      "learning_rate": 4.344584174089097e-06,
      "loss": 2.7134,
      "step": 268800
    },
    {
      "epoch": 2.7413733363226873,
      "grad_norm": 3.479196310043335,
      "learning_rate": 4.310614244270972e-06,
      "loss": 2.7403,
      "step": 269000
    },
    {
      "epoch": 2.7434115321117747,
      "grad_norm": 4.450420379638672,
      "learning_rate": 4.276644314452847e-06,
      "loss": 2.5896,
      "step": 269200
    },
    {
      "epoch": 2.745449727900862,
      "grad_norm": 3.677577018737793,
      "learning_rate": 4.242674384634721e-06,
      "loss": 2.665,
      "step": 269400
    },
    {
      "epoch": 2.74748792368995,
      "grad_norm": 6.099633693695068,
      "learning_rate": 4.208704454816596e-06,
      "loss": 2.7099,
      "step": 269600
    },
    {
      "epoch": 2.7495261194790372,
      "grad_norm": 4.998110771179199,
      "learning_rate": 4.174734524998472e-06,
      "loss": 2.6714,
      "step": 269800
    },
    {
      "epoch": 2.7515643152681246,
      "grad_norm": 3.291121482849121,
      "learning_rate": 4.140764595180347e-06,
      "loss": 2.7186,
      "step": 270000
    },
    {
      "epoch": 2.7536025110572124,
      "grad_norm": 2.8827383518218994,
      "learning_rate": 4.106794665362222e-06,
      "loss": 2.74,
      "step": 270200
    },
    {
      "epoch": 2.7556407068463,
      "grad_norm": 5.627294540405273,
      "learning_rate": 4.0728247355440965e-06,
      "loss": 2.7193,
      "step": 270400
    },
    {
      "epoch": 2.757678902635387,
      "grad_norm": 4.433568954467773,
      "learning_rate": 4.0388548057259714e-06,
      "loss": 2.7279,
      "step": 270600
    },
    {
      "epoch": 2.7597170984244745,
      "grad_norm": 3.387558698654175,
      "learning_rate": 4.004884875907846e-06,
      "loss": 2.677,
      "step": 270800
    },
    {
      "epoch": 2.761755294213562,
      "grad_norm": 2.927614450454712,
      "learning_rate": 3.970914946089722e-06,
      "loss": 2.6724,
      "step": 271000
    },
    {
      "epoch": 2.7637934900026497,
      "grad_norm": 6.106566905975342,
      "learning_rate": 3.936945016271597e-06,
      "loss": 2.6954,
      "step": 271200
    },
    {
      "epoch": 2.765831685791737,
      "grad_norm": 2.5620007514953613,
      "learning_rate": 3.902975086453472e-06,
      "loss": 2.6732,
      "step": 271400
    },
    {
      "epoch": 2.7678698815808245,
      "grad_norm": 4.514806747436523,
      "learning_rate": 3.869005156635347e-06,
      "loss": 2.7189,
      "step": 271600
    },
    {
      "epoch": 2.7699080773699123,
      "grad_norm": 3.9764244556427,
      "learning_rate": 3.835035226817221e-06,
      "loss": 2.7078,
      "step": 271800
    },
    {
      "epoch": 2.7719462731589997,
      "grad_norm": 3.782172203063965,
      "learning_rate": 3.8010652969990963e-06,
      "loss": 2.635,
      "step": 272000
    },
    {
      "epoch": 2.773984468948087,
      "grad_norm": 4.311367511749268,
      "learning_rate": 3.7670953671809713e-06,
      "loss": 2.6757,
      "step": 272200
    },
    {
      "epoch": 2.776022664737175,
      "grad_norm": 4.144481182098389,
      "learning_rate": 3.7331254373628466e-06,
      "loss": 2.6847,
      "step": 272400
    },
    {
      "epoch": 2.778060860526262,
      "grad_norm": 4.043919086456299,
      "learning_rate": 3.6991555075447215e-06,
      "loss": 2.7446,
      "step": 272600
    },
    {
      "epoch": 2.7800990563153496,
      "grad_norm": 4.57282018661499,
      "learning_rate": 3.6651855777265965e-06,
      "loss": 2.6642,
      "step": 272800
    },
    {
      "epoch": 2.7821372521044374,
      "grad_norm": 3.160623073577881,
      "learning_rate": 3.631215647908472e-06,
      "loss": 2.6774,
      "step": 273000
    },
    {
      "epoch": 2.784175447893525,
      "grad_norm": 4.1175618171691895,
      "learning_rate": 3.5972457180903467e-06,
      "loss": 2.7073,
      "step": 273200
    },
    {
      "epoch": 2.786213643682612,
      "grad_norm": 2.711686372756958,
      "learning_rate": 3.5632757882722217e-06,
      "loss": 2.6859,
      "step": 273400
    },
    {
      "epoch": 2.7882518394716995,
      "grad_norm": 3.9768519401550293,
      "learning_rate": 3.529305858454097e-06,
      "loss": 2.6666,
      "step": 273600
    },
    {
      "epoch": 2.790290035260787,
      "grad_norm": 5.078091621398926,
      "learning_rate": 3.495335928635972e-06,
      "loss": 2.6586,
      "step": 273800
    },
    {
      "epoch": 2.7923282310498747,
      "grad_norm": 2.8418045043945312,
      "learning_rate": 3.461365998817847e-06,
      "loss": 2.7202,
      "step": 274000
    },
    {
      "epoch": 2.794366426838962,
      "grad_norm": 3.7691457271575928,
      "learning_rate": 3.4273960689997214e-06,
      "loss": 2.6829,
      "step": 274200
    },
    {
      "epoch": 2.7964046226280495,
      "grad_norm": 3.195188045501709,
      "learning_rate": 3.3934261391815963e-06,
      "loss": 2.7072,
      "step": 274400
    },
    {
      "epoch": 2.7984428184171373,
      "grad_norm": 3.4779891967773438,
      "learning_rate": 3.359456209363471e-06,
      "loss": 2.7451,
      "step": 274600
    },
    {
      "epoch": 2.8004810142062246,
      "grad_norm": 3.1906344890594482,
      "learning_rate": 3.3254862795453466e-06,
      "loss": 2.6919,
      "step": 274800
    },
    {
      "epoch": 2.802519209995312,
      "grad_norm": 4.005129337310791,
      "learning_rate": 3.2915163497272215e-06,
      "loss": 2.65,
      "step": 275000
    },
    {
      "epoch": 2.8045574057844,
      "grad_norm": 4.775338649749756,
      "learning_rate": 3.2575464199090964e-06,
      "loss": 2.6926,
      "step": 275200
    },
    {
      "epoch": 2.806595601573487,
      "grad_norm": 4.861435413360596,
      "learning_rate": 3.2235764900909718e-06,
      "loss": 2.6822,
      "step": 275400
    },
    {
      "epoch": 2.8086337973625746,
      "grad_norm": 3.4099714756011963,
      "learning_rate": 3.1896065602728467e-06,
      "loss": 2.6824,
      "step": 275600
    },
    {
      "epoch": 2.8106719931516624,
      "grad_norm": 3.221134662628174,
      "learning_rate": 3.1556366304547216e-06,
      "loss": 2.6143,
      "step": 275800
    },
    {
      "epoch": 2.8127101889407498,
      "grad_norm": 2.585902690887451,
      "learning_rate": 3.1216667006365965e-06,
      "loss": 2.6426,
      "step": 276000
    },
    {
      "epoch": 2.814748384729837,
      "grad_norm": 5.062975883483887,
      "learning_rate": 3.0876967708184714e-06,
      "loss": 2.662,
      "step": 276200
    },
    {
      "epoch": 2.8167865805189245,
      "grad_norm": 6.252180576324463,
      "learning_rate": 3.053726841000347e-06,
      "loss": 2.6899,
      "step": 276400
    },
    {
      "epoch": 2.818824776308012,
      "grad_norm": 4.510321617126465,
      "learning_rate": 3.0197569111822217e-06,
      "loss": 2.6711,
      "step": 276600
    },
    {
      "epoch": 2.8208629720970997,
      "grad_norm": 3.261461019515991,
      "learning_rate": 2.9857869813640966e-06,
      "loss": 2.6894,
      "step": 276800
    },
    {
      "epoch": 2.822901167886187,
      "grad_norm": 3.421415090560913,
      "learning_rate": 2.9518170515459716e-06,
      "loss": 2.6443,
      "step": 277000
    },
    {
      "epoch": 2.8249393636752744,
      "grad_norm": 4.8341898918151855,
      "learning_rate": 2.9178471217278465e-06,
      "loss": 2.6645,
      "step": 277200
    },
    {
      "epoch": 2.8269775594643622,
      "grad_norm": 4.983767509460449,
      "learning_rate": 2.8838771919097214e-06,
      "loss": 2.6714,
      "step": 277400
    },
    {
      "epoch": 2.8290157552534496,
      "grad_norm": 7.434760570526123,
      "learning_rate": 2.8499072620915968e-06,
      "loss": 2.7455,
      "step": 277600
    },
    {
      "epoch": 2.831053951042537,
      "grad_norm": 4.611947536468506,
      "learning_rate": 2.8159373322734717e-06,
      "loss": 2.7378,
      "step": 277800
    },
    {
      "epoch": 2.833092146831625,
      "grad_norm": 3.6387646198272705,
      "learning_rate": 2.7819674024553466e-06,
      "loss": 2.6675,
      "step": 278000
    },
    {
      "epoch": 2.835130342620712,
      "grad_norm": 6.517463684082031,
      "learning_rate": 2.747997472637222e-06,
      "loss": 2.6319,
      "step": 278200
    },
    {
      "epoch": 2.8371685384097995,
      "grad_norm": 2.677083730697632,
      "learning_rate": 2.7140275428190965e-06,
      "loss": 2.7364,
      "step": 278400
    },
    {
      "epoch": 2.8392067341988874,
      "grad_norm": 3.026233196258545,
      "learning_rate": 2.6800576130009714e-06,
      "loss": 2.6958,
      "step": 278600
    },
    {
      "epoch": 2.8412449299879747,
      "grad_norm": 5.2079668045043945,
      "learning_rate": 2.6460876831828467e-06,
      "loss": 2.699,
      "step": 278800
    },
    {
      "epoch": 2.843283125777062,
      "grad_norm": 3.4005932807922363,
      "learning_rate": 2.6121177533647217e-06,
      "loss": 2.7011,
      "step": 279000
    },
    {
      "epoch": 2.84532132156615,
      "grad_norm": 3.5942113399505615,
      "learning_rate": 2.5781478235465966e-06,
      "loss": 2.652,
      "step": 279200
    },
    {
      "epoch": 2.8473595173552373,
      "grad_norm": 3.567117691040039,
      "learning_rate": 2.544177893728472e-06,
      "loss": 2.7472,
      "step": 279400
    },
    {
      "epoch": 2.8493977131443247,
      "grad_norm": 2.7486391067504883,
      "learning_rate": 2.5102079639103464e-06,
      "loss": 2.6808,
      "step": 279600
    },
    {
      "epoch": 2.851435908933412,
      "grad_norm": 4.8355865478515625,
      "learning_rate": 2.4762380340922218e-06,
      "loss": 2.7254,
      "step": 279800
    },
    {
      "epoch": 2.8534741047224994,
      "grad_norm": 13.40642261505127,
      "learning_rate": 2.4422681042740967e-06,
      "loss": 2.7269,
      "step": 280000
    },
    {
      "epoch": 2.855512300511587,
      "grad_norm": 4.074978828430176,
      "learning_rate": 2.4082981744559716e-06,
      "loss": 2.7333,
      "step": 280200
    },
    {
      "epoch": 2.8575504963006746,
      "grad_norm": 4.902310371398926,
      "learning_rate": 2.3743282446378466e-06,
      "loss": 2.6895,
      "step": 280400
    },
    {
      "epoch": 2.859588692089762,
      "grad_norm": 4.047857761383057,
      "learning_rate": 2.340358314819722e-06,
      "loss": 2.6566,
      "step": 280600
    },
    {
      "epoch": 2.86162688787885,
      "grad_norm": 3.5246472358703613,
      "learning_rate": 2.3063883850015964e-06,
      "loss": 2.7645,
      "step": 280800
    },
    {
      "epoch": 2.863665083667937,
      "grad_norm": 4.314565658569336,
      "learning_rate": 2.2724184551834718e-06,
      "loss": 2.6909,
      "step": 281000
    },
    {
      "epoch": 2.8657032794570245,
      "grad_norm": 4.683866500854492,
      "learning_rate": 2.2384485253653467e-06,
      "loss": 2.7376,
      "step": 281200
    },
    {
      "epoch": 2.8677414752461123,
      "grad_norm": 4.4169721603393555,
      "learning_rate": 2.2044785955472216e-06,
      "loss": 2.7135,
      "step": 281400
    },
    {
      "epoch": 2.8697796710351997,
      "grad_norm": 3.8067591190338135,
      "learning_rate": 2.170508665729097e-06,
      "loss": 2.659,
      "step": 281600
    },
    {
      "epoch": 2.871817866824287,
      "grad_norm": 3.073906421661377,
      "learning_rate": 2.136538735910972e-06,
      "loss": 2.7185,
      "step": 281800
    },
    {
      "epoch": 2.873856062613375,
      "grad_norm": 5.108511924743652,
      "learning_rate": 2.102568806092847e-06,
      "loss": 2.6542,
      "step": 282000
    },
    {
      "epoch": 2.8758942584024623,
      "grad_norm": 4.0376200675964355,
      "learning_rate": 2.0685988762747217e-06,
      "loss": 2.7027,
      "step": 282200
    },
    {
      "epoch": 2.8779324541915496,
      "grad_norm": 2.9706838130950928,
      "learning_rate": 2.0346289464565966e-06,
      "loss": 2.7371,
      "step": 282400
    },
    {
      "epoch": 2.879970649980637,
      "grad_norm": 2.7785415649414062,
      "learning_rate": 2.0006590166384716e-06,
      "loss": 2.6215,
      "step": 282600
    },
    {
      "epoch": 2.8820088457697244,
      "grad_norm": 2.8715755939483643,
      "learning_rate": 1.966689086820347e-06,
      "loss": 2.6748,
      "step": 282800
    },
    {
      "epoch": 2.884047041558812,
      "grad_norm": 2.566049337387085,
      "learning_rate": 1.932719157002222e-06,
      "loss": 2.6774,
      "step": 283000
    },
    {
      "epoch": 2.8860852373478996,
      "grad_norm": 3.5916154384613037,
      "learning_rate": 1.898749227184097e-06,
      "loss": 2.7467,
      "step": 283200
    },
    {
      "epoch": 2.888123433136987,
      "grad_norm": 4.331680774688721,
      "learning_rate": 1.8647792973659717e-06,
      "loss": 2.6866,
      "step": 283400
    },
    {
      "epoch": 2.8901616289260748,
      "grad_norm": 4.574120998382568,
      "learning_rate": 1.8308093675478466e-06,
      "loss": 2.6746,
      "step": 283600
    },
    {
      "epoch": 2.892199824715162,
      "grad_norm": 4.712932109832764,
      "learning_rate": 1.7968394377297218e-06,
      "loss": 2.7008,
      "step": 283800
    },
    {
      "epoch": 2.8942380205042495,
      "grad_norm": 2.9760725498199463,
      "learning_rate": 1.7628695079115969e-06,
      "loss": 2.657,
      "step": 284000
    },
    {
      "epoch": 2.8962762162933373,
      "grad_norm": 3.0610921382904053,
      "learning_rate": 1.7288995780934718e-06,
      "loss": 2.7289,
      "step": 284200
    },
    {
      "epoch": 2.8983144120824247,
      "grad_norm": 6.081277847290039,
      "learning_rate": 1.694929648275347e-06,
      "loss": 2.6964,
      "step": 284400
    },
    {
      "epoch": 2.900352607871512,
      "grad_norm": 3.1743509769439697,
      "learning_rate": 1.6609597184572217e-06,
      "loss": 2.701,
      "step": 284600
    },
    {
      "epoch": 2.9023908036606,
      "grad_norm": 3.7263710498809814,
      "learning_rate": 1.6269897886390966e-06,
      "loss": 2.6786,
      "step": 284800
    },
    {
      "epoch": 2.9044289994496872,
      "grad_norm": 4.891290664672852,
      "learning_rate": 1.5930198588209717e-06,
      "loss": 2.6855,
      "step": 285000
    },
    {
      "epoch": 2.9064671952387746,
      "grad_norm": 3.530374050140381,
      "learning_rate": 1.5590499290028469e-06,
      "loss": 2.6984,
      "step": 285200
    },
    {
      "epoch": 2.908505391027862,
      "grad_norm": 6.433976650238037,
      "learning_rate": 1.5250799991847218e-06,
      "loss": 2.7084,
      "step": 285400
    },
    {
      "epoch": 2.91054358681695,
      "grad_norm": 6.152221202850342,
      "learning_rate": 1.4911100693665967e-06,
      "loss": 2.714,
      "step": 285600
    },
    {
      "epoch": 2.912581782606037,
      "grad_norm": 5.690296649932861,
      "learning_rate": 1.4571401395484718e-06,
      "loss": 2.6292,
      "step": 285800
    },
    {
      "epoch": 2.9146199783951245,
      "grad_norm": 3.8430681228637695,
      "learning_rate": 1.4231702097303468e-06,
      "loss": 2.7092,
      "step": 286000
    },
    {
      "epoch": 2.916658174184212,
      "grad_norm": 9.90380859375,
      "learning_rate": 1.3892002799122217e-06,
      "loss": 2.7382,
      "step": 286200
    },
    {
      "epoch": 2.9186963699732997,
      "grad_norm": 4.937271595001221,
      "learning_rate": 1.3552303500940968e-06,
      "loss": 2.7019,
      "step": 286400
    },
    {
      "epoch": 2.920734565762387,
      "grad_norm": 3.620985746383667,
      "learning_rate": 1.3212604202759718e-06,
      "loss": 2.603,
      "step": 286600
    },
    {
      "epoch": 2.9227727615514745,
      "grad_norm": 3.407888650894165,
      "learning_rate": 1.2872904904578467e-06,
      "loss": 2.668,
      "step": 286800
    },
    {
      "epoch": 2.9248109573405623,
      "grad_norm": 5.409389019012451,
      "learning_rate": 1.2533205606397218e-06,
      "loss": 2.6732,
      "step": 287000
    },
    {
      "epoch": 2.9268491531296497,
      "grad_norm": 4.5086350440979,
      "learning_rate": 1.2193506308215967e-06,
      "loss": 2.6781,
      "step": 287200
    },
    {
      "epoch": 2.928887348918737,
      "grad_norm": 2.849107027053833,
      "learning_rate": 1.1853807010034717e-06,
      "loss": 2.6445,
      "step": 287400
    },
    {
      "epoch": 2.930925544707825,
      "grad_norm": 2.628094434738159,
      "learning_rate": 1.1514107711853468e-06,
      "loss": 2.6393,
      "step": 287600
    },
    {
      "epoch": 2.932963740496912,
      "grad_norm": 4.307394981384277,
      "learning_rate": 1.1174408413672217e-06,
      "loss": 2.6562,
      "step": 287800
    },
    {
      "epoch": 2.9350019362859996,
      "grad_norm": 5.3732171058654785,
      "learning_rate": 1.0834709115490969e-06,
      "loss": 2.7505,
      "step": 288000
    },
    {
      "epoch": 2.9370401320750874,
      "grad_norm": 4.310615539550781,
      "learning_rate": 1.0495009817309718e-06,
      "loss": 2.6745,
      "step": 288200
    },
    {
      "epoch": 2.9390783278641748,
      "grad_norm": 6.483718395233154,
      "learning_rate": 1.0155310519128467e-06,
      "loss": 2.7438,
      "step": 288400
    },
    {
      "epoch": 2.941116523653262,
      "grad_norm": 3.648073196411133,
      "learning_rate": 9.815611220947218e-07,
      "loss": 2.6716,
      "step": 288600
    },
    {
      "epoch": 2.9431547194423495,
      "grad_norm": 2.759943962097168,
      "learning_rate": 9.475911922765969e-07,
      "loss": 2.717,
      "step": 288800
    },
    {
      "epoch": 2.945192915231437,
      "grad_norm": 8.791967391967773,
      "learning_rate": 9.136212624584719e-07,
      "loss": 2.7051,
      "step": 289000
    },
    {
      "epoch": 2.9472311110205247,
      "grad_norm": 4.687225341796875,
      "learning_rate": 8.796513326403468e-07,
      "loss": 2.7708,
      "step": 289200
    },
    {
      "epoch": 2.949269306809612,
      "grad_norm": 4.816114902496338,
      "learning_rate": 8.456814028222219e-07,
      "loss": 2.6707,
      "step": 289400
    },
    {
      "epoch": 2.9513075025986995,
      "grad_norm": 2.97278094291687,
      "learning_rate": 8.117114730040969e-07,
      "loss": 2.6551,
      "step": 289600
    },
    {
      "epoch": 2.9533456983877873,
      "grad_norm": 5.593526840209961,
      "learning_rate": 7.777415431859718e-07,
      "loss": 2.716,
      "step": 289800
    },
    {
      "epoch": 2.9553838941768746,
      "grad_norm": 8.693511009216309,
      "learning_rate": 7.437716133678468e-07,
      "loss": 2.7148,
      "step": 290000
    },
    {
      "epoch": 2.957422089965962,
      "grad_norm": 5.666980743408203,
      "learning_rate": 7.098016835497218e-07,
      "loss": 2.6877,
      "step": 290200
    },
    {
      "epoch": 2.95946028575505,
      "grad_norm": 5.433487415313721,
      "learning_rate": 6.758317537315968e-07,
      "loss": 2.6592,
      "step": 290400
    },
    {
      "epoch": 2.961498481544137,
      "grad_norm": 4.2121992111206055,
      "learning_rate": 6.418618239134718e-07,
      "loss": 2.6976,
      "step": 290600
    },
    {
      "epoch": 2.9635366773332246,
      "grad_norm": 3.9173829555511475,
      "learning_rate": 6.078918940953469e-07,
      "loss": 2.6989,
      "step": 290800
    },
    {
      "epoch": 2.9655748731223124,
      "grad_norm": 4.408733367919922,
      "learning_rate": 5.739219642772218e-07,
      "loss": 2.6666,
      "step": 291000
    },
    {
      "epoch": 2.9676130689113998,
      "grad_norm": 4.996479511260986,
      "learning_rate": 5.399520344590968e-07,
      "loss": 2.708,
      "step": 291200
    },
    {
      "epoch": 2.969651264700487,
      "grad_norm": 3.7781450748443604,
      "learning_rate": 5.059821046409718e-07,
      "loss": 2.6386,
      "step": 291400
    },
    {
      "epoch": 2.9716894604895745,
      "grad_norm": 6.253572940826416,
      "learning_rate": 4.720121748228468e-07,
      "loss": 2.6889,
      "step": 291600
    },
    {
      "epoch": 2.973727656278662,
      "grad_norm": 4.002562522888184,
      "learning_rate": 4.3804224500472185e-07,
      "loss": 2.693,
      "step": 291800
    },
    {
      "epoch": 2.9757658520677497,
      "grad_norm": 4.059589385986328,
      "learning_rate": 4.0407231518659683e-07,
      "loss": 2.6315,
      "step": 292000
    },
    {
      "epoch": 2.977804047856837,
      "grad_norm": 2.9045233726501465,
      "learning_rate": 3.7010238536847186e-07,
      "loss": 2.6643,
      "step": 292200
    },
    {
      "epoch": 2.9798422436459244,
      "grad_norm": 3.509838581085205,
      "learning_rate": 3.361324555503469e-07,
      "loss": 2.6738,
      "step": 292400
    },
    {
      "epoch": 2.9818804394350122,
      "grad_norm": 4.66896390914917,
      "learning_rate": 3.021625257322218e-07,
      "loss": 2.687,
      "step": 292600
    },
    {
      "epoch": 2.9839186352240996,
      "grad_norm": 3.1639139652252197,
      "learning_rate": 2.6819259591409685e-07,
      "loss": 2.6781,
      "step": 292800
    },
    {
      "epoch": 2.985956831013187,
      "grad_norm": 3.6985974311828613,
      "learning_rate": 2.3422266609597188e-07,
      "loss": 2.6251,
      "step": 293000
    },
    {
      "epoch": 2.987995026802275,
      "grad_norm": 5.017507553100586,
      "learning_rate": 2.0025273627784685e-07,
      "loss": 2.6605,
      "step": 293200
    },
    {
      "epoch": 2.990033222591362,
      "grad_norm": 4.8202314376831055,
      "learning_rate": 1.6628280645972186e-07,
      "loss": 2.697,
      "step": 293400
    },
    {
      "epoch": 2.9920714183804495,
      "grad_norm": 3.330521821975708,
      "learning_rate": 1.323128766415969e-07,
      "loss": 2.6369,
      "step": 293600
    },
    {
      "epoch": 2.9941096141695374,
      "grad_norm": 5.312365531921387,
      "learning_rate": 9.834294682347186e-08,
      "loss": 2.7065,
      "step": 293800
    },
    {
      "epoch": 2.9961478099586247,
      "grad_norm": 4.389549255371094,
      "learning_rate": 6.437301700534688e-08,
      "loss": 2.669,
      "step": 294000
    },
    {
      "epoch": 2.998186005747712,
      "grad_norm": 4.931697368621826,
      "learning_rate": 3.040308718722188e-08,
      "loss": 2.705,
      "step": 294200
    },
    {
      "epoch": 3.0,
      "eval_loss": 2.817159652709961,
      "eval_runtime": 194.0658,
      "eval_samples_per_second": 437.949,
      "eval_steps_per_second": 54.744,
      "step": 294378
    }
  ],
  "logging_steps": 200,
  "max_steps": 294378,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.5383904982872883e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
